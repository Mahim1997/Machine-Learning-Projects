{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Offline-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrnubMIEG0BZ"
      },
      "source": [
        "### MOUNT DRIVE ....."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1oJyY78cXnq"
      },
      "source": [
        "## First Import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import pickle\n",
        "\n",
        "from scipy import stats\n",
        "from xml.dom import minidom\n",
        "\n",
        "from sklearn.metrics import pairwise_distances, accuracy_score\n",
        "\n",
        "\n",
        "# !pip3 install tqdm\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "from functools import reduce"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFLKkx6Ybnlr"
      },
      "source": [
        "FOLDER_TRAIN = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/Training/'\n",
        "# FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics.txt'\n",
        "FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics-all.txt'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9hrTtEZp2A8"
      },
      "source": [
        "# !ls \"$FOLDER_TRAIN\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0bZ4kTHcXql"
      },
      "source": [
        "## Set random seed\n",
        "RANDOM_STATE = 22\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThQtktwcXtX",
        "outputId": "65e3899d-f974-47af-9511-d417263edc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Read data\n",
        "topic_remove = '3d_Printer' ## Remove 3D-printer\n",
        "list_doc_types = []\n",
        "with open(FILE_TOPICS, 'r') as fp:\n",
        "    topic = fp.readline()\n",
        "    while topic:\n",
        "        topic = topic.replace(\"\\n\", \"\")\n",
        "        list_doc_types.append(topic)\n",
        "        topic = fp.readline()\n",
        "\n",
        "list_doc_types.remove(topic_remove)\n",
        "print(list_doc_types)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Astronomy', 'Coffee', 'Space', 'Anime', 'Biology', 'Cooking', 'Windows_Phone', 'Arduino', 'Chess', 'Law', 'Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfC2PYXxg35A",
        "outputId": "0c347fa6-c21b-4aa6-de41-5a355282f6a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(list_doc_types)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Astronomy', 'Coffee', 'Space', 'Anime', 'Biology', 'Cooking', 'Windows_Phone', 'Arduino', 'Chess', 'Law', 'Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVPfrYjagVv4"
      },
      "source": [
        "def get_train_val_test_data(file_name):\n",
        "    xmldoc = minidom.parse(file_name)\n",
        "    xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "    \n",
        "    # print(f\"Inside get_train_val_test_data(), len(item_list) = {len(item_list)}\")\n",
        "\n",
        "    item_list = [x.attributes['Body'].value for x in xml_list]\n",
        "\n",
        "    train_list = item_list[0:500] ## first 500 train\n",
        "    val_list =  item_list[500:700] ## next 200 val\n",
        "    test_list = item_list[700:1200] ## next 500 test\n",
        "\n",
        "    ## delete original list.\n",
        "    del item_list\n",
        "    del xmldoc\n",
        "\n",
        "    ## return the new lists\n",
        "    return train_list, val_list, test_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvop_yNmUCMo"
      },
      "source": [
        "def add_to_dataframe(df_old, to_add):\n",
        "    df_old = df_old.append(pd.Series(to_add, index=df_old.columns), ignore_index=True)\n",
        "    return df_old"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MJXly5bt7aC"
      },
      "source": [
        "## Create three dataframes.\n",
        "\n",
        "## https://www.kite.com/python/answers/how-to-create-an-empty-dataframe-with-column-names-in-python\n",
        "column_names =[\"content\", \"Label\"]\n",
        "\n",
        "df_train = pd.DataFrame(columns = column_names)\n",
        "df_val = pd.DataFrame(columns = column_names)\n",
        "df_test = pd.DataFrame(columns = column_names)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxZHmfGt1g9e",
        "outputId": "70f982d4-12eb-4349-836c-154f24e4a57d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "### Populate all topics\n",
        "def populate_data_frames(df_train, df_val, df_test, list_doc_types):\n",
        "    for topic in list_doc_types: ## iterating per topic/label\n",
        "        label = topic ## assign label\n",
        "        \n",
        "        ## read using xml package\n",
        "        file_name = FOLDER_TRAIN + topic + \".xml\"\n",
        "        xmldoc = minidom.parse(file_name)\n",
        "        xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "\n",
        "        ## get train, val, test lists\n",
        "        train_list, val_list, test_list = get_train_val_test_data(file_name=file_name)\n",
        "\n",
        "        print(len(train_list), len(val_list), len(test_list))\n",
        "\n",
        "        ## add to dataframe\n",
        "        for v1 in train_list:\n",
        "            df_train = add_to_dataframe(df_old=df_train, to_add=[v1, label])\n",
        "        for v2 in val_list:\n",
        "            df_val = add_to_dataframe(df_old=df_val, to_add=[v2, label])\n",
        "        for v3 in test_list:\n",
        "            df_test = add_to_dataframe(df_old=df_test, to_add=[v3, label])\n",
        "\n",
        "\n",
        "        ## delete original list\n",
        "        del train_list\n",
        "        del val_list\n",
        "        del test_list\n",
        "\n",
        "    ## return dataframes\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "### Call the function\n",
        "df_train, df_val, df_test = populate_data_frames(df_train=df_train, df_val=df_val, df_test=df_test, list_doc_types=list_doc_types)\n",
        "\n",
        "print(f\"len df_train = {len(df_train)}\")\n",
        "print(f\"len df_val = {len(df_val)}\")\n",
        "print(f\"len df_test = {len(df_test)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "len df_train = 5500\n",
            "len df_val = 2200\n",
            "len df_test = 5500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUI07UZKmPc"
      },
      "source": [
        "## Shuffle dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rcv33P3XXuN"
      },
      "source": [
        "df_train = df_train.sample(frac=1, random_state=RANDOM_STATE)\n",
        "df_val = df_val.sample(frac=1, random_state=RANDOM_STATE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1m0_Du8XYBK"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKSziS4PvdnZ"
      },
      "source": [
        "## https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
        "## https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "## from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIukiiJxDWdu",
        "outputId": "bc511e47-f24f-4177-d4e3-a6a4c8c2f8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "## https://stackoverflow.com/questions/26693736/nltk-and-stopwords-fail-lookuperror\n",
        "## https://stackoverflow.com/questions/26570944/resource-utokenizers-punkt-english-pickle-not-found\n",
        "\n",
        "# nltk.download()\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUsCIVJru269"
      },
      "source": [
        "## Preprocess each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-MlUQ4w1uM2",
        "cellView": "both",
        "outputId": "984a515f-fbdb-41d7-f5c1-3058bd8c0023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "## https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "## First lemmatize, and then stem.\n",
        "\n",
        "def pre_process_stem_sentence(sentence, stop_words, stemmer, lemmatizer):\n",
        "    ## save in another variable\n",
        "    text = sentence\n",
        "    ## remove HTML tags [using soup]\n",
        "    soup = BeautifulSoup(text)\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    ## remove <a href> type things\n",
        "    soup = BeautifulSoup(text) ## create soup again.\n",
        "    for a in soup.findAll('a'):\n",
        "        a.replaceWithChildren()\n",
        "    \n",
        "    text = str(soup) ## reform text\n",
        "\n",
        "\n",
        "    ## remove unicode.\n",
        "    text = re.sub(r\"&nbsp;\", \" \", text)\n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
        "\n",
        "    ## remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"https\\S+\", \"\", text)\n",
        "    ## text = re.sub(r\"www\\S+\", \"\", text)\n",
        "\n",
        "    ## to be safe, remove HTML tags again using regex\n",
        "    #### https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44  \n",
        "    clean = re.compile('<.*?>')\n",
        "    text = re.sub(clean, '', text)\n",
        "\n",
        "    ## convert to small letters.\n",
        "    text = text.lower()\n",
        "\n",
        "    ## replace newlines, tabs -> SPACE\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "\n",
        "    ## {COLON} make <a>:<b> become <a>[space]<b>\n",
        "    # text = text.replace(\": \", \":\") # :[space] -> : \n",
        "    text = text.replace(\":\", \" \")  # : -> [space]\n",
        "    \n",
        "    ## {HYPHEN}\n",
        "    # text = text.replace(\"- \", \"-\")\n",
        "    text = text.replace(\"-\", \" \")\n",
        "\n",
        "    ## numbers removal    \n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "\n",
        "    ## punctuations removal\n",
        "    text = text.translate((str.maketrans('','',string.punctuation)))   \n",
        "\n",
        "\n",
        "    ## [remove anything EXCEPT english letters]\n",
        "    ## https://stackoverflow.com/questions/6323296/python-remove-anything-that-is-not-a-letter-or-number\n",
        "    text = re.sub(  \"[^a-z ]\",              # Anything except 0..9, a..z and A..Z\n",
        "                    \"\",                     # replaced with nothing\n",
        "                    text)                   # in this string   \n",
        "\n",
        "    ## remove space initially and finally.\n",
        "    text = text.lstrip()\n",
        "\n",
        "    ## make double spaces become one space.\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    ## remove stop words (English).\n",
        "    # print(f\"Before stopwords removal, text.len = {len(text)}\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words_removed_words_list = [t for t in tokens if not t in stop_words]\n",
        "\n",
        "    # print(f\"After tokenize and stopwords, len stop_words_removed_words_list = {len(stop_words_removed_words_list)}\")\n",
        "\n",
        "    ## apply lemmatization.\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stop_words_removed_words_list]\n",
        "    \n",
        "    ## apply stemming.\n",
        "    stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
        "\n",
        "    ## return the final pre-processed list-of-words.\n",
        "    return stemmed_words\n",
        "\n",
        "################ Test on one sentence #######################\n",
        "\n",
        "## Obtain once.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "## Preprocess per element of dataframe to test.\n",
        "text_to_process = df_train['content'].iloc[499]\n",
        "print(text_to_process)\n",
        "\n",
        "print(\"--\"*90)\n",
        "\n",
        "preprocessed_text = pre_process_stem_sentence(sentence=text_to_process, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(preprocessed_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<h2>You never mentioned your tablet model... or Arduino model</h2>\n",
            "\n",
            "<p>There are some tablets designed to work with USB flash drives and printers. This is called OTG (on the go.</p>\n",
            "\n",
            "<p><strong>You need:</strong></p>\n",
            "\n",
            "<ul>\n",
            "<li>A tablet supporting OTG</li>\n",
            "<li>An OTG adapter that fits into your tablets USB slot and is compatible</li>\n",
            "<li><a href=\"https://play.google.com/store/apps/details?id=com.primavera.arduino.listener\" rel=\"nofollow\">The Arduino Uno Communicator App</a></li>\n",
            "<li>Arduino Uno (or clone) [Note: It says that it works with Atmega16U2 or Atmega8U2 programmed as a USB-to-serial converter so I would assume that that would cover a few boards more than the Uno.)</li>\n",
            "</ul>\n",
            "\n",
            "<p>Another alternitive is to look into <a href=\"https://www.google.com/search?q=arduino+bluetooth&amp;qscrl=1&amp;tbm=isch&amp;imgil=WyPC4O05Xn4rhM%253A%253Bhttps%253A%252F%252Fencrypted-tbn2.gstatic.com%252Fimages%253Fq%253Dtbn%253AANd9GcSfTtWqIuN9YYdd90OO_tlxsvLUYhV_pl7NfaMhHDzSRq9N6W9rNA%253B1200%253B1200%253B1soQc4CzMzh0MM%253Bhttp%25253A%25252F%25252Fm.dhgate.com%25252Fproduct%25252Fjy-mcu-arduino-bluetooth-wireless-serial%25252F151048482.html&amp;source=iu&amp;usg=__KyMLS4aeubxJnDaMvprgRgADxAs%3D&amp;sa=X&amp;ei=1BUCU635IYmEygGO1oHwCQ&amp;ved=0CGkQ9QEwBg&amp;biw=1280&amp;bih=899\" rel=\"nofollow\">Bluetooth</a> (Note: I just picked a random link but there are hundreds of similar BT adapters)</p>\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "['never', 'mention', 'tablet', 'model', 'arduino', 'model', 'tablet', 'design', 'work', 'usb', 'flash', 'drive', 'printer', 'call', 'otg', 'go', 'need', 'tablet', 'support', 'otg', 'otg', 'adapt', 'fit', 'tablet', 'usb', 'slot', 'compat', 'arduino', 'uno', 'commun', 'app', 'arduino', 'uno', 'clone', 'note', 'say', 'work', 'atmegau', 'atmegau', 'program', 'usb', 'serial', 'convert', 'would', 'assum', 'would', 'cover', 'board', 'uno', 'anoth', 'alternit', 'look', 'bluetooth', 'note', 'pick', 'random', 'link', 'hundr', 'similar', 'bt', 'adapt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4gGdZmwZDZ",
        "outputId": "19bab8d6-cb58-4c77-9203-ad675cf127d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "doc = df_val['content'].iloc[499]\n",
        "print(doc)\n",
        "\n",
        "print(\"--\"*85)\n",
        "\n",
        "preprocessed = pre_process_stem_sentence(sentence=doc, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(len(preprocessed))\n",
        "print(preprocessed)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>This sounds like it's more a matter of determining the original name / wording used to file the copyright. </p>\n",
            "\n",
            "<p>Without knowing more of that language, you're left to the typical sleuthing options:</p>\n",
            "\n",
            "<ul>\n",
            "<li>names of the company owners / major shareholders as of 15-18 years ago. </li>\n",
            "<li>other DBAs and holding companies of the company originally presumed to hold the copyright. </li>\n",
            "<li>brute force search of all categorically-related copyrights in the time range. </li>\n",
            "</ul>\n",
            "\n",
            "<p>The advice I would really like to give you is knowledge of how IP attorneys filed video game copyrights in the time span of 1985-2000.  This could reveal any unexpected filing categories that could have been used as part of niche or experimental copyright strategy (at that time) for this kind of IP. </p>\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "70\n",
            "['sound', 'like', 'matter', 'determin', 'origin', 'name', 'word', 'use', 'file', 'copyright', 'without', 'know', 'languag', 'your', 'left', 'typic', 'sleuth', 'option', 'name', 'compani', 'owner', 'major', 'sharehold', 'year', 'ago', 'dba', 'hold', 'compani', 'compani', 'origin', 'presum', 'hold', 'copyright', 'brute', 'forc', 'search', 'categor', 'relat', 'copyright', 'time', 'rang', 'advic', 'would', 'realli', 'like', 'give', 'knowledg', 'ip', 'attorney', 'file', 'video', 'game', 'copyright', 'time', 'span', 'could', 'reveal', 'unexpect', 'file', 'categori', 'could', 'use', 'part', 'nich', 'experiment', 'copyright', 'strategi', 'time', 'kind', 'ip']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbSKXXrRGGgo"
      },
      "source": [
        "## Preprocess for each sentence of train-dataframe.\n",
        "df_train[\"stemmed_content\"] = df_train.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_val[\"stemmed_content\"] = df_val.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_test[\"stemmed_content\"] = df_test.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puTtacUoHBZ9"
      },
      "source": [
        "# Form Vocabulary from dataframe_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwyJWOOIETs"
      },
      "source": [
        "def print_first_n_keys_and_vals_dict(dictionary, n=5):\n",
        "    print(f\"Len dictionary = {len(dictionary)}, printing first {n} keys, vals\")\n",
        "    itr = 0\n",
        "    for key in dictionary:\n",
        "        print(key, dictionary[key])\n",
        "        itr += 1\n",
        "        if itr == n:\n",
        "            break"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO4hhJYkXqRp",
        "outputId": "cc070fd1-ab88-478b-f9ec-ee49ee536aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Also remove one-len words\n",
        "dictionary_vocab = {} ## empty dict.\n",
        "for doc in df_train['stemmed_content']:\n",
        "    # print(f\"len doc = {len(doc)}\")\n",
        "    for word in doc:\n",
        "        if len(word) <= 1:\n",
        "            continue\n",
        "\n",
        "        len_currently = len(dictionary_vocab) ## add to len. [idx new]\n",
        "\n",
        "        if word not in dictionary_vocab:\n",
        "            dictionary_vocab[word] = (0, len_currently)\n",
        "        else:\n",
        "            (val, idx) = dictionary_vocab[word]\n",
        "            dictionary_vocab[word] = (val + 1, idx) ## Maintain the same index.\n",
        "\n",
        "print(f\"len dictionary_vocab = {len(dictionary_vocab)}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len dictionary_vocab = 17506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSQYB4eBf0sv"
      },
      "source": [
        "# file_name = \"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Vocab.txt\"\n",
        "# with open(file_name, 'w') as fw:\n",
        "#     for voc in dictionary_vocab:\n",
        "#         fw.write(voc)\n",
        "#         fw.write(\"\\n\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAgtrtyp13Tt"
      },
      "source": [
        "# ## https://www.kite.com/python/answers/how-to-save-a-dictionary-to-a-file-in-python\n",
        "# vocab_file = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/vocab_dict_df_train.pkl'\n",
        "\n",
        "# a_file = open(vocab_file, \"wb\")\n",
        "# pickle.dump(dictionary_vocab, a_file)\n",
        "# a_file.close()\n",
        "\n",
        "# # a_file = open(vocab_file, \"rb\")\n",
        "# # dictionary_vocab = pickle.load(a_file)\n",
        "# # print(dictionary_vocab)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJORdHUjHd7h"
      },
      "source": [
        "## Keep only those docs in train whose lengths are above 3 words i.e.\n",
        "### length of stemmed content > 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrZO1SycNmSX",
        "outputId": "cf94a1d0-c3cc-4f48-edbf-efc4b4f3b9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lens = [len(x) for x in df_train['stemmed_content']]\n",
        "print(max(lens))\n",
        "print(min(lens))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1381\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPJ5IMKKNmQV",
        "outputId": "f9a3096f-ebe2-4f55-f6f9-420bab490d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(f\"Before removal, len df_train = {len(df_train)}\")\n",
        "\n",
        "MIN_WORD_COUNT_TO_REMOVE = 2\n",
        "# print(df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE][\"stemmed_content\"])\n",
        "idxToRemove = df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE].index\n",
        "print(f\"To remove num items = {len(idxToRemove)}\")\n",
        "\n",
        "labels_to_remove = df_train['Label'].iloc[idxToRemove].values\n",
        "print(np.unique(labels_to_remove, return_counts=True)) ## Almost fairly distributed.\n",
        "\n",
        "\n",
        "df_train.drop(idxToRemove , inplace=True) ### Removing\n",
        "\n",
        "print(f\"After removal, len df_train = {len(df_train)}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before removal, len df_train = 5500\n",
            "To remove num items = 224\n",
            "(array(['Anime', 'Arduino', 'Astronomy', 'Biology', 'Chess', 'Coffee',\n",
            "       'Cooking', 'Law', 'Space', 'Windows_Phone', 'Wood_Working'],\n",
            "      dtype=object), array([18, 15, 29, 19, 18, 27, 24, 21, 19, 14, 20]))\n",
            "After removal, len df_train = 5276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQCXuG-JhUU"
      },
      "source": [
        "## Create Hamming Distance Vectors by representing with 0/1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJfu0JkdKFRh"
      },
      "source": [
        "def form_hamming_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    for word in list_words:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index. [DO NOT]\n",
        "            # vec[UNKNOWN_WORD_INDEX] = 1\n",
        "            continue\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word]\n",
        "            vec[idx] = 1\n",
        "    return vec"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNrTRsZ2psv-"
      },
      "source": [
        "## Create hamming vectors for each col of dataframe\n",
        "df_train[\"ham_vector\"] = df_train.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"ham_vector\"] = df_val.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"ham_vector\"] = df_test.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNM1YUEWrfgk"
      },
      "source": [
        "## Create eucledian vectors by representing how many times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJXmvI8areMJ"
      },
      "source": [
        "def form_eucledian_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    \n",
        "    ## Form a small dictionary to store each word count\n",
        "    dict_local_vocab = {}\n",
        "    for word in list_words:\n",
        "        if word not in dict_local_vocab:\n",
        "            dict_local_vocab[word] = 1\n",
        "        else:\n",
        "            dict_local_vocab[word] = dict_local_vocab[word] + 1\n",
        "\n",
        "    # print(dict_local_vocab)\n",
        "\n",
        "    # unknown_word_count = 1\n",
        "    for word in dict_local_vocab:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index.\n",
        "            continue\n",
        "            # vec[UNKNOWN_WORD_INDEX] = unknown_word_count\n",
        "            # unknown_word_count += 1\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word] \n",
        "            vec[idx] = dict_local_vocab[word] ## replace with value of this word instead of 1.\n",
        "\n",
        "    del dict_local_vocab\n",
        "    return vec"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvxfO7dkpszM"
      },
      "source": [
        "df_train[\"euc_vector\"] = df_train.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"euc_vector\"] = df_val.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"euc_vector\"] = df_test.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ImEkZTTwY_"
      },
      "source": [
        "# FOLDER_DATASET = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset'\n",
        "# df_train.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/train.csv', index=False)\n",
        "# df_val.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/val.csv', index=False)\n",
        "# df_test.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/test.csv', index=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEuOjilfKauq"
      },
      "source": [
        "# Stack each of these vertically to form hamming and eucledian vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3Cwy-2xb4s"
      },
      "source": [
        "def get_2D_vector(list_np_arr):\n",
        "    vec_2D = np.zeros((len(list_np_arr), len(list_np_arr[0])))\n",
        "    idx = 0\n",
        "    for vec in list_np_arr:\n",
        "        vec_2D[idx] = vec\n",
        "        idx += 1\n",
        "    return vec_2D"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB_4H48rL6kv",
        "outputId": "1a07cbca-0cbe-4350-9fd2-3f21ef7ec4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "vocab_size = len(dictionary_vocab)\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "\n",
        "num_documents = len(df_train)\n",
        "print(f\"num_documents = {num_documents}\")\n",
        "\n",
        "print(df_train.columns.values)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size = 17506\n",
            "num_documents = 5276\n",
            "['content' 'Label' 'stemmed_content' 'ham_vector' 'euc_vector']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oodrsKK6GlG",
        "outputId": "92d7639a-c22d-40c8-ee2a-c1647f2dc385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hamming_vectors_2D = get_2D_vector(list_np_arr=df_train['ham_vector'].values)\n",
        "eucledian_vectors_2D = get_2D_vector(list_np_arr=df_train['euc_vector'].values)\n",
        "print(hamming_vectors_2D.shape, eucledian_vectors_2D.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5276, 17506) (5276, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrEqp1_0LH5"
      },
      "source": [
        "## For now save these dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViUb7lUGzp9N"
      },
      "source": [
        "# df_train.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/train.csv\", index=False)\n",
        "# df_val.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/val.csv\", index=False)\n",
        "# df_test.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/test.csv\", index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmALIJNfzuMJ"
      },
      "source": [
        "## Now, finally create TF-IDF and pickel dump everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__-GfML7BDN",
        "outputId": "047b551a-eade-49c8-b67e-685e1d37ca88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "def compute_TF_IDF_all_train_set(hamming_vectors_2D, eucledian_vectors_2D, alpha=0.0001, beta=0.0001):\n",
        "    num_documents, num_words = eucledian_vectors_2D.shape\n",
        "    print(f\"num_documents, num_words = {num_documents, num_words}\")\n",
        "\n",
        "    TF = np.zeros((num_documents, num_words))\n",
        "\n",
        "    IDF = np.zeros((1, num_words))\n",
        "\n",
        "    ## Calculate TF(d, w) = N(d, w)/W(d) ; where N(d, w): count(w) in document d , W(d): Total #words in document d\n",
        "    for itr in range(num_documents): ## iterate row-wise\n",
        "        doc_eucledian = eucledian_vectors_2D[itr]\n",
        "        total_num_words_doc_eucledian = num_words - np.sum(doc_eucledian == 0)        \n",
        "        TF[itr] = doc_eucledian/total_num_words_doc_eucledian\n",
        "\n",
        "        # if itr == 1:\n",
        "            # print(f\"itr = {itr}, doc_euc[itr] = {np.unique(doc_eucledian[itr], return_counts=True)}\")\n",
        "            # print(f\"itr = {itr}, TF[itr] = {np.unique(TF[itr], return_counts=True)}\")\n",
        "            # break\n",
        "\n",
        "    ## Calculate IDF(d, w) = log( (D + alpha)/(C(w) + beta) ) ; C(w) -> Total # docs with word 'w' ; D -> Total # documents\n",
        "    D = num_documents\n",
        "    for itr in range(num_words): ## itereate col-wise\n",
        "        C_w = np.sum(hamming_vectors_2D[:, itr] == 1) ## first calculate C(w)\n",
        "        \n",
        "        IDF[:, itr] = np.log( (D + alpha) / (C_w + beta) )\n",
        "\n",
        "        # break\n",
        "        \n",
        "    TF_IDF = TF*IDF\n",
        "    del TF\n",
        "    # del IDF\n",
        "    return TF_IDF, IDF\n",
        "\n",
        "TF_IDF_whole_corpus, IDF_whole_corpus = compute_TF_IDF_all_train_set(hamming_vectors_2D=hamming_vectors_2D, eucledian_vectors_2D=eucledian_vectors_2D)\n",
        "print(f\"TF_IDF_whole_corpus.shape = {TF_IDF_whole_corpus.shape}\")\n",
        "print(f\"IDF_whole_corpus.shape = {IDF_whole_corpus.shape}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_documents, num_words = (5276, 17506)\n",
            "TF_IDF_whole_corpus.shape = (5276, 17506)\n",
            "IDF_whole_corpus.shape = (1, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWeLmV_094XH",
        "outputId": "4da3fd84-81aa-494c-c2cb-a7bd6dbdf0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def form_TF_IDF_for_val_test(list_words, IDF, hamming_vectors_2D, eucledian_vectors_2D, vocab_dict, alpha=0.0001, beta=0.0001, epslion=0.000001):\n",
        "    num_docs_train, num_words = eucledian_vectors_2D.shape\n",
        "\n",
        "    TF = np.zeros((1, num_words))\n",
        "    \n",
        "    euc_vec = form_eucledian_vector(list_words=list_words, vocab_dict=vocab_dict)\n",
        "    \n",
        "    W_d_num_words_in_document = 0\n",
        "    for word in list_words:\n",
        "        if word in vocab_dict:\n",
        "            W_d_num_words_in_document += 1\n",
        "\n",
        "    TF = euc_vec/(W_d_num_words_in_document + epslion)\n",
        "\n",
        "    TF_IDF = TF*IDF\n",
        "\n",
        "    return TF_IDF\n",
        "\n",
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "\n",
        "TF_IDF = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                    eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "print(f\"TF_IDF.shape = {TF_IDF.shape}\")\n",
        "# print(np.unique(TF_IDF, return_counts=True))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF_IDF.shape = (1, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us7RvCom6UuF",
        "outputId": "f3611258-72d4-4d1a-b5b0-ce15fdba5138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df_val.columns.values)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['content' 'Label' 'stemmed_content' 'ham_vector' 'euc_vector']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohbmnfhCXJfD"
      },
      "source": [
        "# Now we start with K-Nearest Neighbor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBzTc76r7VNj"
      },
      "source": [
        "#### Form validation 2D set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFmtTTRF7Cmw",
        "outputId": "273cbfc9-b67f-4c4f-db9e-dfc91dcf3498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hamming_vectors_2D_validation = get_2D_vector(list_np_arr=df_val['ham_vector'].values)\n",
        "euclidean_vectors_2D_validation = get_2D_vector(list_np_arr=df_val['euc_vector'].values)\n",
        "print(hamming_vectors_2D_validation.shape, euclidean_vectors_2D_validation.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2200, 17506) (2200, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOdAYwV-5iVa",
        "outputId": "7e96badc-4673-4a21-cb94-13b7879fa2b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf_idf_validation_set = np.asarray([\n",
        "    form_TF_IDF_for_val_test(list_words=x, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "        eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001) for x in df_val['stemmed_content'].values\n",
        "])\n",
        "tf_idf_validation_set = tf_idf_validation_set.reshape(tf_idf_validation_set.shape[0], -1)\n",
        "print(f\"tf_idf_validation_set.shape = {tf_idf_validation_set.shape}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_idf_validation_set.shape = (2200, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RN0SomM8JCp"
      },
      "source": [
        "## Similarity functions for 2D vectorized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDbTTsgSDXUT"
      },
      "source": [
        "###### For vectorized ############\n",
        "def ham(a, b):\n",
        "    # return np.count_nonzero((a!=b[:, None]), axis=-1) ## returns 3-D matrix. Don't use.\n",
        "    return pairwise_distances(a, b, metric='euclidean') ## since binary vectors will return hamming-distance\n",
        "\n",
        "def euclidean(a, b):\n",
        "    # return np.linalg.norm((a-b[:, None]), axis=-1)\n",
        "    return pairwise_distances(a, b, metric='euclidean')\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    # return (b.dot(a.T))/(np.linalg.norm(a, axis=1) * np.linalg.norm(b[:, None], axis=-1)) ## (a@b.T).T  ## to get r2*r1\n",
        "    return pairwise_distances(a, b, metric='cosine')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu-Y52OvggBm",
        "cellView": "form"
      },
      "source": [
        "#@title Similarity functions without pairwise-distances\n",
        "####### For single loop ############\n",
        "# ## Similarity functions.\n",
        "# def ham(a, b):\n",
        "#     return np.count_nonzero((a!=b), axis=1)\n",
        "\n",
        "# def euclidean(a, b):\n",
        "#     return np.linalg.norm((a-b), axis=1)\n",
        "\n",
        "# ### https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
        "# def cosine_similarity(a, b):\n",
        "#     cos_sim = np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "#     return cos_sim\n",
        "\n",
        "\n",
        "# a = np.array([x for x in range(40)])\n",
        "# a = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1])\n",
        "# a = a.reshape(5, -1)\n",
        "# print(\"a: \\n\", a)\n",
        "\n",
        "# b = np.array([1, 0, 1])\n",
        "# b = b.reshape(1, -1)\n",
        "# print(\"\\nb: \", b)\n",
        "\n",
        "\n",
        "# # print(ham(a, b))\n",
        "# print(f\"\\nEuclidean(a, b) = {euclidean(a, b)}\")\n",
        "\n",
        "# a = np.array([2, 1, 3, 4, 5, 1, 10, 3, 22])\n",
        "# n = 4\n",
        "# indices_top = (-a).argsort()[:n]\n",
        "# print(f\"\\n n = {n}, indices_top = {indices_top}\")\n",
        "# print(a[indices_top])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnAxbCgrtNyq",
        "outputId": "b693e3e6-ff4a-42ce-bd2c-f292daf25386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "labels_train = df_train['Label'].values\n",
        "print(labels_train.shape)\n",
        "\n",
        "labels_val = df_val['Label'].values\n",
        "print(labels_val.shape)\n",
        "\n",
        "val_euc_0 = euclidean_vectors_2D_validation[0:2]\n",
        "print(val_euc_0.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5276,)\n",
            "(2200,)\n",
            "(2, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GQLpzcgYY_L",
        "outputId": "4737c5a7-e925-4f09-bed7-4ed66b1ae8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "### KNN Vectorized.\n",
        "class KNN:\n",
        "    def __init__(self, mode=\"hamming\", to_print=False):\n",
        "        self.mode = mode\n",
        "        if to_print == True:\n",
        "            print(f\"KNN __init__(mode={mode})\")\n",
        "        \n",
        "\n",
        "    def compute_distances(self, v):\n",
        "        # print(f\"v.shape = {v.shape}\")\n",
        "        if self.mode == \"hamming\":\n",
        "            self.distances = ham(v, self.vectors_corpus)\n",
        "        elif self.mode == \"euclidean\":\n",
        "            self.distances = euclidean(v, self.vectors_corpus)\n",
        "        elif self.mode == 'cosine_similarity':\n",
        "            self.distances = cosine_similarity(v, self.vectors_corpus)\n",
        "\n",
        "        # print(f\"After compute_distances mode={self.mode}, distances.shape = {self.distances.shape}\")\n",
        "        # print(f\"{self.distances}\")\n",
        "\n",
        "    def populate_vectors(self, vectors_corpus, labels):\n",
        "        self.vectors_corpus = vectors_corpus\n",
        "        self.labels = labels\n",
        "\n",
        "    def predict(self, v, num_neighbors, compute_distance_flag=True):\n",
        "        K = num_neighbors\n",
        "        ## compute distances by using suitable similarity function.\n",
        "        if compute_distance_flag == True:\n",
        "            self.compute_distances(v)\n",
        "\n",
        "        ## take argmax top results indices\n",
        "        ## (-v.T).argsort(axis=0)[:K].reshape(-1, )\n",
        "        indices_top = (self.distances.T).argsort(axis=0)[:K] ## highest value is negative distance ? Don't know why, +ve should be taken.\n",
        "\n",
        "        ## apply indices to labels\n",
        "        top_labels = self.labels[indices_top]\n",
        "\n",
        "        ## take majority vote and return the label\n",
        "        top_most_label = stats.mode(top_labels)[0][0]\n",
        "\n",
        "        ## return the max label\n",
        "        return top_most_label\n",
        "\n",
        "################################################# Test #################################################\n",
        "\n",
        "# knn = KNN(mode='hamming')\n",
        "# knn.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "# knn.predict(val_ham_0, num_neighbors=1)\n",
        "\n",
        "knn = KNN(mode='euclidean')\n",
        "knn.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "ans = knn.predict(val_euc_0, num_neighbors=1)\n",
        "print(ans.shape)\n",
        "print(ans)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2,)\n",
            "['Wood_Working' 'Space']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RQnLJLEXI2X",
        "cellView": "form"
      },
      "source": [
        "#@title KNN Non-Vectorized\n",
        "# class KNN:\n",
        "#     def __init__(self, K = 1, mode=\"hamming\", to_print=False):\n",
        "#         self.mode = mode\n",
        "#         self.K = K\n",
        "#         if to_print == True:\n",
        "#             print(f\"KNN __init__(K={K}, mode={mode})\")\n",
        "        \n",
        "\n",
        "#     def populate_vectors(self, vectors_corpus, labels):\n",
        "#         self.vectors_corpus = vectors_corpus\n",
        "#         self.labels = labels\n",
        "\n",
        "#     def compute_distances(self, v):\n",
        "#         if self.mode == \"hamming\":\n",
        "#             v = v.reshape(1, -1) ## Reshape vector.\n",
        "#             self.distances = ham(v, self.vectors_corpus)\n",
        "#         elif self.mode == \"euclidean\":\n",
        "#             v = v.reshape(1, -1) ## Reshape vector.\n",
        "#             self.distances = euclidean(v, self.vectors_corpus)\n",
        "#         elif self.mode == 'cosine_similarity':\n",
        "#             v = v.reshape(-1)\n",
        "#             self.distances = cosine_similarity(v, self.vectors_corpus)\n",
        "\n",
        "#         # print(f\"After compute_distances mode={self.mode}, distances = {self.distances}\")\n",
        "\n",
        "#     def predict(self, v):\n",
        "#         ## compute distances by using suitable similarity function.\n",
        "#         self.compute_distances(v)\n",
        "\n",
        "#         ## take argmax top results indices\n",
        "#         if self.mode == 'cosine_similarity':\n",
        "#             indices_top = (-self.distances).argsort()[:self.K] ## for some reason, this works for cosine-similarity\n",
        "#         else:\n",
        "#             indices_top = (self.distances).argsort()[:self.K] ## THIS works for hamming and euclidean\n",
        "\n",
        "#         ## apply indices to labels\n",
        "#         top_labels = self.labels[indices_top]\n",
        "\n",
        "#         ## take majority vote and return the label\n",
        "#         top_most_label = stats.mode(top_labels)[0][0]\n",
        "\n",
        "#         ## return the max label\n",
        "#         return top_most_label\n",
        "\n",
        "# ################################################# Test #################################################\n",
        "\n",
        "# knn = KNN(K=1, mode='hamming')\n",
        "# knn.populate_vectors(hamming_vectors_2D, labels)\n",
        "# knn.predict(val_ham_0)\n",
        "\n",
        "# # knn = KNN(K=7, mode='euclidean')\n",
        "# # knn.populate_vectors(eucledian_vectors_2D, labels)\n",
        "# # knn.predict(val_euc_0)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9LVZRhU8sle"
      },
      "source": [
        "## Test for one value of K"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-qoC6HLuIdN",
        "outputId": "7d4d5304-2053-46e2-fece-5f0d6394d55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "###### Test for one K #######\n",
        "K = 1\n",
        "knn = KNN(mode='hamming')\n",
        "knn.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "y_preds = knn.predict(hamming_vectors_2D_validation, num_neighbors=K)\n",
        "acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "print(f\"Acc Hamming K = {K} = {acc}%\")\n",
        "\n",
        "# print(f\"Acc Hamming K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")\n",
        "del knn\n",
        "\n",
        "\n",
        "# knn = KNN(mode='euclidean')\n",
        "# knn.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "# y_preds = knn.predict(euclidean_vectors_2D_validation, num_neighbors=K)\n",
        "\n",
        "# print(f\"Acc Euclidean K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")\n",
        "\n",
        "\n",
        "# knn = KNN(mode='cosine_similarity')\n",
        "# knn.populate_vectors(TF_IDF_whole_corpus, labels_train)\n",
        "# y_preds = knn.predict(tf_idf_validation_set, num_neighbors=K)\n",
        "\n",
        "# print(f\"Acc TF-IDF-Cosine K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc Hamming K = 1 = 39.68181818181818%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wzDleMoXe-b"
      },
      "source": [
        "## Applying tests on validation set for KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKPaf3QVXens",
        "outputId": "1570b542-2cb6-4257-9c85-93483e929160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "%%time\n",
        "## Initialize results dataframe\n",
        "column_names = [\"Similarity-Measure\", \"K\", \"Accuracy(%)\"]\n",
        "df_results_knn = pd.DataFrame(columns=column_names)\n",
        "\n",
        "### Initialize objects\n",
        "knn_ham = KNN(mode='hamming')\n",
        "knn_euc = KNN(mode='euclidean')\n",
        "knn_cosine = KNN(mode='cosine_similarity')\n",
        "\n",
        "### Populate initial vectors\n",
        "knn_ham.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "knn_euc.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "knn_cosine.populate_vectors(TF_IDF_whole_corpus, labels_train)\n",
        "\n",
        "### Compute distances for each val-set\n",
        "knn_ham.compute_distances(hamming_vectors_2D_validation)\n",
        "knn_euc.compute_distances(euclidean_vectors_2D_validation)\n",
        "knn_cosine.compute_distances(tf_idf_validation_set)\n",
        "\n",
        "### Predict for each value of K\n",
        "for k in [1, 3, 5]: ## [1, 3, 5, 7, 9, 11, 13, 15]:\n",
        "    print(f\"Predicting for k = {k}\")\n",
        "    ## Predict and Append to dataframe.  \n",
        "    ## Signature: def predict(self, v, num_neighbors, compute_distance_flag=True)\n",
        "    y_preds = knn_ham.predict(hamming_vectors_2D_validation, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = np.sum(labels_val==y_preds)/len(labels_val)*100 ## accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Hamming\", k, acc])\n",
        "    \n",
        "    y_preds = knn_euc.predict(euclidean_vectors_2D_validation, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Euclidean\", k, acc])\n",
        "\n",
        "    y_preds = knn_cosine.predict(tf_idf_validation_set, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"TF-IDF-cosine-sim\", k, acc])\n",
        "    \n",
        "\n",
        "### Delete each objects\n",
        "del knn_ham\n",
        "del knn_euc\n",
        "del knn_cosine\n",
        "\n",
        "### Print\n",
        "print(f\"len df_results_knn = {len(df_results_knn)}\")\n",
        "print(df_results_knn.head(3))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting for k = 1\n",
            "Predicting for k = 3\n",
            "Predicting for k = 5\n",
            "len df_results_knn = 9\n",
            "  Similarity-Measure  K  Accuracy(%)\n",
            "0            Hamming  1    39.681818\n",
            "1          Euclidean  1    55.954545\n",
            "2  TF-IDF-cosine-sim  1    78.863636\n",
            "CPU times: user 1min 19s, sys: 1.02 s, total: 1min 20s\n",
            "Wall time: 45.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvq54JeJXelm",
        "outputId": "50f16d84-5f84-421b-d038-6c910d5846c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "print(f\"len df_results_knn = {len(df_results_knn)}\")\n",
        "display(df_results_knn)\n",
        "df_results_knn.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Using-2D-Pairwise/KNN-val-topics.csv', index=False)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_knn = 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Similarity-Measure</th>\n",
              "      <th>K</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>1</td>\n",
              "      <td>39.681818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>1</td>\n",
              "      <td>55.954545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>1</td>\n",
              "      <td>78.863636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>3</td>\n",
              "      <td>40.045455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>3</td>\n",
              "      <td>54.590909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>3</td>\n",
              "      <td>79.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>5</td>\n",
              "      <td>42.681818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>5</td>\n",
              "      <td>53.590909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>5</td>\n",
              "      <td>80.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Similarity-Measure  K  Accuracy(%)\n",
              "0            Hamming  1    39.681818\n",
              "1          Euclidean  1    55.954545\n",
              "2  TF-IDF-cosine-sim  1    78.863636\n",
              "3            Hamming  3    40.045455\n",
              "4          Euclidean  3    54.590909\n",
              "5  TF-IDF-cosine-sim  3    79.181818\n",
              "6            Hamming  5    42.681818\n",
              "7          Euclidean  5    53.590909\n",
              "8  TF-IDF-cosine-sim  5    80.500000"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K-q5b6ZzqVP",
        "outputId": "c60f99d7-2430-41c5-c067-dfd1c3bdfe3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "mat = df_results_knn.values\n",
        "print(mat.shape)\n",
        "print(mat)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9, 3)\n",
            "[['Hamming' 1 39.68181818181818]\n",
            " ['Euclidean' 1 55.95454545454545]\n",
            " ['TF-IDF-cosine-sim' 1 78.86363636363637]\n",
            " ['Hamming' 3 40.04545454545455]\n",
            " ['Euclidean' 3 54.59090909090909]\n",
            " ['TF-IDF-cosine-sim' 3 79.18181818181819]\n",
            " ['Hamming' 5 42.68181818181818]\n",
            " ['Euclidean' 5 53.590909090909086]\n",
            " ['TF-IDF-cosine-sim' 5 80.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0LcdCK_RW8E"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBwmpIgW4XOD"
      },
      "source": [
        "### Combine all documents of each class i into one document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EagCOsAhzqiu"
      },
      "source": [
        "# unique_labels = np.unique(df_train['Label'].values)\n",
        "# dictionary_list_words_for_NB = {}\n",
        "# # for label in unique_labels:\n",
        "# #     if label not in dictionary_list_words:\n",
        "# #         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "    \n",
        "# for (list_words, label) in zip(df_train['stemmed_content'].values, df_train['Label'].values):\n",
        "#     if label not in dictionary_list_words_for_NB:\n",
        "#         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "#     dictionary_list_words_for_NB[label].append(list_words)\n",
        "\n",
        "# ## reduce/flat-out to make 1D list.\n",
        "# ## https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
        "# for label in dictionary_list_words_for_NB:\n",
        "#     dictionary_list_words_for_NB[label] = reduce(operator.concat, dictionary_list_words_for_NB[label])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkQbU3mCONb"
      },
      "source": [
        "## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "def keywithmaxval(d):\n",
        "     \"\"\" a) create a list of the dict's keys and values; \n",
        "         b) return the key with the max value\"\"\"  \n",
        "     v=list(d.values())\n",
        "     k=list(d.keys())\n",
        "     return k[v.index(max(v))]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAcwIAKOFMJF",
        "outputId": "608f6fd6-24de-4d37-8351-de6774d3a9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self, vocab_size, alpha=0.01, to_print=False):\n",
        "        if to_print == True:\n",
        "            print(f\"NaiveBayes __init(alpha={alpha})__\")\n",
        "        self.alpha = alpha\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dictionary_list_words_for_NB = {}\n",
        "        self.dictionary_prior_probabilities = {}\n",
        "        self.dictionary_count_words_per_class = {}\n",
        "        self.dictionary_total_words_per_class = {}\n",
        "    \n",
        "\n",
        "    def fit(self, list_list_words, labels):\n",
        "        self.list_list_words = list_list_words\n",
        "        self.labels = labels\n",
        "        self.form_dictionary_list_words()\n",
        "        self.compute_probabilities()\n",
        "\n",
        "\n",
        "    def form_dictionary_list_words(self):\n",
        "        for (list_words, label) in zip(self.list_list_words, self.labels):\n",
        "            if label not in self.dictionary_list_words_for_NB:\n",
        "                self.dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "            self.dictionary_list_words_for_NB[label].append(list_words)\n",
        "        \n",
        "        for label in self.dictionary_list_words_for_NB: ## reduce/flat-out to make 1D list.\n",
        "            self.dictionary_list_words_for_NB[label] = reduce(operator.concat, self.dictionary_list_words_for_NB[label])\n",
        "\n",
        "\n",
        "    def compute_probabilities(self):\n",
        "        ## Compute prior probabilities/initial guesses\n",
        "        (classes, cnts) = np.unique(self.labels, return_counts=True)\n",
        "        cnts = cnts/np.sum(cnts) ## C_i / (C_1 + C_2 + ... + C_n)\n",
        "        for (lab, itr) in zip(classes, range(len(classes))):\n",
        "            self.dictionary_prior_probabilities[lab] = cnts[itr]\n",
        "\n",
        "        ## Compute per-word probabilities\n",
        "\n",
        "        ## Counter increment\n",
        "        for lab in classes:\n",
        "            num_words_this_class = 0\n",
        "            self.dictionary_count_words_per_class[lab] = {}\n",
        "            for word in self.dictionary_list_words_for_NB[lab]:\n",
        "                if word not in self.dictionary_count_words_per_class[lab]:\n",
        "                    self.dictionary_count_words_per_class[lab][word] = 0 ## initialize counter to 0.\n",
        "                self.dictionary_count_words_per_class[lab][word] = self.dictionary_count_words_per_class[lab][word] + 1 ## increment counter\n",
        "                num_words_this_class += 1\n",
        "            self.dictionary_total_words_per_class[lab] = num_words_this_class\n",
        "\n",
        "    def predict(self, list_words):\n",
        "        ## Compute probabilities for each label.\n",
        "        dict_probabilities_per_class = {}\n",
        "\n",
        "        for lab in np.unique(self.labels):\n",
        "            prob_log_curr_class = np.log(self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # prob_log_curr_class = (self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # print(f\"Before, prob_log_curr_class = {prob_log_curr_class}\")\n",
        "            for word in list_words: ## iterate per word\n",
        "                if word in self.dictionary_count_words_per_class[lab]: ## if word exists in THIS document.\n",
        "                    ## use smoothing factor alpha\n",
        "                    # prob_log_curr_class += np.log((self.dictionary_count_words_per_class[lab][word] + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size)) \n",
        "                    word_prob = self.dictionary_count_words_per_class[lab][word]\n",
        "                    # prob_log_curr_class = prob_log_curr_class*word_prob\n",
        "                else:\n",
        "                    word_prob = 0\n",
        "                prob_log_curr_class += np.log( (word_prob + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size) )\n",
        "\n",
        "\n",
        "            dict_probabilities_per_class[lab] = prob_log_curr_class\n",
        "            # print(dict_probabilities_per_class)\n",
        "\n",
        "        ## Get max probability class.\n",
        "        ## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "        return keywithmaxval(d=dict_probabilities_per_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################### Checking ############################################################\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=len(dictionary_vocab))\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "check = df_val['stemmed_content'].iloc[0]\n",
        "p = NB.predict(check)\n",
        "print(p)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uILDNAL3FddD",
        "outputId": "cc54c9df-1373-4bed-f9c9-732e922ed8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list_new = [\n",
        "['coffee', 'tea', 'dew', 'dew', 'dew', 'dew'],\n",
        "['coffee', 'noir', 'homelander', 'dew'],\n",
        "['noir', 'noir', 'fool', 'fool', 'noir']\n",
        "]\n",
        "\n",
        "labs_new = [\n",
        "    'bev',\n",
        "    'supe',\n",
        "    'misc'\n",
        "]\n",
        "\n",
        "vocab_size = 6\n",
        "\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=6)\n",
        "NB.fit(list_new, labs_new)\n",
        "print(NB.predict(['coffee', 'tea', 'dew', 'dew', 'dew', 'dew']))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f13yqf6EKAaJ"
      },
      "source": [
        "## Validation on NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgWpyxjCFMVM",
        "outputId": "93410cdc-47ea-42a3-d715-e554c5a4bbc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"ALPHA\", \"Accuracy(%)\"]\n",
        "\n",
        "df_results_NB = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_results_NB.head())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ALPHA, Accuracy(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5FaA8KG8zu",
        "outputId": "f0ae6490-0b30-4e67-f995-f43b8edda7a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "alpha_values = np.linspace(start=0.01, stop=1.0, num=10)\n",
        "print(alpha_values)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01 0.12 0.23 0.34 0.45 0.56 0.67 0.78 0.89 1.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rC01YFFMRn",
        "outputId": "f159af13-a56e-472f-9e36-fea2b4bbd339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "### Validation NaiveBayes ###\n",
        "for alpha in (alpha_values):\n",
        "    NB = NaiveBayes(alpha=alpha, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "    NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "    nb_acc = 0\n",
        "    ## Predict each val set ##\n",
        "    for (x, y) in (zip(df_val['stemmed_content'].values, df_val['Label'].values)):\n",
        "        y_pred = NB.predict(x)\n",
        "        if y_pred == y:\n",
        "            nb_acc += 1\n",
        "\n",
        "\n",
        "    ## Append to dataframe and print.\n",
        "    # print(f\"NB alpha = {alpha}, accuracy = {nb_acc/len(df_val)*100} %\")\n",
        "    df_results_NB = add_to_dataframe(df_old=df_results_NB, to_add=[alpha, nb_acc/len(df_val)*100])\n",
        "    del NB"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.01)__\n",
            "NaiveBayes __init(alpha=0.12)__\n",
            "NaiveBayes __init(alpha=0.23)__\n",
            "NaiveBayes __init(alpha=0.34)__\n",
            "NaiveBayes __init(alpha=0.45)__\n",
            "NaiveBayes __init(alpha=0.56)__\n",
            "NaiveBayes __init(alpha=0.67)__\n",
            "NaiveBayes __init(alpha=0.78)__\n",
            "NaiveBayes __init(alpha=0.89)__\n",
            "NaiveBayes __init(alpha=1.0)__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbwK_kMxTY",
        "outputId": "fade612a-1693-4fac-edb8-56352ac2f653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "df_results_NB.sort_values(by=['Accuracy(%)'], inplace=True, ascending=False)\n",
        "display(df_results_NB.head(10))\n",
        "# df_results_NB.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Using-2D-Pairwise/NB-val-topics.csv', index=False)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.23</td>\n",
              "      <td>88.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.12</td>\n",
              "      <td>88.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.34</td>\n",
              "      <td>88.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.45</td>\n",
              "      <td>88.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.56</td>\n",
              "      <td>88.227273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.67</td>\n",
              "      <td>88.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.78</td>\n",
              "      <td>88.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.89</td>\n",
              "      <td>87.954545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.00</td>\n",
              "      <td>87.818182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ALPHA  Accuracy(%)\n",
              "0   0.01    88.409091\n",
              "2   0.23    88.363636\n",
              "1   0.12    88.318182\n",
              "3   0.34    88.318182\n",
              "4   0.45    88.272727\n",
              "5   0.56    88.227273\n",
              "6   0.67    88.090909\n",
              "7   0.78    88.000000\n",
              "8   0.89    87.954545\n",
              "9   1.00    87.818182"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4G-LMq-nUTK"
      },
      "source": [
        "# Hypothesis testing on Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msSFrab2iWbT"
      },
      "source": [
        "del hamming_vectors_2D_validation\n",
        "del euclidean_vectors_2D_validation\n",
        "del tf_idf_validation_set"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG7zmcwEnbbT"
      },
      "source": [
        "### Create and fit best performing models on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjLPICwjpSdJ",
        "outputId": "013da236-ee5f-4b81-8693-7eed819565d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "print(words_val)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sagul', 'give', 'deflect', 'valu', 'horizont', 'shelf', 'span', 'use', 'variou', 'materi', 'thick', 'unless', 'work', 'extrem', 'heavi', 'load', 'weight', 'countertop', 'peopl', 'danc', 'materi', 'vertic', 'cabinet', 'wall', 'adequ', 'make', 'wall', 'thinner', 'still', 'remain', 'surprisingli', 'strong', 'long', 'cabinet', 'design', 'prevent', 'rack', 'even', 'particleboard', 'known', 'strength', 'hold', 'well', 'vertic', 'compressionbuckl', 'may', 'know', 'youv', 'ever', 'pack', 'book', 'store', 'cheap', 'particleboard', 'bookcas']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaPA10d8na_G",
        "outputId": "198ba70a-0d3c-432a-d273-2f40fd3f6875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##### KNN was K=5, TF-IDF #####\n",
        "K_best = 5 ## 15\n",
        "knn = KNN(mode='cosine_similarity', to_print=True)\n",
        "knn.populate_vectors(TF_IDF_whole_corpus, labels_train)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN __init__(mode=cosine_similarity)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCgX5cNKpard"
      },
      "source": [
        "# val_tf_idf = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "#                 eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "# print(knn.predict(val_tf_idf, num_neighbors=5))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOeVWWRdna3D",
        "outputId": "bb457e1c-e831-4b63-9dee-d334a454f4af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "##### Naive Bayes was alpha = 0.02/0.04, we will take 0.04 #####\n",
        "NB = NaiveBayes(alpha=0.04, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.04)__\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-3ee62855362e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##### Naive Bayes was alpha = 0.02/0.04, we will take 0.04 #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_list_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stemmed_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHqRiSQYpfEB",
        "outputId": "6a1519eb-8912-488d-e0cb-eef3931fc561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "print(NB.predict(words_val))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-a4f3c1a91aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-4af91bb4beb8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, list_words)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdict_probabilities_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mprob_log_curr_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_prior_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## start with prior probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# prob_log_curr_class = (self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NaiveBayes' object has no attribute 'labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuz4R0PDoZsy"
      },
      "source": [
        "del df_train\n",
        "del df_val"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7ms6GZhm4CM"
      },
      "source": [
        "### Split test dataset 50 iterations per 10 of each topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er7I3xb2jxbS",
        "outputId": "f051b306-b1b0-48a4-b8ed-9b2fb57d8e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df_test.columns.values)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['content' 'Label' 'stemmed_content' 'ham_vector' 'euc_vector']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCLTyO1nn34U"
      },
      "source": [
        "df_test.sort_values(by=['Label'], inplace=True)\n",
        "df_test.drop(labels=['content', 'ham_vector', 'euc_vector'], axis=1, inplace=True)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08aKywOkullM"
      },
      "source": [
        "column_names = [\"KNN-Acc(%)\", \"NB-Acc(%)\"]\n",
        "\n",
        "df_results_test_set = pd.DataFrame(columns=column_names)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU6c7hr920ir"
      },
      "source": [
        "## Faster prediction of test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0ZN_w3t7qr9",
        "outputId": "67270779-00c6-4d94-fb75-03e3a5e1aebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"K_best = {K_best}\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K_best = 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3CIkX-Z2w65",
        "outputId": "75fb531c-e062-4963-c146-cc979247b4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "### For KNN ###\n",
        "tf_idf_test_set = np.asarray([\n",
        "    form_TF_IDF_for_val_test(list_words=x, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "        eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001) for x in df_test['stemmed_content'].values\n",
        "])\n",
        "tf_idf_test_set = tf_idf_test_set.reshape(tf_idf_test_set.shape[0], -1)\n",
        "print(f\"tf_idf_test_set.shape = {tf_idf_test_set.shape}\")\n",
        "\n",
        "y_preds_knn = knn.predict(tf_idf_test_set, num_neighbors=K_best)\n",
        "print(f\"y_preds_knn.shape = {y_preds_knn.shape}\")"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_idf_test_set.shape = (5500, 17506)\n",
            "y_preds_knn.shape = (5500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khRLhB1m7GvK"
      },
      "source": [
        "del tf_idf_test_set\n",
        "del knn"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNvVB7hc2w_n",
        "outputId": "cf67fa6b-ee3d-442a-eb8c-4b2d1738db36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### For Naive Bayes ###\n",
        "y_preds_nb = []\n",
        "for x in df_test['stemmed_content'].values:\n",
        "    y_preds_nb.append( NB.predict(x) )\n",
        "y_preds_nb = np.array(y_preds_nb)\n",
        "print(f\"y_preds_nb.shape = {y_preds_nb.shape}\")\n",
        "del NB"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_preds_nb.shape = (5500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKRXXlxv2wwX"
      },
      "source": [
        "y_test = df_test['Label'].values"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiSb_eb8AZV-"
      },
      "source": [
        ""
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cah_z5aJ8Bcu",
        "outputId": "bcf6fbfa-529a-44e9-fd61-634cd962a2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_docs_in_each_topic = 500 ##\n",
        "initial_offset = np.array([i*num_docs_in_each_topic for i in range(0, 11)])\n",
        "for counter_test in range(0, 50): ## run iterations 50 times\n",
        "    offsets = initial_offset + counter_test*10\n",
        "    start_indices = offsets\n",
        "    end_indices = offsets + 10\n",
        "    \n",
        "    print(\"\\nCounter = \", counter_test)\n",
        "    # print(offsets)\n",
        "    # print(start_indices)\n",
        "    # print(end_indices)\n",
        "\n",
        "    ### Testing here ###\n",
        "    nb_correct = knn_correct = 0\n",
        "    for i in range(len(start_indices)): ## add all to list.\n",
        "        ## print(np.unique(df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values, return_counts=True), end=' ')\n",
        "        ## for (x, y) in zip(df_test.iloc[start_indices[i]:end_indices[i]]['stemmed_content'].values, df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values):\n",
        "        for y, y_knn, y_nb in zip(y_test[start_indices[i]:end_indices[i]], y_preds_knn[start_indices[i]:end_indices[i]], y_preds_nb[start_indices[i]:end_indices[i]]):\n",
        "            if y_nb == y:\n",
        "                nb_correct += 1\n",
        "            if y_knn == y:\n",
        "                knn_correct += 1\n",
        "\n",
        "    NUM_DOCUMENTS = 10* len(np.unique(y_test))\n",
        "    knn_acc = knn_correct/( NUM_DOCUMENTS )*100  ## 10*11 total 110 test documents per iteration. [50 iterations]\n",
        "    nb_acc = nb_correct/( NUM_DOCUMENTS )*100\n",
        "    print(f\"KNN-Acc = {knn_acc}%, NB-Acc = {nb_acc}%\")\n",
        "\n",
        "    df_results_test_set = add_to_dataframe(df_old=df_results_test_set, to_add=[knn_acc, nb_acc])\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Counter =  0\n",
            "KNN-Acc = 86.36363636363636%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  1\n",
            "KNN-Acc = 75.45454545454545%, NB-Acc = 84.54545454545455%\n",
            "\n",
            "Counter =  2\n",
            "KNN-Acc = 76.36363636363637%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  3\n",
            "KNN-Acc = 85.45454545454545%, NB-Acc = 90.0%\n",
            "\n",
            "Counter =  4\n",
            "KNN-Acc = 80.9090909090909%, NB-Acc = 91.81818181818183%\n",
            "\n",
            "Counter =  5\n",
            "KNN-Acc = 78.18181818181819%, NB-Acc = 86.36363636363636%\n",
            "\n",
            "Counter =  6\n",
            "KNN-Acc = 72.72727272727273%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  7\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 91.81818181818183%\n",
            "\n",
            "Counter =  8\n",
            "KNN-Acc = 77.27272727272727%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  9\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  10\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 90.0%\n",
            "\n",
            "Counter =  11\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 92.72727272727272%\n",
            "\n",
            "Counter =  12\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  13\n",
            "KNN-Acc = 80.9090909090909%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  14\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 90.0%\n",
            "\n",
            "Counter =  15\n",
            "KNN-Acc = 80.0%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  16\n",
            "KNN-Acc = 77.27272727272727%, NB-Acc = 91.81818181818183%\n",
            "\n",
            "Counter =  17\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 86.36363636363636%\n",
            "\n",
            "Counter =  18\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  19\n",
            "KNN-Acc = 86.36363636363636%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  20\n",
            "KNN-Acc = 74.54545454545455%, NB-Acc = 85.45454545454545%\n",
            "\n",
            "Counter =  21\n",
            "KNN-Acc = 80.0%, NB-Acc = 87.27272727272727%\n",
            "\n",
            "Counter =  22\n",
            "KNN-Acc = 86.36363636363636%, NB-Acc = 92.72727272727272%\n",
            "\n",
            "Counter =  23\n",
            "KNN-Acc = 86.36363636363636%, NB-Acc = 94.54545454545455%\n",
            "\n",
            "Counter =  24\n",
            "KNN-Acc = 80.0%, NB-Acc = 83.63636363636363%\n",
            "\n",
            "Counter =  25\n",
            "KNN-Acc = 78.18181818181819%, NB-Acc = 93.63636363636364%\n",
            "\n",
            "Counter =  26\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 94.54545454545455%\n",
            "\n",
            "Counter =  27\n",
            "KNN-Acc = 80.0%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  28\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 94.54545454545455%\n",
            "\n",
            "Counter =  29\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  30\n",
            "KNN-Acc = 77.27272727272727%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  31\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 92.72727272727272%\n",
            "\n",
            "Counter =  32\n",
            "KNN-Acc = 80.9090909090909%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  33\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 87.27272727272727%\n",
            "\n",
            "Counter =  34\n",
            "KNN-Acc = 84.54545454545455%, NB-Acc = 91.81818181818183%\n",
            "\n",
            "Counter =  35\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 95.45454545454545%\n",
            "\n",
            "Counter =  36\n",
            "KNN-Acc = 79.0909090909091%, NB-Acc = 86.36363636363636%\n",
            "\n",
            "Counter =  37\n",
            "KNN-Acc = 80.0%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  38\n",
            "KNN-Acc = 75.45454545454545%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  39\n",
            "KNN-Acc = 79.0909090909091%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  40\n",
            "KNN-Acc = 77.27272727272727%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  41\n",
            "KNN-Acc = 69.0909090909091%, NB-Acc = 82.72727272727273%\n",
            "\n",
            "Counter =  42\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 92.72727272727272%\n",
            "\n",
            "Counter =  43\n",
            "KNN-Acc = 84.54545454545455%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  44\n",
            "KNN-Acc = 80.0%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  45\n",
            "KNN-Acc = 77.27272727272727%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  46\n",
            "KNN-Acc = 86.36363636363636%, NB-Acc = 90.0%\n",
            "\n",
            "Counter =  47\n",
            "KNN-Acc = 80.9090909090909%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  48\n",
            "KNN-Acc = 85.45454545454545%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  49\n",
            "KNN-Acc = 73.63636363636363%, NB-Acc = 87.27272727272727%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcIeiN3r8Bgb",
        "outputId": "464429c4-b192-416f-91e7-a8a368c91862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "display(df_results_test_set)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>75.454545</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>76.363636</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>72.727273</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>74.545455</td>\n",
              "      <td>85.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>83.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>93.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>95.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>79.090909</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>75.454545</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>79.090909</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>69.090909</td>\n",
              "      <td>82.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>73.636364</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    KNN-Acc(%)  NB-Acc(%)\n",
              "0    86.363636  88.181818\n",
              "1    75.454545  84.545455\n",
              "2    76.363636  90.909091\n",
              "3    85.454545  90.000000\n",
              "4    80.909091  91.818182\n",
              "5    78.181818  86.363636\n",
              "6    72.727273  90.909091\n",
              "7    81.818182  91.818182\n",
              "8    77.272727  90.909091\n",
              "9    81.818182  89.090909\n",
              "10   81.818182  90.000000\n",
              "11   81.818182  92.727273\n",
              "12   81.818182  88.181818\n",
              "13   80.909091  90.909091\n",
              "14   83.636364  90.000000\n",
              "15   80.000000  88.181818\n",
              "16   77.272727  91.818182\n",
              "17   83.636364  86.363636\n",
              "18   81.818182  90.909091\n",
              "19   86.363636  90.909091\n",
              "20   74.545455  85.454545\n",
              "21   80.000000  87.272727\n",
              "22   86.363636  92.727273\n",
              "23   86.363636  94.545455\n",
              "24   80.000000  83.636364\n",
              "25   78.181818  93.636364\n",
              "26   82.727273  94.545455\n",
              "27   80.000000  89.090909\n",
              "28   83.636364  94.545455\n",
              "29   82.727273  90.909091\n",
              "30   77.272727  89.090909\n",
              "31   82.727273  92.727273\n",
              "32   80.909091  89.090909\n",
              "33   82.727273  87.272727\n",
              "34   84.545455  91.818182\n",
              "35   81.818182  95.454545\n",
              "36   79.090909  86.363636\n",
              "37   80.000000  90.909091\n",
              "38   75.454545  90.909091\n",
              "39   79.090909  89.090909\n",
              "40   77.272727  88.181818\n",
              "41   69.090909  82.727273\n",
              "42   83.636364  92.727273\n",
              "43   84.545455  89.090909\n",
              "44   80.000000  88.181818\n",
              "45   77.272727  89.090909\n",
              "46   86.363636  90.000000\n",
              "47   80.909091  88.181818\n",
              "48   85.454545  89.090909\n",
              "49   73.636364  87.272727"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtHCDNs92xxx"
      },
      "source": [
        "## Predictions using normal for each example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNcU7hdTn3tc"
      },
      "source": [
        "# num_docs_in_each_topic = 500 ##\n",
        "# initial_offset = np.array([i*num_docs_in_each_topic for i in range(0, 11)])\n",
        "# for counter_test in range(0, 50): ## run iterations 50 times\n",
        "#     offsets = initial_offset + counter_test*10\n",
        "#     start_indices = offsets\n",
        "#     end_indices = offsets + 10\n",
        "    \n",
        "#     print(\"\\nCounter = \", counter_test)\n",
        "#     # print(offsets)\n",
        "#     # print(start_indices)\n",
        "#     # print(end_indices)\n",
        "\n",
        "#     ### Testing here ###\n",
        "#     nb_correct = knn_correct = 0\n",
        "#     for i in range(len(start_indices)): ## add all to list.\n",
        "#         # print(np.unique(df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values, return_counts=True), end=' ')\n",
        "#         for (x, y) in zip(df_test.iloc[start_indices[i]:end_indices[i]]['stemmed_content'].values, df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values):\n",
        "            \n",
        "#             #### For KNN ####\n",
        "#             x_tf_idf = form_TF_IDF_for_val_test(list_words=x, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "#                 eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "#             y_pred = knn.predict(x_tf_idf, num_neighbors=K_best)\n",
        "#             if y_pred == y:\n",
        "#                 knn_correct += 1\n",
        "            \n",
        "#             #### For Naive Bayes #####\n",
        "#             y_pred = NB.predict(x)\n",
        "#             if y_pred == y:\n",
        "#                 nb_correct += 1\n",
        "            \n",
        "    \n",
        "#     print(f\"Done for counter_test = {counter_test}\")\n",
        "\n",
        "#     NUM_DOCUMENTS = 10* len(np.unique(df_test['Label'].values))\n",
        "#     knn_acc = knn_correct/( NUM_DOCUMENTS )*100  ## 10*11 total 110 test documents per iteration. [50 iterations]\n",
        "#     nb_acc = nb_correct/( NUM_DOCUMENTS )*100\n",
        "#     print(f\"KNN-Acc = {knn_acc}%, NB-Acc = {nb_acc}%\")\n",
        "\n",
        "#     df_results_test_set = add_to_dataframe(df_old=df_results_test_set, to_add=[knn_acc, nb_acc])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrDQX97qrchw"
      },
      "source": [
        "df_results_test_set.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Using-2D-Pairwise/Test-Set-KNN-K=5-NB.csv', index=False)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkTag67arcpn",
        "outputId": "bf70d76b-1c64-4b56-9753-d4338b0918d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"len df_results_test_set = {len(df_results_test_set)}\")\n",
        "# display(df_results_test_set)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_test_set = 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBHCUUtF2Kz8"
      },
      "source": [
        "## Load and analyze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dee3qAsxrcg6",
        "outputId": "a19b0135-f343-4053-857f-fd289cefd6df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "# df_results_test_set = pd.read_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Test-Set-KNN-NB-with-K-equal-5.csv')\n",
        "print(df_results_test_set.head(5))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-bf4bbee2b437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_results_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Test-Set-KNN-NB-with-K-equal-5.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_results_test_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Test-Set-KNN-NB-with-K-equal-5.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4tqi3TS3LH0",
        "outputId": "1c811e01-937c-470b-e97a-38f7dc4d7cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "knn_acc = df_results_test_set['KNN-Acc(%)'].values\n",
        "nb_acc = df_results_test_set['NB-Acc(%)'].values\n",
        "print(knn_acc.shape, nb_acc.shape)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50,) (50,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDJKAptY5mEK",
        "outputId": "54fe36c8-eb8e-4a1c-a7b8-fc560dc32dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "column_names = [\"Method\", \"Mean-Acc(%)\", \"Std-dev\", \"Std-Error\", \"Minimum-Acc(%)\", \"Maximum-Acc(%)\"]\n",
        "\n",
        "df_stats = pd.DataFrame(columns=column_names)\n",
        "\n",
        "## Summarized results.\n",
        "df_stats = add_to_dataframe(df_old=df_stats, to_add=[\"KNN K=5, TF-IDF\", np.mean(knn_acc), np.std(knn_acc), stats.sem(knn_acc, axis=None, ddof=0), min(knn_acc), max(knn_acc)])\n",
        "df_stats = add_to_dataframe(df_old=df_stats, to_add=[\"NB alpha = 0.04\", np.mean(nb_acc), np.std(nb_acc), stats.sem(nb_acc, axis=None, ddof=0), min(nb_acc), max(nb_acc)])\n",
        "\n",
        "print(df_stats)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Method  Mean-Acc(%)  ...  Minimum-Acc(%)  Maximum-Acc(%)\n",
            "0  KNN K=5, TF-IDF    80.636364  ...       69.090909       86.363636\n",
            "1  NB alpha = 0.04    89.763636  ...       82.727273       95.454545\n",
            "\n",
            "[2 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYKuEjwr7vdM"
      },
      "source": [
        "## Computing T-statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyDAasE1RozN"
      },
      "source": [
        "knn_acc = df_results_test_set['KNN-Acc(%)'].values\n",
        "nb_acc = df_results_test_set['NB-Acc(%)'].values"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBFks-IP64CR",
        "outputId": "5a3d905d-7ff5-47ea-9fc1-dca74aac4954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Using K = 5\n",
        "\n",
        "## https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
        "\n",
        "# ans = stats.ttest_rel(nb_acc, knn_acc)\n",
        "# ans = stats.ttest_ind(nb_acc, knn_acc, equal_var=True)\n",
        "ans = stats.ttest_rel(nb_acc, knn_acc)\n",
        "print(ans)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ttest_relResult(statistic=17.570878565892993, pvalue=8.56639952050839e-23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNcoJHAZ64EK",
        "outputId": "eb05382b-7de1-4ea0-c5a9-5b6ee001a68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Using K = 15\n",
        "\n",
        "## https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
        "\n",
        "# ans = stats.ttest_rel(nb_acc, knn_acc)\n",
        "# ans = stats.ttest_ind(nb_acc, knn_acc, equal_var=True)\n",
        "ans = stats.ttest_rel(nb_acc, knn_acc)\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ttest_relResult(statistic=14.245213763349957, pvalue=4.766189926008192e-19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI990uMi634Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}