{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Offline-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrnubMIEG0BZ"
      },
      "source": [
        "### MOUNT DRIVE ....."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1oJyY78cXnq"
      },
      "source": [
        "## First Import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import pickle\n",
        "\n",
        "from scipy import stats\n",
        "from xml.dom import minidom"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFLKkx6Ybnlr"
      },
      "source": [
        "FOLDER_TRAIN = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/Training/'\n",
        "# FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics.txt'\n",
        "FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics-all.txt'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9hrTtEZp2A8"
      },
      "source": [
        "# !ls \"$FOLDER_TRAIN\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0bZ4kTHcXql"
      },
      "source": [
        "## Set random seed\n",
        "RANDOM_STATE = 22\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThQtktwcXtX",
        "outputId": "946a1a47-2950-4103-fa9b-b77cd14b3b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%%time\n",
        "## Read data\n",
        "topic_remove = '3d_Printer' ## Remove 3D-printer\n",
        "list_doc_types = []\n",
        "with open(FILE_TOPICS, 'r') as fp:\n",
        "    topic = fp.readline()\n",
        "    while topic:\n",
        "        topic = topic.replace(\"\\n\", \"\")\n",
        "        list_doc_types.append(topic)\n",
        "        topic = fp.readline()\n",
        "\n",
        "list_doc_types.remove(topic_remove)\n",
        "print(list_doc_types)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Astronomy', 'Coffee', 'Space', 'Anime', 'Biology', 'Cooking', 'Windows_Phone', 'Arduino', 'Chess', 'Law', 'Wood_Working']\n",
            "CPU times: user 640 µs, sys: 790 µs, total: 1.43 ms\n",
            "Wall time: 1.67 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfC2PYXxg35A",
        "outputId": "c00d15e1-b92e-414d-82e0-d2ed7157ffdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(list_doc_types)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Astronomy', 'Coffee', 'Space', 'Anime', 'Biology', 'Cooking', 'Windows_Phone', 'Arduino', 'Chess', 'Law', 'Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVPfrYjagVv4"
      },
      "source": [
        "def get_train_val_test_data(file_name):\n",
        "    xmldoc = minidom.parse(file_name)\n",
        "    xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "    \n",
        "    # print(f\"Inside get_train_val_test_data(), len(item_list) = {len(item_list)}\")\n",
        "\n",
        "    item_list = [x.attributes['Body'].value for x in xml_list]\n",
        "\n",
        "    train_list = item_list[0:500] ## first 500 train\n",
        "    val_list =  item_list[500:700] ## next 200 val\n",
        "    test_list = item_list[700:1200] ## next 500 test\n",
        "\n",
        "    ## delete original list.\n",
        "    del item_list\n",
        "    del xmldoc\n",
        "\n",
        "    ## return the new lists\n",
        "    return train_list, val_list, test_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvop_yNmUCMo"
      },
      "source": [
        "def add_to_dataframe(df_old, to_add):\n",
        "    df_old = df_old.append(pd.Series(to_add, index=df_old.columns), ignore_index=True)\n",
        "    return df_old"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MJXly5bt7aC"
      },
      "source": [
        "## Create three dataframes.\n",
        "\n",
        "## https://www.kite.com/python/answers/how-to-create-an-empty-dataframe-with-column-names-in-python\n",
        "column_names =[\"content\", \"Label\"]\n",
        "\n",
        "df_train = pd.DataFrame(columns = column_names)\n",
        "df_val = pd.DataFrame(columns = column_names)\n",
        "df_test = pd.DataFrame(columns = column_names)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxZHmfGt1g9e",
        "outputId": "a093ceec-e53e-4e3d-ac40-7188fe547801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "%%time\n",
        "### Populate all topics\n",
        "def populate_data_frames(df_train, df_val, df_test, list_doc_types):\n",
        "    for topic in list_doc_types: ## iterating per topic/label\n",
        "        label = topic ## assign label\n",
        "        \n",
        "        ## read using xml package\n",
        "        file_name = FOLDER_TRAIN + topic + \".xml\"\n",
        "        xmldoc = minidom.parse(file_name)\n",
        "        xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "\n",
        "        ## get train, val, test lists\n",
        "        train_list, val_list, test_list = get_train_val_test_data(file_name=file_name)\n",
        "\n",
        "        print(len(train_list), len(val_list), len(test_list))\n",
        "\n",
        "        ## add to dataframe\n",
        "        for v1 in train_list:\n",
        "            df_train = add_to_dataframe(df_old=df_train, to_add=[v1, label])\n",
        "        for v2 in val_list:\n",
        "            df_val = add_to_dataframe(df_old=df_val, to_add=[v2, label])\n",
        "        for v3 in test_list:\n",
        "            df_test = add_to_dataframe(df_old=df_test, to_add=[v3, label])\n",
        "\n",
        "\n",
        "        ## delete original list\n",
        "        del train_list\n",
        "        del val_list\n",
        "        del test_list\n",
        "\n",
        "    ## return dataframes\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "### Call the function\n",
        "df_train, df_val, df_test = populate_data_frames(df_train=df_train, df_val=df_val, df_test=df_test, list_doc_types=list_doc_types)\n",
        "\n",
        "print(f\"len df_train = {len(df_train)}\")\n",
        "print(f\"len df_val = {len(df_val)}\")\n",
        "print(f\"len df_test = {len(df_test)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "len df_train = 5500\n",
            "len df_val = 2200\n",
            "len df_test = 5500\n",
            "CPU times: user 1min 4s, sys: 1.56 s, total: 1min 5s\n",
            "Wall time: 1min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUI07UZKmPc"
      },
      "source": [
        "## Shuffle dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rcv33P3XXuN"
      },
      "source": [
        "df_train = df_train.sample(frac=1, random_state=RANDOM_STATE)\n",
        "df_val = df_val.sample(frac=1, random_state=RANDOM_STATE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1m0_Du8XYBK"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKSziS4PvdnZ"
      },
      "source": [
        "## https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
        "## https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "## from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIukiiJxDWdu",
        "outputId": "0541824e-451b-4e04-9cf8-62eb428cc037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "## https://stackoverflow.com/questions/26693736/nltk-and-stopwords-fail-lookuperror\n",
        "## https://stackoverflow.com/questions/26570944/resource-utokenizers-punkt-english-pickle-not-found\n",
        "\n",
        "# nltk.download()\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUsCIVJru269"
      },
      "source": [
        "## Preprocess each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-MlUQ4w1uM2",
        "cellView": "both",
        "outputId": "ae0c3bb6-2cff-4a40-93e7-2db41aa34bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "## https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "## First lemmatize, and then stem.\n",
        "\n",
        "def pre_process_stem_sentence(sentence, stop_words, stemmer, lemmatizer):\n",
        "    ## save in another variable\n",
        "    text = sentence\n",
        "    ## remove HTML tags [using soup]\n",
        "    soup = BeautifulSoup(text)\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    ## remove <a href> type things\n",
        "    soup = BeautifulSoup(text) ## create soup again.\n",
        "    for a in soup.findAll('a'):\n",
        "        a.replaceWithChildren()\n",
        "    \n",
        "    text = str(soup) ## reform text\n",
        "\n",
        "\n",
        "    ## remove unicode.\n",
        "    text = re.sub(r\"&nbsp;\", \" \", text)\n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
        "\n",
        "    ## remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"https\\S+\", \"\", text)\n",
        "    ## text = re.sub(r\"www\\S+\", \"\", text)\n",
        "\n",
        "    ## to be safe, remove HTML tags again using regex\n",
        "    #### https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44  \n",
        "    clean = re.compile('<.*?>')\n",
        "    text = re.sub(clean, '', text)\n",
        "\n",
        "    ## convert to small letters.\n",
        "    text = text.lower()\n",
        "\n",
        "    ## replace newlines, tabs -> SPACE\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "\n",
        "    ## {COLON} make <a>:<b> become <a>[space]<b>\n",
        "    # text = text.replace(\": \", \":\") # :[space] -> : \n",
        "    text = text.replace(\":\", \" \")  # : -> [space]\n",
        "    \n",
        "    ## {HYPHEN}\n",
        "    # text = text.replace(\"- \", \"-\")\n",
        "    text = text.replace(\"-\", \" \")\n",
        "\n",
        "    \n",
        "    ## [remove anything EXCEPT english letters]\n",
        "    ## https://stackoverflow.com/questions/6323296/python-remove-anything-that-is-not-a-letter-or-number\n",
        "    text = re.sub(  \"[^a-z ]\",              # Anything except 0..9, a..z and A..Z\n",
        "                    \"\",                     # replaced with nothing\n",
        "                    text)                   # in this string   \n",
        "\n",
        "    ## remove space initially and finally.\n",
        "    text = text.lstrip()\n",
        "\n",
        "    ## make double spaces become one space.\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    ## remove stop words (English).\n",
        "    # print(f\"Before stopwords removal, text.len = {len(text)}\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words_removed_words_list = [t for t in tokens if not t in stop_words]\n",
        "\n",
        "    # print(f\"After tokenize and stopwords, len stop_words_removed_words_list = {len(stop_words_removed_words_list)}\")\n",
        "\n",
        "    ## apply lemmatization.\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stop_words_removed_words_list]\n",
        "    \n",
        "    ## apply stemming.\n",
        "    stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
        "\n",
        "    ## return the final pre-processed list-of-words.\n",
        "    return stemmed_words\n",
        "\n",
        "################ Test on one sentence #######################\n",
        "\n",
        "## Obtain once.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "## Preprocess per element of dataframe to test.\n",
        "text_to_process = df_train['content'].iloc[499]\n",
        "print(text_to_process)\n",
        "\n",
        "print(\"--\"*90)\n",
        "\n",
        "preprocessed_text = pre_process_stem_sentence(sentence=text_to_process, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(preprocessed_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<h2>You never mentioned your tablet model... or Arduino model</h2>\n",
            "\n",
            "<p>There are some tablets designed to work with USB flash drives and printers. This is called OTG (on the go.</p>\n",
            "\n",
            "<p><strong>You need:</strong></p>\n",
            "\n",
            "<ul>\n",
            "<li>A tablet supporting OTG</li>\n",
            "<li>An OTG adapter that fits into your tablets USB slot and is compatible</li>\n",
            "<li><a href=\"https://play.google.com/store/apps/details?id=com.primavera.arduino.listener\" rel=\"nofollow\">The Arduino Uno Communicator App</a></li>\n",
            "<li>Arduino Uno (or clone) [Note: It says that it works with Atmega16U2 or Atmega8U2 programmed as a USB-to-serial converter so I would assume that that would cover a few boards more than the Uno.)</li>\n",
            "</ul>\n",
            "\n",
            "<p>Another alternitive is to look into <a href=\"https://www.google.com/search?q=arduino+bluetooth&amp;qscrl=1&amp;tbm=isch&amp;imgil=WyPC4O05Xn4rhM%253A%253Bhttps%253A%252F%252Fencrypted-tbn2.gstatic.com%252Fimages%253Fq%253Dtbn%253AANd9GcSfTtWqIuN9YYdd90OO_tlxsvLUYhV_pl7NfaMhHDzSRq9N6W9rNA%253B1200%253B1200%253B1soQc4CzMzh0MM%253Bhttp%25253A%25252F%25252Fm.dhgate.com%25252Fproduct%25252Fjy-mcu-arduino-bluetooth-wireless-serial%25252F151048482.html&amp;source=iu&amp;usg=__KyMLS4aeubxJnDaMvprgRgADxAs%3D&amp;sa=X&amp;ei=1BUCU635IYmEygGO1oHwCQ&amp;ved=0CGkQ9QEwBg&amp;biw=1280&amp;bih=899\" rel=\"nofollow\">Bluetooth</a> (Note: I just picked a random link but there are hundreds of similar BT adapters)</p>\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "['never', 'mention', 'tablet', 'model', 'arduino', 'model', 'tablet', 'design', 'work', 'usb', 'flash', 'drive', 'printer', 'call', 'otg', 'go', 'need', 'tablet', 'support', 'otg', 'otg', 'adapt', 'fit', 'tablet', 'usb', 'slot', 'compat', 'arduino', 'uno', 'commun', 'app', 'arduino', 'uno', 'clone', 'note', 'say', 'work', 'atmegau', 'atmegau', 'program', 'usb', 'serial', 'convert', 'would', 'assum', 'would', 'cover', 'board', 'uno', 'anoth', 'alternit', 'look', 'bluetooth', 'note', 'pick', 'random', 'link', 'hundr', 'similar', 'bt', 'adapt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHlPyKTDg9a6",
        "cellView": "form"
      },
      "source": [
        "#@title Not using this\n",
        "## https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "## First lemmatize, and then stem.\n",
        "\n",
        "# def pre_process_stem_sentence(sentence, stop_words, stemmer, lemmatizer):\n",
        "#     ## save in another variable\n",
        "#     text = sentence\n",
        "\n",
        "#     ## remove HTML tags.\n",
        "#     clean = re.compile('<.*?>')\n",
        "#     text = re.sub(clean, '', text)\n",
        "\n",
        "#     ## Convert to small letters.\n",
        "#     text = text.lower()\n",
        "\n",
        "#     # Number Removal\n",
        "#     text = re.sub(r'[-+]?\\d+', '', text)\n",
        "\n",
        "\n",
        "#     ## remove links\n",
        "#     text = re.sub(r\"https\\S+\", \"\", text)\n",
        "#     text = re.sub(r\"www\\S+\", \"\", text)\n",
        "#     text = re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "\n",
        "#     text = text.replace(\"-\", \" \")\n",
        "    \n",
        "#     # Remove punctuations\n",
        "#     text = text.translate((str.maketrans('','',string.punctuation)))    \n",
        "\n",
        "    \n",
        "\n",
        "#     ## remove unicodes.\n",
        "#     # text = re.sub(r\"&nbsp;\", \" \", text)\n",
        "#     # text = re.sub(r'[-+]?\\d+', '', text)\n",
        "#     text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
        "#     # encoded_string = text.encode(\"ascii\", \"ignore\")\n",
        "#     # text = encoded_string.decode()\n",
        "\n",
        "\n",
        "\n",
        "#     # text = re.sub(  \"[^a-z ]\",              # Anything except 0..9, a..z and A..Z\n",
        "#     #                 \"\",                     # replaced with nothing\n",
        "#     #                 text)                   # in this string   \n",
        "\n",
        "    \n",
        "\n",
        "#     ## remove space initially and finally.\n",
        "#     # text = text.lstrip()\n",
        "\n",
        "#     ## make double spaces become one space.\n",
        "#     # text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "#     # Tokenize\n",
        "#     text = word_tokenize(text)\n",
        "\n",
        "#     # Remove stopwords\n",
        "#     # stop_words = set(stopwords.words('english'))\n",
        "#     text = [word for word in text if not word in stop_words]\n",
        "\n",
        "#     # Lemmatize tokens\n",
        "#     text = [lemmatizer.lemmatize(word) for word in text]\n",
        "\n",
        "#     # Stemming tokens\n",
        "#     text = [stemmer.stem(word) for word in text]\n",
        "\n",
        "#     return text\n",
        "\n",
        "################ Test on one sentence #######################\n",
        "\n",
        "# ## Obtain once.\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# stemmer = PorterStemmer()\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# ## Preprocess per element of dataframe to test.\n",
        "# text_to_process = df_train['content'].iloc[499]\n",
        "# print(text_to_process)\n",
        "\n",
        "# print(\"--\"*90)\n",
        "\n",
        "# preprocessed_text = pre_process_stem_sentence(sentence=text_to_process, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "# print(preprocessed_text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4gGdZmwZDZ",
        "outputId": "bfb1ad2b-8a70-46ad-a12e-9fdb9b319ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "doc = df_train['content'].iloc[499]\n",
        "print(doc)\n",
        "\n",
        "print(\"--\"*85)\n",
        "\n",
        "preprocessed = pre_process_stem_sentence(sentence=doc, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(len(preprocessed))\n",
        "print(preprocessed)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<h2>You never mentioned your tablet model... or Arduino model</h2>\n",
            "\n",
            "<p>There are some tablets designed to work with USB flash drives and printers. This is called OTG (on the go.</p>\n",
            "\n",
            "<p><strong>You need:</strong></p>\n",
            "\n",
            "<ul>\n",
            "<li>A tablet supporting OTG</li>\n",
            "<li>An OTG adapter that fits into your tablets USB slot and is compatible</li>\n",
            "<li><a href=\"https://play.google.com/store/apps/details?id=com.primavera.arduino.listener\" rel=\"nofollow\">The Arduino Uno Communicator App</a></li>\n",
            "<li>Arduino Uno (or clone) [Note: It says that it works with Atmega16U2 or Atmega8U2 programmed as a USB-to-serial converter so I would assume that that would cover a few boards more than the Uno.)</li>\n",
            "</ul>\n",
            "\n",
            "<p>Another alternitive is to look into <a href=\"https://www.google.com/search?q=arduino+bluetooth&amp;qscrl=1&amp;tbm=isch&amp;imgil=WyPC4O05Xn4rhM%253A%253Bhttps%253A%252F%252Fencrypted-tbn2.gstatic.com%252Fimages%253Fq%253Dtbn%253AANd9GcSfTtWqIuN9YYdd90OO_tlxsvLUYhV_pl7NfaMhHDzSRq9N6W9rNA%253B1200%253B1200%253B1soQc4CzMzh0MM%253Bhttp%25253A%25252F%25252Fm.dhgate.com%25252Fproduct%25252Fjy-mcu-arduino-bluetooth-wireless-serial%25252F151048482.html&amp;source=iu&amp;usg=__KyMLS4aeubxJnDaMvprgRgADxAs%3D&amp;sa=X&amp;ei=1BUCU635IYmEygGO1oHwCQ&amp;ved=0CGkQ9QEwBg&amp;biw=1280&amp;bih=899\" rel=\"nofollow\">Bluetooth</a> (Note: I just picked a random link but there are hundreds of similar BT adapters)</p>\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "61\n",
            "['never', 'mention', 'tablet', 'model', 'arduino', 'model', 'tablet', 'design', 'work', 'usb', 'flash', 'drive', 'printer', 'call', 'otg', 'go', 'need', 'tablet', 'support', 'otg', 'otg', 'adapt', 'fit', 'tablet', 'usb', 'slot', 'compat', 'arduino', 'uno', 'commun', 'app', 'arduino', 'uno', 'clone', 'note', 'say', 'work', 'atmegau', 'atmegau', 'program', 'usb', 'serial', 'convert', 'would', 'assum', 'would', 'cover', 'board', 'uno', 'anoth', 'alternit', 'look', 'bluetooth', 'note', 'pick', 'random', 'link', 'hundr', 'similar', 'bt', 'adapt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbSKXXrRGGgo"
      },
      "source": [
        "## Preprocess for each sentence of train-dataframe.\n",
        "df_train[\"stemmed_content\"] = df_train.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_val[\"stemmed_content\"] = df_val.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_test[\"stemmed_content\"] = df_test.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puTtacUoHBZ9"
      },
      "source": [
        "# Form Vocabulary from dataframe_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwyJWOOIETs"
      },
      "source": [
        "def print_first_n_keys_and_vals_dict(dictionary, n=5):\n",
        "    print(f\"Len dictionary = {len(dictionary)}, printing first {n} keys, vals\")\n",
        "    itr = 0\n",
        "    for key in dictionary:\n",
        "        print(key, dictionary[key])\n",
        "        itr += 1\n",
        "        if itr == n:\n",
        "            break"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO4hhJYkXqRp",
        "outputId": "e1a309a6-4068-48a2-e48f-139ef5fd613a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%%time\n",
        "### Also remove one-len words\n",
        "dictionary_vocab = {} ## empty dict.\n",
        "for doc in df_train['stemmed_content']:\n",
        "    # print(f\"len doc = {len(doc)}\")\n",
        "    for word in doc:\n",
        "        if len(word) == 1:\n",
        "            continue\n",
        "\n",
        "        len_currently = len(dictionary_vocab) ## add to len. [idx new]\n",
        "\n",
        "        if word not in dictionary_vocab:\n",
        "            dictionary_vocab[word] = (0, len_currently)\n",
        "        else:\n",
        "            (val, idx) = dictionary_vocab[word]\n",
        "            dictionary_vocab[word] = (val + 1, idx) ## Maintain the same index.\n",
        "\n",
        "print(f\"len dictionary_vocab = {len(dictionary_vocab)}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len dictionary_vocab = 17506\n",
            "CPU times: user 215 ms, sys: 0 ns, total: 215 ms\n",
            "Wall time: 215 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSQYB4eBf0sv"
      },
      "source": [
        "# file_name = \"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Vocab.txt\"\n",
        "# with open(file_name, 'w') as fw:\n",
        "#     for voc in dictionary_vocab:\n",
        "#         fw.write(voc)\n",
        "#         fw.write(\"\\n\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAgtrtyp13Tt"
      },
      "source": [
        "# ## https://www.kite.com/python/answers/how-to-save-a-dictionary-to-a-file-in-python\n",
        "# vocab_file = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/vocab_dict_df_train.pkl'\n",
        "\n",
        "# a_file = open(vocab_file, \"wb\")\n",
        "# pickle.dump(dictionary_vocab, a_file)\n",
        "# a_file.close()\n",
        "\n",
        "# # a_file = open(vocab_file, \"rb\")\n",
        "# # dictionary_vocab = pickle.load(a_file)\n",
        "# # print(dictionary_vocab)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJORdHUjHd7h"
      },
      "source": [
        "## Keep only those docs in train whose lengths are above 3 words i.e.\n",
        "### length of stemmed content > 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrZO1SycNmSX",
        "outputId": "b45aa312-8c6b-4a18-81f0-954dffd5fb56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lens = [len(x) for x in df_train['stemmed_content']]\n",
        "print(max(lens))\n",
        "print(min(lens))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1381\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPJ5IMKKNmQV",
        "outputId": "980d5677-2122-4a9b-f98e-1a76c736375f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MIN_WORD_COUNT_TO_REMOVE = 3\n",
        "# print(df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE][\"stemmed_content\"])\n",
        "idxToRemove = df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE].index\n",
        "df_train.drop(idxToRemove , inplace=True)\n",
        "\n",
        "print(f\"After removal, len df_train = {len(df_train)}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After removal, len df_train = 5255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQCXuG-JhUU"
      },
      "source": [
        "## Create Hamming Distance Vectors by representing with 0/1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJfu0JkdKFRh"
      },
      "source": [
        "def form_hamming_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    for word in list_words:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index. [DO NOT]\n",
        "            # vec[UNKNOWN_WORD_INDEX] = 1\n",
        "            continue\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word]\n",
        "            vec[idx] = 1\n",
        "    return vec"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNrTRsZ2psv-"
      },
      "source": [
        "## Create hamming vectors for each col of dataframe\n",
        "df_train[\"ham_vector\"] = df_train.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"ham_vector\"] = df_val.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"ham_vector\"] = df_test.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNM1YUEWrfgk"
      },
      "source": [
        "## Create eucledian vectors by representing how many times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJXmvI8areMJ"
      },
      "source": [
        "def form_eucledian_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    \n",
        "    ## Form a small dictionary to store each word count\n",
        "    dict_local_vocab = {}\n",
        "    for word in list_words:\n",
        "        if word not in dict_local_vocab:\n",
        "            dict_local_vocab[word] = 1\n",
        "        else:\n",
        "            dict_local_vocab[word] = dict_local_vocab[word] + 1\n",
        "\n",
        "    # print(dict_local_vocab)\n",
        "\n",
        "    # unknown_word_count = 1\n",
        "    for word in dict_local_vocab:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index.\n",
        "            continue\n",
        "            # vec[UNKNOWN_WORD_INDEX] = unknown_word_count\n",
        "            # unknown_word_count += 1\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word] \n",
        "            vec[idx] = dict_local_vocab[word] ## replace with value of this word instead of 1.\n",
        "\n",
        "    del dict_local_vocab\n",
        "    return vec"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvxfO7dkpszM"
      },
      "source": [
        "df_train[\"euc_vector\"] = df_train.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"euc_vector\"] = df_val.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"euc_vector\"] = df_test.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEuOjilfKauq"
      },
      "source": [
        "# Stack each of these vertically to form hamming and eucledian vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB_4H48rL6kv",
        "outputId": "64350807-54e7-4188-ea99-6840b13195ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "vocab_size = len(dictionary_vocab) ## PLUS 1 for <UNKNOWN> word\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "\n",
        "num_documents = len(df_train)\n",
        "print(f\"num_documents = {num_documents}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size = 17506\n",
            "num_documents = 5255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFUcRAcIKaPR",
        "outputId": "279930f8-f6bd-4d21-89a6-173f94396e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "hamming_vectors_2D = np.zeros((num_documents, vocab_size))\n",
        "print(f\"hamming_vectors_2D.shape = {hamming_vectors_2D.shape}\")\n",
        "\n",
        "idx = 0\n",
        "for ham_vec in df_train['ham_vector'].values:\n",
        "    hamming_vectors_2D[idx] = ham_vec\n",
        "    idx += 1\n",
        "\n",
        "print(hamming_vectors_2D.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hamming_vectors_2D.shape = (5255, 17506)\n",
            "(5255, 17506)\n",
            "CPU times: user 268 ms, sys: 432 ms, total: 700 ms\n",
            "Wall time: 701 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_5z3V5lLu08",
        "outputId": "36ca4e9d-5d0b-4a63-e3ee-c55773e6b5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "eucledian_vectors_2D = np.zeros((num_documents, vocab_size))\n",
        "print(f\"eucledian_vectors_2D.shape = {eucledian_vectors_2D.shape}\")\n",
        "\n",
        "idx = 0\n",
        "for vec in df_train['euc_vector'].values:\n",
        "    eucledian_vectors_2D[idx] = vec\n",
        "    idx += 1\n",
        "\n",
        "print(eucledian_vectors_2D.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eucledian_vectors_2D.shape = (5255, 17506)\n",
            "(5255, 17506)\n",
            "CPU times: user 259 ms, sys: 451 ms, total: 710 ms\n",
            "Wall time: 710 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZJFsqnGS1dv"
      },
      "source": [
        "# df_train.columns.values\n",
        "labels = df_train['Label'].values"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrEqp1_0LH5"
      },
      "source": [
        "## For now save these dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViUb7lUGzp9N"
      },
      "source": [
        "# df_train.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/train.csv\", index=False)\n",
        "# df_val.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/val.csv\", index=False)\n",
        "# df_test.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/test.csv\", index=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmALIJNfzuMJ"
      },
      "source": [
        "## Now, finally create TF-IDF and pickel dump everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI9h2IBZ-nYv",
        "cellView": "form"
      },
      "source": [
        "#@title Previous TF-IDF\n",
        "# %%time\n",
        "def compute_TF_IDF_all_train_set(hamming_vectors_2D, eucledian_vectors_2D, alpha=0.0001, beta=0.0001):\n",
        "    num_documents, num_words = eucledian_vectors_2D.shape\n",
        "    print(f\"num_documents, num_words = {num_documents, num_words}\")\n",
        "\n",
        "    TF = np.zeros((num_documents, num_words))\n",
        "    IDF = np.zeros((num_documents, num_words))\n",
        "\n",
        "    ## Calculate TF(d, w) = N(d, w)/W(d) ; where N(d, w): count(w) in document d , W(d): Total #words in document d\n",
        "    for itr in range(num_documents): ## iterate row-wise\n",
        "        doc_eucledian = eucledian_vectors_2D[itr]\n",
        "        total_num_words_doc_eucledian = num_words - np.sum(doc_eucledian == 0)        \n",
        "        TF[itr] = doc_eucledian/total_num_words_doc_eucledian\n",
        "\n",
        "        # if itr == 1:\n",
        "            # print(f\"itr = {itr}, doc_euc[itr] = {np.unique(doc_eucledian[itr], return_counts=True)}\")\n",
        "            # print(f\"itr = {itr}, TF[itr] = {np.unique(TF[itr], return_counts=True)}\")\n",
        "            # break\n",
        "\n",
        "    ## Calculate IDF(d, w) = log( (D + alpha)/(C(w) + beta) ) ; C(w) -> Total # docs with word 'w' ; D -> Total # documents\n",
        "    D = num_documents\n",
        "    for itr in range(num_words): ## itereate col-wise\n",
        "        C_w = np.sum(hamming_vectors_2D[:, itr] == 1) ## first calculate C(w)\n",
        "        IDF[:, itr] = np.log( (D + alpha) / (C_w + beta) )\n",
        "\n",
        "        # break\n",
        "        \n",
        "    TF_IDF = TF*IDF\n",
        "    del TF\n",
        "    # del IDF\n",
        "    return TF_IDF, IDF\n",
        "\n",
        "# TF_IDF, IDF = compute_TF_IDF_all_train_set(hamming_vectors_2D=hamming_vectors_2D, eucledian_vectors_2D=eucledian_vectors_2D)\n",
        "# print(f\"TF_IDF.shape = {TF_IDF.shape}\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__-GfML7BDN",
        "outputId": "bc584b0b-0718-4664-b9eb-5b640954df86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# %%time\n",
        "def compute_TF_IDF_all_train_set(hamming_vectors_2D, eucledian_vectors_2D, alpha=0.0001, beta=0.0001):\n",
        "    num_documents, num_words = eucledian_vectors_2D.shape\n",
        "    print(f\"num_documents, num_words = {num_documents, num_words}\")\n",
        "\n",
        "    TF = np.zeros((num_documents, num_words))\n",
        "\n",
        "    IDF = np.zeros((1, num_words))\n",
        "\n",
        "    ## Calculate TF(d, w) = N(d, w)/W(d) ; where N(d, w): count(w) in document d , W(d): Total #words in document d\n",
        "    for itr in range(num_documents): ## iterate row-wise\n",
        "        doc_eucledian = eucledian_vectors_2D[itr]\n",
        "        total_num_words_doc_eucledian = num_words - np.sum(doc_eucledian == 0)        \n",
        "        TF[itr] = doc_eucledian/total_num_words_doc_eucledian\n",
        "\n",
        "        # if itr == 1:\n",
        "            # print(f\"itr = {itr}, doc_euc[itr] = {np.unique(doc_eucledian[itr], return_counts=True)}\")\n",
        "            # print(f\"itr = {itr}, TF[itr] = {np.unique(TF[itr], return_counts=True)}\")\n",
        "            # break\n",
        "\n",
        "    ## Calculate IDF(d, w) = log( (D + alpha)/(C(w) + beta) ) ; C(w) -> Total # docs with word 'w' ; D -> Total # documents\n",
        "    D = num_documents\n",
        "    for itr in range(num_words): ## itereate col-wise\n",
        "        C_w = np.sum(hamming_vectors_2D[:, itr] == 1) ## first calculate C(w)\n",
        "        \n",
        "        IDF[:, itr] = np.log( (D + alpha) / (C_w + beta) )\n",
        "\n",
        "        # break\n",
        "        \n",
        "    TF_IDF = TF*IDF\n",
        "    del TF\n",
        "    # del IDF\n",
        "    return TF_IDF, IDF\n",
        "\n",
        "TF_IDF_whole_corpus, IDF_whole_corpus = compute_TF_IDF_all_train_set(hamming_vectors_2D=hamming_vectors_2D, eucledian_vectors_2D=eucledian_vectors_2D)\n",
        "print(f\"TF_IDF_whole_corpus.shape = {TF_IDF_whole_corpus.shape}\")\n",
        "print(f\"IDF_whole_corpus.shape = {IDF_whole_corpus.shape}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_documents, num_words = (5255, 17506)\n",
            "TF_IDF_whole_corpus.shape = (5255, 17506)\n",
            "IDF_whole_corpus.shape = (1, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWeLmV_094XH",
        "outputId": "95402af5-fc92-484c-daf4-6b8c65c6a7eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def form_TF_IDF_for_val_test(list_words, IDF, hamming_vectors_2D, eucledian_vectors_2D, vocab_dict, alpha=0.0001, beta=0.0001):\n",
        "    num_docs_train, num_words = eucledian_vectors_2D.shape\n",
        "\n",
        "    TF = np.zeros((1, num_words))\n",
        "    \n",
        "    euc_vec = form_eucledian_vector(list_words=list_words, vocab_dict=vocab_dict)\n",
        "    \n",
        "    W_d_num_words_in_document = 0\n",
        "    for word in list_words:\n",
        "        if word in vocab_dict:\n",
        "            W_d_num_words_in_document += 1\n",
        "\n",
        "    TF = euc_vec/W_d_num_words_in_document\n",
        "\n",
        "    TF_IDF = TF*IDF\n",
        "\n",
        "    return TF_IDF\n",
        "\n",
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "\n",
        "TF_IDF = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                    eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "print(f\"TF_IDF.shape = {TF_IDF.shape}\")\n",
        "# print(np.unique(TF_IDF, return_counts=True))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF_IDF.shape = (1, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohbmnfhCXJfD"
      },
      "source": [
        "# Now we start with K-Nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl5ewWSbnxve",
        "outputId": "6ddae159-d030-483f-cce9-0c53e9900277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "df_val.columns.values"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['content', 'Label', 'stemmed_content', 'ham_vector', 'euc_vector'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th8E8_thY2en",
        "outputId": "bb398397-b3ca-4300-973c-26f1bf8e4027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "val_ham_0 = df_val['ham_vector'].iloc[0]\n",
        "print(val_ham_0.shape)\n",
        "\n",
        "val_euc_0 = df_val['euc_vector'].iloc[0]\n",
        "print(val_euc_0.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17506,)\n",
            "(17506,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu-Y52OvggBm"
      },
      "source": [
        "## Similarity functions.\n",
        "def ham(a, b):\n",
        "    return np.count_nonzero((a!=b), axis=1)\n",
        "\n",
        "def euclidean(a, b):\n",
        "    return np.linalg.norm((a-b), axis=1)\n",
        "\n",
        "### https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
        "def cosine_similarity(a, b):\n",
        "    cos_sim = np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "    return cos_sim"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GQLpzcgYY_L"
      },
      "source": [
        "# # a = np.array([x for x in range(40)])\n",
        "# a = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1])\n",
        "# a = a.reshape(5, -1)\n",
        "# print(a)\n",
        "\n",
        "# b = np.array([1, 0, 1])\n",
        "# b = b.reshape(1, -1)\n",
        "# print(\"\\n\", b)\n",
        "\n",
        "\n",
        "# print(ham(a, b))\n",
        "# print(euclidean(a, b))\n",
        "\n",
        "# a = np.array([2, 1, 3, 4, 5, 1, 10, 3, 22])\n",
        "# n = 4\n",
        "# indices_top = (-a).argsort()[:n]\n",
        "# print(f\"indices_top = {indices_top}\")\n",
        "# print(a[indices_top])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RQnLJLEXI2X",
        "outputId": "62595243-8ff0-4912-b47c-0df84a0f53eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class KNN:\n",
        "    def __init__(self, K = 1, mode=\"hamming\", printing=False):\n",
        "        self.mode = mode\n",
        "        self.K = K\n",
        "        if printing == True:\n",
        "            print(f\"KNN __init__(K={K}, mode={mode})\")\n",
        "        \n",
        "\n",
        "    def populate_vectors(self, vectors_corpus, labels):\n",
        "        self.vectors_corpus = vectors_corpus\n",
        "        self.labels = labels\n",
        "\n",
        "    def compute_distances(self, v):\n",
        "        if self.mode == \"hamming\":\n",
        "            v = v.reshape(1, -1) ## Reshape vector.\n",
        "            self.distances = ham(v, self.vectors_corpus)\n",
        "        elif self.mode == \"euclidean\":\n",
        "            v = v.reshape(1, -1) ## Reshape vector.\n",
        "            self.distances = euclidean(v, self.vectors_corpus)\n",
        "        elif self.mode == 'cosine_similarity':\n",
        "            v = v.reshape(-1)\n",
        "            self.distances = cosine_similarity(v, self.vectors_corpus)\n",
        "\n",
        "        # print(f\"After compute_distances mode={self.mode}, distances = {self.distances}\")\n",
        "\n",
        "    def predict(self, v):\n",
        "        ## compute distances by using suitable similarity function.\n",
        "        self.compute_distances(v)\n",
        "\n",
        "        ## take argmax top results indices\n",
        "        if self.mode == 'cosine_similarity':\n",
        "            indices_top = (-self.distances).argsort()[:self.K] ## for some reason, this works for cosine-similarity\n",
        "        else:\n",
        "            indices_top = (self.distances).argsort()[:self.K] ## THIS works for hamming and euclidean\n",
        "\n",
        "        ## apply indices to labels\n",
        "        top_labels = self.labels[indices_top]\n",
        "\n",
        "        ## take majority vote and return the label\n",
        "        top_most_label = stats.mode(top_labels)[0][0]\n",
        "\n",
        "        ## return the max label\n",
        "        return top_most_label\n",
        "\n",
        "################################################# Test #################################################\n",
        "\n",
        "knn = KNN(K=1, mode='hamming')\n",
        "knn.populate_vectors(hamming_vectors_2D, labels)\n",
        "knn.predict(val_ham_0)\n",
        "\n",
        "# knn = KNN(K=7, mode='euclidean')\n",
        "# knn.populate_vectors(eucledian_vectors_2D, labels)\n",
        "# knn.predict(val_euc_0)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Coffee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQjSecagA0nX",
        "outputId": "7b706834-8c68-405f-d76b-b4c06ba3832b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "knn = KNN(K=1, mode='cosine_similarity')\n",
        "knn.populate_vectors(TF_IDF_whole_corpus, labels)\n",
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "tf_idf_val_0 = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                    eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "knn.predict(tf_idf_val_0)\n",
        "tf_idf_val_0 = tf_idf_val_0.reshape(-1)\n",
        "cs = cosine_similarity(tf_idf_val_0, knn.vectors_corpus)\n",
        "print(cs.shape)\n",
        "print(cs)\n",
        "\n",
        "indices_top = (-cs).argsort()[:knn.K]\n",
        "print(indices_top)\n",
        "print(knn.labels[indices_top])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5255,)\n",
            "[0.         0.00026631 0.00062275 ... 0.00010283 0.         0.        ]\n",
            "[3056]\n",
            "['Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xr7vkezDOiB",
        "outputId": "fb1e8b32-8c84-4382-f8ed-53c6a462b8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "tf_idf_val_0 = tf_idf_val_0.reshape(-1)\n",
        "print(tf_idf_val_0.shape)\n",
        "print(TF_IDF_whole_corpus.shape)\n",
        "\n",
        "cs = cosine_similarity(tf_idf_val_0, TF_IDF_whole_corpus)\n",
        "\n",
        "print(cs.shape)\n",
        "print(cs)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17506,)\n",
            "(5255, 17506)\n",
            "(5255,)\n",
            "[0.         0.00026631 0.00062275 ... 0.00010283 0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wzDleMoXe-b"
      },
      "source": [
        "## Applying tests on validation set for KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELPWJfGnICjU",
        "outputId": "723d3f20-c4d9-4a49-a9d1-97ca33b0dfc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "# def add_to_dataframe_results(df_old, to_add):\n",
        "#     df_old = df_old.append(pd.Series(to_add, index=df_train.columns), ignore_index=True)\n",
        "#     return df_old\n",
        "\n",
        "column_names = [\"Similarity-Measure\", \"K\", \"Accuracy(%)\"]\n",
        "\n",
        "df_results_knn = pd.DataFrame(columns=column_names)\n",
        "\n",
        "# df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Hamming\", k, 22])\n",
        "\n",
        "display(df_results_knn.head())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Similarity-Measure</th>\n",
              "      <th>K</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Similarity-Measure, K, Accuracy(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G--zPyF5baEC"
      },
      "source": [
        "# !pip3 install tqdm\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "from functools import reduce"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKPaf3QVXens",
        "outputId": "a8b736db-0354-4926-ed18-d2c1253cd399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%%time\n",
        "for k in [1, 3, 5]: # [1, 3, 5]:\n",
        "    knn_ham = KNN(K=k, mode='hamming')\n",
        "    knn_euc = KNN(K=k, mode='euclidean')\n",
        "    knn_cosine = KNN(K=k, mode='cosine_similarity')\n",
        "\n",
        "\n",
        "    knn_ham.populate_vectors(hamming_vectors_2D, labels)\n",
        "    knn_euc.populate_vectors(eucledian_vectors_2D, labels)\n",
        "    knn_cosine.populate_vectors(TF_IDF_whole_corpus, labels)\n",
        "\n",
        "    \n",
        "\n",
        "    ham_correct = 0\n",
        "    euc_correct = 0\n",
        "    cosine_correct = 0\n",
        "\n",
        "    itr = 0\n",
        "    for (ham_val, lab_true) in (zip(df_val['ham_vector'].values, df_val['Label'].values)):\n",
        "        itr += 1\n",
        "        # if itr%300 == 0:\n",
        "            # print(f\"Ham itr = {itr}\")\n",
        "        lab_pred = knn_ham.predict(ham_val)\n",
        "        if lab_pred == lab_true:\n",
        "            ham_correct += 1\n",
        "\n",
        "    itr = 0\n",
        "    for (euc_val, lab_true) in (zip(df_val['euc_vector'].values, df_val['Label'].values)):\n",
        "        itr += 1\n",
        "        # if itr%300 == 0:\n",
        "            # print(f\"Euc itr = {itr}\")\n",
        "        lab_pred = knn_euc.predict(euc_val)\n",
        "        if lab_pred == lab_true:\n",
        "            euc_correct += 1\n",
        "\n",
        "    itr = 0\n",
        "    for (words_val, lab_true) in (zip(df_val['stemmed_content'].values, df_val['Label'].values)):\n",
        "        itr += 1\n",
        "        # if itr%300 == 0:\n",
        "        #     print(f\"TF-IDF itr = {itr}\")\n",
        "        val_tf_idf = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "        # if (itr % 10) == 0:\n",
        "            # print(np.unique(val_tf_idf, return_counts=True))\n",
        "            # break\n",
        "        lab_pred = knn_cosine.predict(val_tf_idf)\n",
        "        # print(knn_cosine.distances.shape)\n",
        "\n",
        "        if lab_pred == lab_true:\n",
        "            cosine_correct += 1\n",
        "\n",
        "\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Hamming\", k, ham_correct/len(df_val)*100])\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Euclidean\", k, euc_correct/len(df_val)*100])\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"TF-IDF-cosine-sim\", k, cosine_correct/len(df_val)*100])\n",
        "\n",
        "    print(f\"k = {k}, Hamming: {(ham_correct/len(df_val)*100)}%, Euclidean: {(euc_correct/len(df_val)*100)}%, TF-IDF: {(cosine_correct/len(df_val)*100)}%\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "k = 1, Hamming: 46.45454545454545%, Euclidean: 58.09090909090909%, TF-IDF: 72.0909090909091%\n",
            "k = 3, Hamming: 45.95454545454545%, Euclidean: 55.90909090909091%, TF-IDF: 76.95454545454545%\n",
            "k = 5, Hamming: 48.090909090909086%, Euclidean: 57.27272727272727%, TF-IDF: 78.72727272727272%\n",
            "CPU times: user 1h 21min 52s, sys: 43.5 s, total: 1h 22min 35s\n",
            "Wall time: 1h 10min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvq54JeJXelm",
        "outputId": "59bb7db6-c7bb-4475-e041-30dbc6102752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "print(f\"len df_results_knn = {len(df_results_knn)}\")\n",
        "display(df_results_knn)\n",
        "df_results_knn.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/KNN-val-11-topics.csv', index=False)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_knn = 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Similarity-Measure</th>\n",
              "      <th>K</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>1</td>\n",
              "      <td>46.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>1</td>\n",
              "      <td>58.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>1</td>\n",
              "      <td>72.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>3</td>\n",
              "      <td>45.954545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>3</td>\n",
              "      <td>55.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>3</td>\n",
              "      <td>76.954545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>5</td>\n",
              "      <td>48.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>5</td>\n",
              "      <td>57.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>5</td>\n",
              "      <td>78.727273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Similarity-Measure  K  Accuracy(%)\n",
              "0            Hamming  1    46.454545\n",
              "1          Euclidean  1    58.090909\n",
              "2  TF-IDF-cosine-sim  1    72.090909\n",
              "3            Hamming  3    45.954545\n",
              "4          Euclidean  3    55.909091\n",
              "5  TF-IDF-cosine-sim  3    76.954545\n",
              "6            Hamming  5    48.090909\n",
              "7          Euclidean  5    57.272727\n",
              "8  TF-IDF-cosine-sim  5    78.727273"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K-q5b6ZzqVP",
        "outputId": "5e863428-113e-4027-f0a7-c85842d034f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "mat = df_results_knn.values\n",
        "print(mat.shape)\n",
        "print(mat)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9, 3)\n",
            "[['Hamming' 1 46.45454545454545]\n",
            " ['Euclidean' 1 58.09090909090909]\n",
            " ['TF-IDF-cosine-sim' 1 72.0909090909091]\n",
            " ['Hamming' 3 45.95454545454545]\n",
            " ['Euclidean' 3 55.90909090909091]\n",
            " ['TF-IDF-cosine-sim' 3 76.95454545454545]\n",
            " ['Hamming' 5 48.090909090909086]\n",
            " ['Euclidean' 5 57.27272727272727]\n",
            " ['TF-IDF-cosine-sim' 5 78.72727272727272]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0LcdCK_RW8E"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBwmpIgW4XOD"
      },
      "source": [
        "### Combine all documents of each class i into one document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EagCOsAhzqiu"
      },
      "source": [
        "# unique_labels = np.unique(df_train['Label'].values)\n",
        "# dictionary_list_words_for_NB = {}\n",
        "# # for label in unique_labels:\n",
        "# #     if label not in dictionary_list_words:\n",
        "# #         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "    \n",
        "# for (list_words, label) in zip(df_train['stemmed_content'].values, df_train['Label'].values):\n",
        "#     if label not in dictionary_list_words_for_NB:\n",
        "#         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "#     dictionary_list_words_for_NB[label].append(list_words)\n",
        "\n",
        "# ## reduce/flat-out to make 1D list.\n",
        "# ## https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
        "# for label in dictionary_list_words_for_NB:\n",
        "#     dictionary_list_words_for_NB[label] = reduce(operator.concat, dictionary_list_words_for_NB[label])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkQbU3mCONb"
      },
      "source": [
        "## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "def keywithmaxval(d):\n",
        "     \"\"\" a) create a list of the dict's keys and values; \n",
        "         b) return the key with the max value\"\"\"  \n",
        "     v=list(d.values())\n",
        "     k=list(d.keys())\n",
        "     return k[v.index(max(v))]"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAcwIAKOFMJF",
        "outputId": "80026e83-3255-4c13-88f0-55c31ac5bafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self, vocab_size, alpha=0.01, to_print=False):\n",
        "        if to_print == True:\n",
        "            print(f\"NaiveBayes __init(alpha={alpha})__\")\n",
        "        self.alpha = alpha\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dictionary_list_words_for_NB = {}\n",
        "        self.dictionary_prior_probabilities = {}\n",
        "        self.dictionary_count_words_per_class = {}\n",
        "        self.dictionary_total_words_per_class = {}\n",
        "    \n",
        "\n",
        "    def fit(self, list_list_words, labels):\n",
        "        self.list_list_words = list_list_words\n",
        "        self.labels = labels\n",
        "        self.form_dictionary_list_words()\n",
        "        self.compute_probabilities()\n",
        "\n",
        "\n",
        "    def form_dictionary_list_words(self):\n",
        "        for (list_words, label) in zip(self.list_list_words, self.labels):\n",
        "            if label not in self.dictionary_list_words_for_NB:\n",
        "                self.dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "            self.dictionary_list_words_for_NB[label].append(list_words)\n",
        "        \n",
        "        for label in self.dictionary_list_words_for_NB: ## reduce/flat-out to make 1D list.\n",
        "            self.dictionary_list_words_for_NB[label] = reduce(operator.concat, self.dictionary_list_words_for_NB[label])\n",
        "\n",
        "\n",
        "    def compute_probabilities(self):\n",
        "        ## Compute prior probabilities/initial guesses\n",
        "        (classes, cnts) = np.unique(self.labels, return_counts=True)\n",
        "        cnts = cnts/np.sum(cnts) ## C_i / (C_1 + C_2 + ... + C_n)\n",
        "        for (lab, itr) in zip(classes, range(len(classes))):\n",
        "            self.dictionary_prior_probabilities[lab] = cnts[itr]\n",
        "\n",
        "        ## Compute per-word probabilities\n",
        "\n",
        "        ## Counter increment\n",
        "        for lab in classes:\n",
        "            num_words_this_class = 0\n",
        "            self.dictionary_count_words_per_class[lab] = {}\n",
        "            for word in self.dictionary_list_words_for_NB[lab]:\n",
        "                if word not in self.dictionary_count_words_per_class[lab]:\n",
        "                    self.dictionary_count_words_per_class[lab][word] = 0 ## initialize counter to 0.\n",
        "                self.dictionary_count_words_per_class[lab][word] = self.dictionary_count_words_per_class[lab][word] + 1 ## increment counter\n",
        "                num_words_this_class += 1\n",
        "            self.dictionary_total_words_per_class[lab] = num_words_this_class\n",
        "\n",
        "    def predict(self, list_words):\n",
        "        ## Compute probabilities for each label.\n",
        "        dict_probabilities_per_class = {}\n",
        "\n",
        "        for lab in np.unique(self.labels):\n",
        "            prob_log_curr_class = np.log(self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # prob_log_curr_class = (self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # print(f\"Before, prob_log_curr_class = {prob_log_curr_class}\")\n",
        "            for word in list_words: ## iterate per word\n",
        "                if word in self.dictionary_count_words_per_class[lab]: ## if word exists in THIS document.\n",
        "                    ## use smoothing factor alpha\n",
        "                    # prob_log_curr_class += np.log((self.dictionary_count_words_per_class[lab][word] + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size)) \n",
        "                    word_prob = self.dictionary_count_words_per_class[lab][word]\n",
        "                    # prob_log_curr_class = prob_log_curr_class*word_prob\n",
        "                else:\n",
        "                    word_prob = 0\n",
        "                prob_log_curr_class += np.log( (word_prob + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size) )\n",
        "\n",
        "\n",
        "            dict_probabilities_per_class[lab] = prob_log_curr_class\n",
        "            # print(dict_probabilities_per_class)\n",
        "\n",
        "        ## Get max probability class.\n",
        "        ## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "        return keywithmaxval(d=dict_probabilities_per_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################### Checking ############################################################\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=len(dictionary_vocab))\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "check = df_val['stemmed_content'].iloc[0]\n",
        "p = NB.predict(check)\n",
        "print(p)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uILDNAL3FddD",
        "outputId": "ad4a50b4-a9d9-45af-93d7-1bc04b9d2eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list_new = [\n",
        "['coffee', 'tea', 'dew', 'dew', 'dew', 'dew'],\n",
        "['coffee', 'noir', 'homelander', 'dew'],\n",
        "['noir', 'noir', 'fool', 'fool', 'noir']\n",
        "]\n",
        "\n",
        "labs_new = [\n",
        "    'bev',\n",
        "    'supe',\n",
        "    'misc'\n",
        "]\n",
        "\n",
        "vocab_size = 6\n",
        "\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=6)\n",
        "NB.fit(list_new, labs_new)\n",
        "print(NB.predict(['coffee', 'tea', 'dew', 'dew', 'dew', 'dew']))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f13yqf6EKAaJ"
      },
      "source": [
        "## Validation on NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgWpyxjCFMVM",
        "outputId": "bb3fe58e-f30f-43e6-a2df-a70d6d6a825d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"ALPHA\", \"Accuracy(%)\"]\n",
        "\n",
        "df_results_NB = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_results_NB.head())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ALPHA, Accuracy(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5FaA8KG8zu",
        "outputId": "0483590a-50bc-41fa-f1b2-bc4b64cdef21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "alpha_values = np.linspace(start=0.01, stop=1.0, num=100)\n",
        "print(alpha_values)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14\n",
            " 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28\n",
            " 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42\n",
            " 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56\n",
            " 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7\n",
            " 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84\n",
            " 0.85 0.86 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98\n",
            " 0.99 1.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rC01YFFMRn",
        "outputId": "7c8cc6f1-6bfe-4c53-9982-d78b1f78e77b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "### Validation NaiveBayes ###\n",
        "for alpha in (alpha_values):\n",
        "    NB = NaiveBayes(alpha=alpha, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "    NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "    nb_acc = 0\n",
        "    ## Predict each val set ##\n",
        "    for (x, y) in (zip(df_val['stemmed_content'].values, df_val['Label'].values)):\n",
        "        y_pred = NB.predict(x)\n",
        "        if y_pred == y:\n",
        "            nb_acc += 1\n",
        "\n",
        "\n",
        "    ## Append to dataframe and print.\n",
        "    # print(f\"NB alpha = {alpha}, accuracy = {nb_acc/len(df_val)*100} %\")\n",
        "    df_results_NB = add_to_dataframe(df_old=df_results_NB, to_add=[alpha, nb_acc/len(df_val)*100])\n",
        "        "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.01)__\n",
            "NaiveBayes __init(alpha=0.02)__\n",
            "NaiveBayes __init(alpha=0.03)__\n",
            "NaiveBayes __init(alpha=0.04)__\n",
            "NaiveBayes __init(alpha=0.05)__\n",
            "NaiveBayes __init(alpha=0.060000000000000005)__\n",
            "NaiveBayes __init(alpha=0.06999999999999999)__\n",
            "NaiveBayes __init(alpha=0.08)__\n",
            "NaiveBayes __init(alpha=0.09)__\n",
            "NaiveBayes __init(alpha=0.09999999999999999)__\n",
            "NaiveBayes __init(alpha=0.11)__\n",
            "NaiveBayes __init(alpha=0.12)__\n",
            "NaiveBayes __init(alpha=0.13)__\n",
            "NaiveBayes __init(alpha=0.14)__\n",
            "NaiveBayes __init(alpha=0.15000000000000002)__\n",
            "NaiveBayes __init(alpha=0.16)__\n",
            "NaiveBayes __init(alpha=0.17)__\n",
            "NaiveBayes __init(alpha=0.18000000000000002)__\n",
            "NaiveBayes __init(alpha=0.19)__\n",
            "NaiveBayes __init(alpha=0.2)__\n",
            "NaiveBayes __init(alpha=0.21000000000000002)__\n",
            "NaiveBayes __init(alpha=0.22)__\n",
            "NaiveBayes __init(alpha=0.23)__\n",
            "NaiveBayes __init(alpha=0.24000000000000002)__\n",
            "NaiveBayes __init(alpha=0.25)__\n",
            "NaiveBayes __init(alpha=0.26)__\n",
            "NaiveBayes __init(alpha=0.27)__\n",
            "NaiveBayes __init(alpha=0.28)__\n",
            "NaiveBayes __init(alpha=0.29000000000000004)__\n",
            "NaiveBayes __init(alpha=0.3)__\n",
            "NaiveBayes __init(alpha=0.31)__\n",
            "NaiveBayes __init(alpha=0.32)__\n",
            "NaiveBayes __init(alpha=0.33)__\n",
            "NaiveBayes __init(alpha=0.34)__\n",
            "NaiveBayes __init(alpha=0.35000000000000003)__\n",
            "NaiveBayes __init(alpha=0.36000000000000004)__\n",
            "NaiveBayes __init(alpha=0.37)__\n",
            "NaiveBayes __init(alpha=0.38)__\n",
            "NaiveBayes __init(alpha=0.39)__\n",
            "NaiveBayes __init(alpha=0.4)__\n",
            "NaiveBayes __init(alpha=0.41000000000000003)__\n",
            "NaiveBayes __init(alpha=0.42000000000000004)__\n",
            "NaiveBayes __init(alpha=0.43)__\n",
            "NaiveBayes __init(alpha=0.44)__\n",
            "NaiveBayes __init(alpha=0.45)__\n",
            "NaiveBayes __init(alpha=0.46)__\n",
            "NaiveBayes __init(alpha=0.47000000000000003)__\n",
            "NaiveBayes __init(alpha=0.48000000000000004)__\n",
            "NaiveBayes __init(alpha=0.49)__\n",
            "NaiveBayes __init(alpha=0.5)__\n",
            "NaiveBayes __init(alpha=0.51)__\n",
            "NaiveBayes __init(alpha=0.52)__\n",
            "NaiveBayes __init(alpha=0.53)__\n",
            "NaiveBayes __init(alpha=0.54)__\n",
            "NaiveBayes __init(alpha=0.55)__\n",
            "NaiveBayes __init(alpha=0.56)__\n",
            "NaiveBayes __init(alpha=0.5700000000000001)__\n",
            "NaiveBayes __init(alpha=0.5800000000000001)__\n",
            "NaiveBayes __init(alpha=0.59)__\n",
            "NaiveBayes __init(alpha=0.6)__\n",
            "NaiveBayes __init(alpha=0.61)__\n",
            "NaiveBayes __init(alpha=0.62)__\n",
            "NaiveBayes __init(alpha=0.63)__\n",
            "NaiveBayes __init(alpha=0.64)__\n",
            "NaiveBayes __init(alpha=0.65)__\n",
            "NaiveBayes __init(alpha=0.66)__\n",
            "NaiveBayes __init(alpha=0.67)__\n",
            "NaiveBayes __init(alpha=0.68)__\n",
            "NaiveBayes __init(alpha=0.6900000000000001)__\n",
            "NaiveBayes __init(alpha=0.7000000000000001)__\n",
            "NaiveBayes __init(alpha=0.7100000000000001)__\n",
            "NaiveBayes __init(alpha=0.72)__\n",
            "NaiveBayes __init(alpha=0.73)__\n",
            "NaiveBayes __init(alpha=0.74)__\n",
            "NaiveBayes __init(alpha=0.75)__\n",
            "NaiveBayes __init(alpha=0.76)__\n",
            "NaiveBayes __init(alpha=0.77)__\n",
            "NaiveBayes __init(alpha=0.78)__\n",
            "NaiveBayes __init(alpha=0.79)__\n",
            "NaiveBayes __init(alpha=0.8)__\n",
            "NaiveBayes __init(alpha=0.81)__\n",
            "NaiveBayes __init(alpha=0.8200000000000001)__\n",
            "NaiveBayes __init(alpha=0.8300000000000001)__\n",
            "NaiveBayes __init(alpha=0.8400000000000001)__\n",
            "NaiveBayes __init(alpha=0.85)__\n",
            "NaiveBayes __init(alpha=0.86)__\n",
            "NaiveBayes __init(alpha=0.87)__\n",
            "NaiveBayes __init(alpha=0.88)__\n",
            "NaiveBayes __init(alpha=0.89)__\n",
            "NaiveBayes __init(alpha=0.9)__\n",
            "NaiveBayes __init(alpha=0.91)__\n",
            "NaiveBayes __init(alpha=0.92)__\n",
            "NaiveBayes __init(alpha=0.93)__\n",
            "NaiveBayes __init(alpha=0.9400000000000001)__\n",
            "NaiveBayes __init(alpha=0.9500000000000001)__\n",
            "NaiveBayes __init(alpha=0.9600000000000001)__\n",
            "NaiveBayes __init(alpha=0.97)__\n",
            "NaiveBayes __init(alpha=0.98)__\n",
            "NaiveBayes __init(alpha=0.99)__\n",
            "NaiveBayes __init(alpha=1.0)__\n",
            "CPU times: user 16min 58s, sys: 6.66 s, total: 17min 4s\n",
            "Wall time: 17min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbwK_kMxTY"
      },
      "source": [
        "df_results_NB.sort_values(by=['Accuracy(%)'], inplace=True, ascending=False)\n",
        "df_results_NB.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/NB-val-11-topics.csv', index=False)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtew8yEpMxep",
        "outputId": "1bf9aa42-9f68-46d8-8f88-030eb54cd13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "display(df_results_NB.head(10))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.03</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.07</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.25</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.06</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.09</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.08</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.15</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ALPHA  Accuracy(%)\n",
              "1    0.02    88.500000\n",
              "3    0.04    88.500000\n",
              "2    0.03    88.454545\n",
              "4    0.05    88.454545\n",
              "6    0.07    88.454545\n",
              "24   0.25    88.409091\n",
              "5    0.06    88.409091\n",
              "8    0.09    88.409091\n",
              "7    0.08    88.409091\n",
              "14   0.15    88.409091"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKuhCM4VdKDC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4G-LMq-nUTK"
      },
      "source": [
        "## Results analysis for Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ErSLdvlpSju"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MLIM89InViN"
      },
      "source": [
        "df_res = pd.read_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/NB-val-11-topics.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zh4Sn4QnVu-"
      },
      "source": [
        "print(f\"len df_res = {len(df_res)}\")\n",
        "print(df_res.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6P47TLtnVoD"
      },
      "source": [
        "df_res.sort_values(by=['Accuracy(%)'], inplace=True, ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jTYNoTOnVl8"
      },
      "source": [
        "display(df_res.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq5mXZWIn35J"
      },
      "source": [
        "df_res[(df_res['ALPHA']<=0.3) & (df_res['ALPHA']>=0.2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCLTyO1nn34U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezBkJzUdn33g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iStYWbffn32i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LS6FnDun3vf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNcU7hdTn3tc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}