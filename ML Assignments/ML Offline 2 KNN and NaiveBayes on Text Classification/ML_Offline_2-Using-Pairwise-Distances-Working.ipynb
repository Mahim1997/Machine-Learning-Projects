{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Offline-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrnubMIEG0BZ"
      },
      "source": [
        "### MOUNT DRIVE ....."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1oJyY78cXnq"
      },
      "source": [
        "## First Import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import pickle\n",
        "\n",
        "from scipy import stats\n",
        "from xml.dom import minidom\n",
        "\n",
        "from sklearn.metrics import pairwise_distances, accuracy_score\n",
        "\n",
        "\n",
        "# !pip3 install tqdm\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "from functools import reduce"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFLKkx6Ybnlr"
      },
      "source": [
        "FOLDER_TRAIN = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/Training/'\n",
        "FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics.txt'\n",
        "# FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics-all.txt'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9hrTtEZp2A8"
      },
      "source": [
        "# !ls \"$FOLDER_TRAIN\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0bZ4kTHcXql"
      },
      "source": [
        "## Set random seed\n",
        "RANDOM_STATE = 22\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThQtktwcXtX",
        "outputId": "bd1ea24e-d15e-42d8-ff20-d89fbe872f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%%time\n",
        "## Read data\n",
        "topic_remove = '3d_Printer' ## Remove 3D-printer\n",
        "list_doc_types = []\n",
        "with open(FILE_TOPICS, 'r') as fp:\n",
        "    topic = fp.readline()\n",
        "    while topic:\n",
        "        topic = topic.replace(\"\\n\", \"\")\n",
        "        list_doc_types.append(topic)\n",
        "        topic = fp.readline()\n",
        "\n",
        "list_doc_types.remove(topic_remove)\n",
        "print(list_doc_types)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Coffee', 'Arduino', 'Anime']\n",
            "CPU times: user 289 µs, sys: 683 µs, total: 972 µs\n",
            "Wall time: 11.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfC2PYXxg35A",
        "outputId": "d1a407af-683d-41e3-9c2e-a4eb4d0fcc0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(list_doc_types)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Coffee', 'Arduino', 'Anime']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVPfrYjagVv4"
      },
      "source": [
        "def get_train_val_test_data(file_name):\n",
        "    xmldoc = minidom.parse(file_name)\n",
        "    xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "    \n",
        "    # print(f\"Inside get_train_val_test_data(), len(item_list) = {len(item_list)}\")\n",
        "\n",
        "    item_list = [x.attributes['Body'].value for x in xml_list]\n",
        "\n",
        "    train_list = item_list[0:500] ## first 500 train\n",
        "    val_list =  item_list[500:700] ## next 200 val\n",
        "    test_list = item_list[700:1200] ## next 500 test\n",
        "\n",
        "    ## delete original list.\n",
        "    del item_list\n",
        "    del xmldoc\n",
        "\n",
        "    ## return the new lists\n",
        "    return train_list, val_list, test_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvop_yNmUCMo"
      },
      "source": [
        "def add_to_dataframe(df_old, to_add):\n",
        "    df_old = df_old.append(pd.Series(to_add, index=df_old.columns), ignore_index=True)\n",
        "    return df_old"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MJXly5bt7aC"
      },
      "source": [
        "## Create three dataframes.\n",
        "\n",
        "## https://www.kite.com/python/answers/how-to-create-an-empty-dataframe-with-column-names-in-python\n",
        "column_names =[\"content\", \"Label\"]\n",
        "\n",
        "df_train = pd.DataFrame(columns = column_names)\n",
        "df_val = pd.DataFrame(columns = column_names)\n",
        "df_test = pd.DataFrame(columns = column_names)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxZHmfGt1g9e",
        "outputId": "50c7580a-d0e8-44b6-96c7-ea92958c2040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "### Populate all topics\n",
        "def populate_data_frames(df_train, df_val, df_test, list_doc_types):\n",
        "    for topic in list_doc_types: ## iterating per topic/label\n",
        "        label = topic ## assign label\n",
        "        \n",
        "        ## read using xml package\n",
        "        file_name = FOLDER_TRAIN + topic + \".xml\"\n",
        "        xmldoc = minidom.parse(file_name)\n",
        "        xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "\n",
        "        ## get train, val, test lists\n",
        "        train_list, val_list, test_list = get_train_val_test_data(file_name=file_name)\n",
        "\n",
        "        print(len(train_list), len(val_list), len(test_list))\n",
        "\n",
        "        ## add to dataframe\n",
        "        for v1 in train_list:\n",
        "            df_train = add_to_dataframe(df_old=df_train, to_add=[v1, label])\n",
        "        for v2 in val_list:\n",
        "            df_val = add_to_dataframe(df_old=df_val, to_add=[v2, label])\n",
        "        for v3 in test_list:\n",
        "            df_test = add_to_dataframe(df_old=df_test, to_add=[v3, label])\n",
        "\n",
        "\n",
        "        ## delete original list\n",
        "        del train_list\n",
        "        del val_list\n",
        "        del test_list\n",
        "\n",
        "    ## return dataframes\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "### Call the function\n",
        "df_train, df_val, df_test = populate_data_frames(df_train=df_train, df_val=df_val, df_test=df_test, list_doc_types=list_doc_types)\n",
        "\n",
        "print(f\"len df_train = {len(df_train)}\")\n",
        "print(f\"len df_val = {len(df_val)}\")\n",
        "print(f\"len df_test = {len(df_test)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "len df_train = 1500\n",
            "len df_val = 600\n",
            "len df_test = 1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUI07UZKmPc"
      },
      "source": [
        "## Shuffle dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rcv33P3XXuN"
      },
      "source": [
        "df_train = df_train.sample(frac=1, random_state=RANDOM_STATE)\n",
        "df_val = df_val.sample(frac=1, random_state=RANDOM_STATE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1m0_Du8XYBK"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKSziS4PvdnZ"
      },
      "source": [
        "## https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
        "## https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "## from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIukiiJxDWdu",
        "outputId": "f3009aa9-f435-4db3-c42e-7c7db77fb0b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "## https://stackoverflow.com/questions/26693736/nltk-and-stopwords-fail-lookuperror\n",
        "## https://stackoverflow.com/questions/26570944/resource-utokenizers-punkt-english-pickle-not-found\n",
        "\n",
        "# nltk.download()\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUsCIVJru269"
      },
      "source": [
        "## Preprocess each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-MlUQ4w1uM2",
        "cellView": "both",
        "outputId": "4ae734e3-f9b0-46b2-b550-5e2eeb117014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "## https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "## First lemmatize, and then stem.\n",
        "\n",
        "def pre_process_stem_sentence(sentence, stop_words, stemmer, lemmatizer):\n",
        "    ## save in another variable\n",
        "    text = sentence\n",
        "    ## remove HTML tags [using soup]\n",
        "    soup = BeautifulSoup(text)\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    ## remove <a href> type things\n",
        "    soup = BeautifulSoup(text) ## create soup again.\n",
        "    for a in soup.findAll('a'):\n",
        "        a.replaceWithChildren()\n",
        "    \n",
        "    text = str(soup) ## reform text\n",
        "\n",
        "\n",
        "    ## remove unicode.\n",
        "    text = re.sub(r\"&nbsp;\", \" \", text)\n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
        "\n",
        "    ## remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"https\\S+\", \"\", text)\n",
        "    ## text = re.sub(r\"www\\S+\", \"\", text)\n",
        "\n",
        "    ## to be safe, remove HTML tags again using regex\n",
        "    #### https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44  \n",
        "    clean = re.compile('<.*?>')\n",
        "    text = re.sub(clean, '', text)\n",
        "\n",
        "    ## convert to small letters.\n",
        "    text = text.lower()\n",
        "\n",
        "    ## replace newlines, tabs -> SPACE\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "\n",
        "    ## {COLON} make <a>:<b> become <a>[space]<b>\n",
        "    # text = text.replace(\": \", \":\") # :[space] -> : \n",
        "    text = text.replace(\":\", \" \")  # : -> [space]\n",
        "    \n",
        "    ## {HYPHEN}\n",
        "    # text = text.replace(\"- \", \"-\")\n",
        "    text = text.replace(\"-\", \" \")\n",
        "\n",
        "    ## numbers removal    \n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "\n",
        "    ## punctuations removal\n",
        "    text = text.translate((str.maketrans('','',string.punctuation)))   \n",
        "\n",
        "\n",
        "    ## [remove anything EXCEPT english letters]\n",
        "    ## https://stackoverflow.com/questions/6323296/python-remove-anything-that-is-not-a-letter-or-number\n",
        "    text = re.sub(  \"[^a-z ]\",              # Anything except 0..9, a..z and A..Z\n",
        "                    \"\",                     # replaced with nothing\n",
        "                    text)                   # in this string   \n",
        "\n",
        "    ## remove space initially and finally.\n",
        "    text = text.lstrip()\n",
        "\n",
        "    ## make double spaces become one space.\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    ## remove stop words (English).\n",
        "    # print(f\"Before stopwords removal, text.len = {len(text)}\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words_removed_words_list = [t for t in tokens if not t in stop_words]\n",
        "\n",
        "    # print(f\"After tokenize and stopwords, len stop_words_removed_words_list = {len(stop_words_removed_words_list)}\")\n",
        "\n",
        "    ## apply lemmatization.\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stop_words_removed_words_list]\n",
        "    \n",
        "    ## apply stemming.\n",
        "    stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
        "\n",
        "    ## return the final pre-processed list-of-words.\n",
        "    return stemmed_words\n",
        "\n",
        "################ Test on one sentence #######################\n",
        "\n",
        "## Obtain once.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "## Preprocess per element of dataframe to test.\n",
        "text_to_process = df_train['content'].iloc[499]\n",
        "print(text_to_process)\n",
        "\n",
        "print(\"--\"*90)\n",
        "\n",
        "preprocessed_text = pre_process_stem_sentence(sentence=text_to_process, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(preprocessed_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>Well, according to this <a href=\"http://dragonball.wikia.com/wiki/Pan#Trivia\">Pan's trivia section</a>, the <strong>in-real-world reason</strong> seems to be that Akira Toriyama couldn't figure out how to draw a female SSJ. </p>\n",
            "\n",
            "<p>One of the possible reasons, <strong>in the DB universe</strong>, is that Pan never really trained to become a more powerful warrior. The other kids, through the generations, managed to reach that level quite quickly yes, but they trained a lot too. Pan simply didn't feel that need that much.</p>\n",
            "\n",
            "<p>The female SSJ appear in the videogames only.</p>\n",
            "\n",
            "<p>Just for completeness, I discovered that according to an italian collection on the DB franchise, one of the fundamental requirements to become SSJ is to be a male. That would explain why Goku Jr and Vegeta Jr can still become SSJ regardless of the blood being diluted. This has been said across the internet, but there's nothing official about this.</p>\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "['well', 'accord', 'pan', 'trivia', 'section', 'real', 'world', 'reason', 'seem', 'akira', 'toriyama', 'couldnt', 'figur', 'draw', 'femal', 'ssj', 'one', 'possibl', 'reason', 'db', 'univers', 'pan', 'never', 'realli', 'train', 'becom', 'power', 'warrior', 'kid', 'gener', 'manag', 'reach', 'level', 'quit', 'quickli', 'ye', 'train', 'lot', 'pan', 'simpli', 'didnt', 'feel', 'need', 'much', 'femal', 'ssj', 'appear', 'videogam', 'complet', 'discov', 'accord', 'italian', 'collect', 'db', 'franchis', 'one', 'fundament', 'requir', 'becom', 'ssj', 'male', 'would', 'explain', 'goku', 'jr', 'vegeta', 'jr', 'still', 'becom', 'ssj', 'regardless', 'blood', 'dilut', 'said', 'across', 'internet', 'there', 'noth', 'offici']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4gGdZmwZDZ",
        "outputId": "9d3e0ff2-3dbb-4085-a276-3a9afc1f8afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "doc = df_val['content'].iloc[499]\n",
        "print(doc)\n",
        "\n",
        "print(\"--\"*85)\n",
        "\n",
        "preprocessed = pre_process_stem_sentence(sentence=doc, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(len(preprocessed))\n",
        "print(preprocessed)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>I've found some information on <a href=\"http://naruto.wikia.com/wiki/Madara_Uchiha\" rel=\"nofollow\">this</a> site why Madara got his eyes and his young body.</p>\n",
            "\n",
            "<blockquote>\n",
            "  <p>Due to the manner of his revival and the experiments done by Kabuto, Madara retained access to abilities he obtained late in life (like his Wood Release and Rinnegan), while at the same time retaining the youthful body of his prime.</p>\n",
            "</blockquote>\n",
            "\n",
            "<p>So he hasn't been revived \"normally\"</p>\n",
            "\n",
            "<p>Since Nagato never lost his eyes he's revived with them too. So Madara actually did duplicate his eyes with the help of Kabuto. Since we don't know what would have happened if he would have been revived \"normally\" (at least not yet) we can't say whether it is that easy to duplicate the eyes or it was a coincidence that the eyes are now duplicated.</p>\n",
            "\n",
            "<p>Long answer short: \n",
            "It is possible to duplicate the eyes.</p>\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "72\n",
            "['ive', 'found', 'inform', 'site', 'madara', 'got', 'eye', 'young', 'bodi', 'due', 'manner', 'reviv', 'experi', 'done', 'kabuto', 'madara', 'retain', 'access', 'abil', 'obtain', 'late', 'life', 'like', 'wood', 'releas', 'rinnegan', 'time', 'retain', 'youth', 'bodi', 'prime', 'hasnt', 'reviv', 'normal', 'sinc', 'nagato', 'never', 'lost', 'eye', 'he', 'reviv', 'madara', 'actual', 'duplic', 'eye', 'help', 'kabuto', 'sinc', 'dont', 'know', 'would', 'happen', 'would', 'reviv', 'normal', 'least', 'yet', 'cant', 'say', 'whether', 'easi', 'duplic', 'eye', 'coincid', 'eye', 'duplic', 'long', 'answer', 'short', 'possibl', 'duplic', 'eye']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbSKXXrRGGgo"
      },
      "source": [
        "## Preprocess for each sentence of train-dataframe.\n",
        "df_train[\"stemmed_content\"] = df_train.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_val[\"stemmed_content\"] = df_val.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_test[\"stemmed_content\"] = df_test.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puTtacUoHBZ9"
      },
      "source": [
        "# Form Vocabulary from dataframe_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwyJWOOIETs"
      },
      "source": [
        "def print_first_n_keys_and_vals_dict(dictionary, n=5):\n",
        "    print(f\"Len dictionary = {len(dictionary)}, printing first {n} keys, vals\")\n",
        "    itr = 0\n",
        "    for key in dictionary:\n",
        "        print(key, dictionary[key])\n",
        "        itr += 1\n",
        "        if itr == n:\n",
        "            break"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO4hhJYkXqRp",
        "outputId": "22bb66d8-b2ab-466b-bc29-84655bf9d0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Also remove one-len words\n",
        "dictionary_vocab = {} ## empty dict.\n",
        "for doc in df_train['stemmed_content']:\n",
        "    # print(f\"len doc = {len(doc)}\")\n",
        "    for word in doc:\n",
        "        if len(word) <= 1:\n",
        "            continue\n",
        "\n",
        "        len_currently = len(dictionary_vocab) ## add to len. [idx new]\n",
        "\n",
        "        if word not in dictionary_vocab:\n",
        "            dictionary_vocab[word] = (0, len_currently)\n",
        "        else:\n",
        "            (val, idx) = dictionary_vocab[word]\n",
        "            dictionary_vocab[word] = (val + 1, idx) ## Maintain the same index.\n",
        "\n",
        "print(f\"len dictionary_vocab = {len(dictionary_vocab)}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len dictionary_vocab = 8702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSQYB4eBf0sv"
      },
      "source": [
        "# file_name = \"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Vocab.txt\"\n",
        "# with open(file_name, 'w') as fw:\n",
        "#     for voc in dictionary_vocab:\n",
        "#         fw.write(voc)\n",
        "#         fw.write(\"\\n\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAgtrtyp13Tt"
      },
      "source": [
        "# ## https://www.kite.com/python/answers/how-to-save-a-dictionary-to-a-file-in-python\n",
        "# vocab_file = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/vocab_dict_df_train.pkl'\n",
        "\n",
        "# a_file = open(vocab_file, \"wb\")\n",
        "# pickle.dump(dictionary_vocab, a_file)\n",
        "# a_file.close()\n",
        "\n",
        "# # a_file = open(vocab_file, \"rb\")\n",
        "# # dictionary_vocab = pickle.load(a_file)\n",
        "# # print(dictionary_vocab)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJORdHUjHd7h"
      },
      "source": [
        "## Keep only those docs in train whose lengths are above 3 words i.e.\n",
        "### length of stemmed content > 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrZO1SycNmSX",
        "outputId": "c0fdae2d-31c6-44c9-ba96-3ea0bc691d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lens = [len(x) for x in df_train['stemmed_content']]\n",
        "print(max(lens))\n",
        "print(min(lens))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1381\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPJ5IMKKNmQV",
        "outputId": "2cb95b6c-90d5-4c30-91b3-6393ac7ee733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MIN_WORD_COUNT_TO_REMOVE = 3\n",
        "# print(df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE][\"stemmed_content\"])\n",
        "idxToRemove = df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE].index\n",
        "df_train.drop(idxToRemove , inplace=True)\n",
        "\n",
        "print(f\"After removal, len df_train = {len(df_train)}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After removal, len df_train = 1454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQCXuG-JhUU"
      },
      "source": [
        "## Create Hamming Distance Vectors by representing with 0/1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJfu0JkdKFRh"
      },
      "source": [
        "def form_hamming_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    for word in list_words:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index. [DO NOT]\n",
        "            # vec[UNKNOWN_WORD_INDEX] = 1\n",
        "            continue\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word]\n",
        "            vec[idx] = 1\n",
        "    return vec"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNrTRsZ2psv-"
      },
      "source": [
        "## Create hamming vectors for each col of dataframe\n",
        "df_train[\"ham_vector\"] = df_train.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"ham_vector\"] = df_val.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"ham_vector\"] = df_test.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNM1YUEWrfgk"
      },
      "source": [
        "## Create eucledian vectors by representing how many times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJXmvI8areMJ"
      },
      "source": [
        "def form_eucledian_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    \n",
        "    ## Form a small dictionary to store each word count\n",
        "    dict_local_vocab = {}\n",
        "    for word in list_words:\n",
        "        if word not in dict_local_vocab:\n",
        "            dict_local_vocab[word] = 1\n",
        "        else:\n",
        "            dict_local_vocab[word] = dict_local_vocab[word] + 1\n",
        "\n",
        "    # print(dict_local_vocab)\n",
        "\n",
        "    # unknown_word_count = 1\n",
        "    for word in dict_local_vocab:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index.\n",
        "            continue\n",
        "            # vec[UNKNOWN_WORD_INDEX] = unknown_word_count\n",
        "            # unknown_word_count += 1\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word] \n",
        "            vec[idx] = dict_local_vocab[word] ## replace with value of this word instead of 1.\n",
        "\n",
        "    del dict_local_vocab\n",
        "    return vec"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvxfO7dkpszM"
      },
      "source": [
        "df_train[\"euc_vector\"] = df_train.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"euc_vector\"] = df_val.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"euc_vector\"] = df_test.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ImEkZTTwY_"
      },
      "source": [
        "# FOLDER_DATASET = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset'\n",
        "# df_train.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/train.csv', index=False)\n",
        "# df_val.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/val.csv', index=False)\n",
        "# df_test.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/test.csv', index=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEuOjilfKauq"
      },
      "source": [
        "# Stack each of these vertically to form hamming and eucledian vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3Cwy-2xb4s"
      },
      "source": [
        "def get_2D_vector(list_np_arr):\n",
        "    vec_2D = np.zeros((len(list_np_arr), len(list_np_arr[0])))\n",
        "    idx = 0\n",
        "    for vec in list_np_arr:\n",
        "        vec_2D[idx] = vec\n",
        "        idx += 1\n",
        "    return vec_2D"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB_4H48rL6kv",
        "outputId": "92487e7d-8007-4e55-d5de-efc4a0b7b9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "vocab_size = len(dictionary_vocab) ## PLUS 1 for <UNKNOWN> word\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "\n",
        "num_documents = len(df_train)\n",
        "print(f\"num_documents = {num_documents}\")\n",
        "\n",
        "print(df_train.columns.values)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size = 8702\n",
            "num_documents = 1454\n",
            "['content' 'Label' 'stemmed_content' 'ham_vector' 'euc_vector']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oodrsKK6GlG",
        "outputId": "f44ee817-a45a-40e7-9f65-2ce1620190f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Obtain vectors 2D representation ###\n",
        "hamming_vectors_2D = get_2D_vector(list_np_arr=df_train['ham_vector'].values)\n",
        "eucledian_vectors_2D = get_2D_vector(list_np_arr=df_train['euc_vector'].values)\n",
        "print(hamming_vectors_2D.shape, eucledian_vectors_2D.shape)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1454, 8702) (1454, 8702)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrEqp1_0LH5"
      },
      "source": [
        "## For now save these dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViUb7lUGzp9N"
      },
      "source": [
        "# df_train.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/train.csv\", index=False)\n",
        "# df_val.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/val.csv\", index=False)\n",
        "# df_test.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/test.csv\", index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmALIJNfzuMJ"
      },
      "source": [
        "## Now, finally create TF-IDF and pickel dump everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__-GfML7BDN",
        "outputId": "5b5935f3-c893-48cf-b51f-0f376996e325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# %%time\n",
        "def compute_TF_IDF_all_train_set(hamming_vectors_2D, eucledian_vectors_2D, alpha=0.0001, beta=0.0001):\n",
        "    num_documents, num_words = eucledian_vectors_2D.shape\n",
        "    print(f\"num_documents, num_words = {num_documents, num_words}\")\n",
        "\n",
        "    TF = np.zeros((num_documents, num_words))\n",
        "\n",
        "    IDF = np.zeros((1, num_words))\n",
        "\n",
        "    ## Calculate TF(d, w) = N(d, w)/W(d) ; where N(d, w): count(w) in document d , W(d): Total #words in document d\n",
        "    for itr in range(num_documents): ## iterate row-wise\n",
        "        doc_eucledian = eucledian_vectors_2D[itr]\n",
        "        total_num_words_doc_eucledian = num_words - np.sum(doc_eucledian == 0)        \n",
        "        TF[itr] = doc_eucledian/total_num_words_doc_eucledian\n",
        "\n",
        "        # if itr == 1:\n",
        "            # print(f\"itr = {itr}, doc_euc[itr] = {np.unique(doc_eucledian[itr], return_counts=True)}\")\n",
        "            # print(f\"itr = {itr}, TF[itr] = {np.unique(TF[itr], return_counts=True)}\")\n",
        "            # break\n",
        "\n",
        "    ## Calculate IDF(d, w) = log( (D + alpha)/(C(w) + beta) ) ; C(w) -> Total # docs with word 'w' ; D -> Total # documents\n",
        "    D = num_documents\n",
        "    for itr in range(num_words): ## itereate col-wise\n",
        "        C_w = np.sum(hamming_vectors_2D[:, itr] == 1) ## first calculate C(w)\n",
        "        \n",
        "        IDF[:, itr] = np.log( (D + alpha) / (C_w + beta) )\n",
        "\n",
        "        # break\n",
        "        \n",
        "    TF_IDF = TF*IDF\n",
        "    del TF\n",
        "    # del IDF\n",
        "    return TF_IDF, IDF\n",
        "\n",
        "TF_IDF_whole_corpus, IDF_whole_corpus = compute_TF_IDF_all_train_set(hamming_vectors_2D=hamming_vectors_2D, eucledian_vectors_2D=eucledian_vectors_2D)\n",
        "print(f\"TF_IDF_whole_corpus.shape = {TF_IDF_whole_corpus.shape}\")\n",
        "print(f\"IDF_whole_corpus.shape = {IDF_whole_corpus.shape}\")"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_documents, num_words = (1454, 8702)\n",
            "TF_IDF_whole_corpus.shape = (1454, 8702)\n",
            "IDF_whole_corpus.shape = (1, 8702)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWeLmV_094XH",
        "outputId": "bee64028-454a-4806-d110-059d49b3fd64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def form_TF_IDF_for_val_test(list_words, IDF, hamming_vectors_2D, eucledian_vectors_2D, vocab_dict, alpha=0.0001, beta=0.0001, epslion=0.000001):\n",
        "    num_docs_train, num_words = eucledian_vectors_2D.shape\n",
        "\n",
        "    TF = np.zeros((1, num_words))\n",
        "    \n",
        "    euc_vec = form_eucledian_vector(list_words=list_words, vocab_dict=vocab_dict)\n",
        "    \n",
        "    W_d_num_words_in_document = 0\n",
        "    for word in list_words:\n",
        "        if word in vocab_dict:\n",
        "            W_d_num_words_in_document += 1\n",
        "\n",
        "    TF = euc_vec/(W_d_num_words_in_document + epslion)\n",
        "\n",
        "    TF_IDF = TF*IDF\n",
        "\n",
        "    return TF_IDF\n",
        "\n",
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "\n",
        "TF_IDF = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                    eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "print(f\"TF_IDF.shape = {TF_IDF.shape}\")\n",
        "# print(np.unique(TF_IDF, return_counts=True))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF_IDF.shape = (1, 8702)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us7RvCom6UuF",
        "outputId": "b4446c53-21a1-4f85-fc27-41a7fedef949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df_val.columns.values)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['content' 'Label' 'stemmed_content' 'ham_vector' 'euc_vector']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohbmnfhCXJfD"
      },
      "source": [
        "# Now we start with K-Nearest Neighbor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBzTc76r7VNj"
      },
      "source": [
        "#### Form validation 2D set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFmtTTRF7Cmw",
        "outputId": "2929bba9-e447-4f48-aa0c-c05ab97edaf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hamming_vectors_2D_validation = get_2D_vector(list_np_arr=df_val['ham_vector'].values)\n",
        "euclidean_vectors_2D_validation = get_2D_vector(list_np_arr=df_val['euc_vector'].values)\n",
        "print(hamming_vectors_2D_validation.shape, euclidean_vectors_2D_validation.shape)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600, 8702) (600, 8702)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOdAYwV-5iVa",
        "outputId": "e92bdf30-ece6-4321-cc56-6610a3cfdd06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf_idf_validation_set = np.asarray([\n",
        "    form_TF_IDF_for_val_test(list_words=x, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "        eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001) for x in df_val['stemmed_content'].values\n",
        "])\n",
        "tf_idf_validation_set = tf_idf_validation_set.reshape(tf_idf_validation_set.shape[0], -1)\n",
        "print(f\"tf_idf_validation_set.shape = {tf_idf_validation_set.shape}\")"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_idf_validation_set.shape = (600, 8702)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RN0SomM8JCp"
      },
      "source": [
        "## Similarity functions for 2D vectorized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDbTTsgSDXUT"
      },
      "source": [
        "###### For vectorized ############\n",
        "def ham(a, b):\n",
        "    # return np.count_nonzero((a!=b[:, None]), axis=-1) ## returns 3-D matrix. Don't use.\n",
        "    return pairwise_distances(a, b, metric='euclidean') ## since binary vectors will return hamming-distance\n",
        "\n",
        "def euclidean(a, b):\n",
        "    # return np.linalg.norm((a-b[:, None]), axis=-1)\n",
        "    return pairwise_distances(a, b, metric='euclidean')\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    # return (b.dot(a.T))/(np.linalg.norm(a, axis=1) * np.linalg.norm(b[:, None], axis=-1)) ## (a@b.T).T  ## to get r2*r1\n",
        "    return pairwise_distances(a, b, metric='cosine')"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu-Y52OvggBm",
        "cellView": "form"
      },
      "source": [
        "#@title Similarity functions without pairwise-distances\n",
        "####### For single loop ############\n",
        "# ## Similarity functions.\n",
        "# def ham(a, b):\n",
        "#     return np.count_nonzero((a!=b), axis=1)\n",
        "\n",
        "# def euclidean(a, b):\n",
        "#     return np.linalg.norm((a-b), axis=1)\n",
        "\n",
        "# ### https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
        "# def cosine_similarity(a, b):\n",
        "#     cos_sim = np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "#     return cos_sim\n",
        "\n",
        "\n",
        "# a = np.array([x for x in range(40)])\n",
        "# a = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1])\n",
        "# a = a.reshape(5, -1)\n",
        "# print(\"a: \\n\", a)\n",
        "\n",
        "# b = np.array([1, 0, 1])\n",
        "# b = b.reshape(1, -1)\n",
        "# print(\"\\nb: \", b)\n",
        "\n",
        "\n",
        "# # print(ham(a, b))\n",
        "# print(f\"\\nEuclidean(a, b) = {euclidean(a, b)}\")\n",
        "\n",
        "# a = np.array([2, 1, 3, 4, 5, 1, 10, 3, 22])\n",
        "# n = 4\n",
        "# indices_top = (-a).argsort()[:n]\n",
        "# print(f\"\\n n = {n}, indices_top = {indices_top}\")\n",
        "# print(a[indices_top])"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnAxbCgrtNyq",
        "outputId": "2d53878c-966d-4446-a29d-90a5a5869bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "labels_train = df_train['Label'].values\n",
        "print(labels_train.shape)\n",
        "\n",
        "labels_val = df_val['Label'].values\n",
        "print(labels_val.shape)\n",
        "\n",
        "val_euc_0 = euclidean_vectors_2D_validation[0:2]\n",
        "print(val_euc_0.shape)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1454,)\n",
            "(600,)\n",
            "(2, 8702)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GQLpzcgYY_L",
        "outputId": "0c55122a-e867-4216-ead0-b9b05b5ce2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "### KNN Vectorized.\n",
        "class KNN:\n",
        "    def __init__(self, mode=\"hamming\", to_print=False):\n",
        "        self.mode = mode\n",
        "        if to_print == True:\n",
        "            print(f\"KNN __init__(mode={mode})\")\n",
        "        \n",
        "\n",
        "    def compute_distances(self, v):\n",
        "        # print(f\"v.shape = {v.shape}\")\n",
        "        if self.mode == \"hamming\":\n",
        "            self.distances = ham(v, self.vectors_corpus)\n",
        "        elif self.mode == \"euclidean\":\n",
        "            self.distances = euclidean(v, self.vectors_corpus)\n",
        "        elif self.mode == 'cosine_similarity':\n",
        "            self.distances = cosine_similarity(v, self.vectors_corpus)\n",
        "\n",
        "        # print(f\"After compute_distances mode={self.mode}, distances.shape = {self.distances.shape}\")\n",
        "        # print(f\"{self.distances}\")\n",
        "\n",
        "    def populate_vectors(self, vectors_corpus, labels):\n",
        "        self.vectors_corpus = vectors_corpus\n",
        "        self.labels = labels\n",
        "\n",
        "    def predict(self, v, num_neighbors, compute_distance_flag=True):\n",
        "        K = num_neighbors\n",
        "        ## compute distances by using suitable similarity function.\n",
        "        if compute_distance_flag == True:\n",
        "            self.compute_distances(v)\n",
        "\n",
        "        ## take argmax top results indices\n",
        "        ## (-v.T).argsort(axis=0)[:K].reshape(-1, )\n",
        "        indices_top = (self.distances.T).argsort(axis=0)[:K] ## highest value is negative distance ? Don't know why, +ve should be taken.\n",
        "\n",
        "        ## apply indices to labels\n",
        "        top_labels = self.labels[indices_top]\n",
        "\n",
        "        ## take majority vote and return the label\n",
        "        top_most_label = stats.mode(top_labels)[0][0]\n",
        "\n",
        "        ## return the max label\n",
        "        return top_most_label\n",
        "\n",
        "################################################# Test #################################################\n",
        "\n",
        "# knn = KNN(mode='hamming')\n",
        "# knn.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "# knn.predict(val_ham_0, num_neighbors=1)\n",
        "\n",
        "knn = KNN(mode='euclidean')\n",
        "knn.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "ans = knn.predict(val_euc_0, num_neighbors=1)\n",
        "print(ans.shape)\n",
        "print(ans)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2,)\n",
            "['Coffee' 'Arduino']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RQnLJLEXI2X",
        "cellView": "form"
      },
      "source": [
        "#@title KNN Non-Vectorized\n",
        "# class KNN:\n",
        "#     def __init__(self, K = 1, mode=\"hamming\", to_print=False):\n",
        "#         self.mode = mode\n",
        "#         self.K = K\n",
        "#         if to_print == True:\n",
        "#             print(f\"KNN __init__(K={K}, mode={mode})\")\n",
        "        \n",
        "\n",
        "#     def populate_vectors(self, vectors_corpus, labels):\n",
        "#         self.vectors_corpus = vectors_corpus\n",
        "#         self.labels = labels\n",
        "\n",
        "#     def compute_distances(self, v):\n",
        "#         if self.mode == \"hamming\":\n",
        "#             v = v.reshape(1, -1) ## Reshape vector.\n",
        "#             self.distances = ham(v, self.vectors_corpus)\n",
        "#         elif self.mode == \"euclidean\":\n",
        "#             v = v.reshape(1, -1) ## Reshape vector.\n",
        "#             self.distances = euclidean(v, self.vectors_corpus)\n",
        "#         elif self.mode == 'cosine_similarity':\n",
        "#             v = v.reshape(-1)\n",
        "#             self.distances = cosine_similarity(v, self.vectors_corpus)\n",
        "\n",
        "#         # print(f\"After compute_distances mode={self.mode}, distances = {self.distances}\")\n",
        "\n",
        "#     def predict(self, v):\n",
        "#         ## compute distances by using suitable similarity function.\n",
        "#         self.compute_distances(v)\n",
        "\n",
        "#         ## take argmax top results indices\n",
        "#         if self.mode == 'cosine_similarity':\n",
        "#             indices_top = (-self.distances).argsort()[:self.K] ## for some reason, this works for cosine-similarity\n",
        "#         else:\n",
        "#             indices_top = (self.distances).argsort()[:self.K] ## THIS works for hamming and euclidean\n",
        "\n",
        "#         ## apply indices to labels\n",
        "#         top_labels = self.labels[indices_top]\n",
        "\n",
        "#         ## take majority vote and return the label\n",
        "#         top_most_label = stats.mode(top_labels)[0][0]\n",
        "\n",
        "#         ## return the max label\n",
        "#         return top_most_label\n",
        "\n",
        "# ################################################# Test #################################################\n",
        "\n",
        "# knn = KNN(K=1, mode='hamming')\n",
        "# knn.populate_vectors(hamming_vectors_2D, labels)\n",
        "# knn.predict(val_ham_0)\n",
        "\n",
        "# # knn = KNN(K=7, mode='euclidean')\n",
        "# # knn.populate_vectors(eucledian_vectors_2D, labels)\n",
        "# # knn.predict(val_euc_0)"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9LVZRhU8sle"
      },
      "source": [
        "## Test for one value of K"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-qoC6HLuIdN",
        "outputId": "63710c07-c868-4079-aa2d-a6b275213b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "###### Test for one K #######\n",
        "K = 1\n",
        "knn = KNN(mode='hamming')\n",
        "knn.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "y_preds = knn.predict(hamming_vectors_2D_validation, num_neighbors=K)\n",
        "acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "print(f\"Acc Hamming K = {K} = {acc}%\")\n",
        "\n",
        "# print(f\"Acc Hamming K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")\n",
        "\n",
        "\n",
        "\n",
        "# knn = KNN(mode='euclidean')\n",
        "# knn.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "# y_preds = knn.predict(euclidean_vectors_2D_validation, num_neighbors=K)\n",
        "\n",
        "# print(f\"Acc Euclidean K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")\n",
        "\n",
        "\n",
        "# knn = KNN(mode='cosine_similarity')\n",
        "# knn.populate_vectors(TF_IDF_whole_corpus, labels_train)\n",
        "# y_preds = knn.predict(tf_idf_validation_set, num_neighbors=K)\n",
        "\n",
        "# print(f\"Acc TF-IDF-Cosine K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc Hamming K = 1 = 72.16666666666667%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wzDleMoXe-b"
      },
      "source": [
        "## Applying tests on validation set for KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ejjdHag_sJZ",
        "outputId": "482712ac-632c-4f3a-f904-c1f9d4eecb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# column_names = [\"Similarity-Measure\", \"K\", \"Accuracy(%)\"]\n",
        "# df_results_knn = pd.DataFrame(columns=column_names)\n",
        "# print(df_results_knn.head())"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Similarity-Measure, K, Accuracy(%)]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKPaf3QVXens",
        "outputId": "a6b33d32-4bdb-461a-d41f-9292f95d078d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%%time\n",
        "## Initialize results dataframe\n",
        "column_names = [\"Similarity-Measure\", \"K\", \"Accuracy(%)\"]\n",
        "df_results_knn = pd.DataFrame(columns=column_names)\n",
        "\n",
        "### Initialize objects\n",
        "knn_ham = KNN(mode='hamming')\n",
        "knn_euc = KNN(mode='euclidean')\n",
        "knn_cosine = KNN(mode='cosine_similarity')\n",
        "\n",
        "### Populate initial vectors\n",
        "knn_ham.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "knn_euc.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "knn_cosine.populate_vectors(TF_IDF_whole_corpus, labels_train)\n",
        "\n",
        "### Compute distances for each val-set\n",
        "knn_ham.compute_distances(hamming_vectors_2D_validation)\n",
        "knn_euc.compute_distances(euclidean_vectors_2D_validation)\n",
        "knn_cosine.compute_distances(tf_idf_validation_set)\n",
        "\n",
        "### Predict for each value of K\n",
        "for k in [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23]:\n",
        "    ## Predict and Append to dataframe.  \n",
        "    ## Signature: def predict(self, v, num_neighbors, compute_distance_flag=True)\n",
        "    y_preds = knn_ham.predict(hamming_vectors_2D_validation, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Hamming\", k, acc])\n",
        "    \n",
        "    y_preds = knn_euc.predict(hamming_vectors_2D_validation, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Euclidean\", k, acc])\n",
        "\n",
        "    y_preds = knn_cosine.predict(hamming_vectors_2D_validation, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"TF-IDF-cosine-sim\", k, acc])\n",
        "    \n",
        "\n",
        "### Delete each objects\n",
        "del knn_ham\n",
        "del knn_euc\n",
        "del knn_cosine\n",
        "\n",
        "### Print\n",
        "print(f\"len df_results_knn = {len(df_results_knn)}\")\n",
        "print(df_results_knn.head(3))"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_knn = 36\n",
            "  Similarity-Measure  K  Accuracy(%)\n",
            "0            Hamming  1    72.166667\n",
            "1          Euclidean  1    78.500000\n",
            "2  TF-IDF-cosine-sim  1    93.333333\n",
            "CPU times: user 5.8 s, sys: 276 ms, total: 6.08 s\n",
            "Wall time: 4.44 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvq54JeJXelm",
        "outputId": "b6c6cdfc-4cec-4911-fc9a-901dc0baf63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f\"len df_results_knn = {len(df_results_knn)}\")\n",
        "display(df_results_knn)\n",
        "# df_results_knn.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/KNN-val-3-topics.csv', index=False)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_knn = 36\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Similarity-Measure</th>\n",
              "      <th>K</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>1</td>\n",
              "      <td>72.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>1</td>\n",
              "      <td>78.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>1</td>\n",
              "      <td>93.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>3</td>\n",
              "      <td>78.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>3</td>\n",
              "      <td>80.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>3</td>\n",
              "      <td>95.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>5</td>\n",
              "      <td>78.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>5</td>\n",
              "      <td>81.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>5</td>\n",
              "      <td>95.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>7</td>\n",
              "      <td>76.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>7</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>7</td>\n",
              "      <td>95.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>9</td>\n",
              "      <td>75.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>9</td>\n",
              "      <td>78.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>9</td>\n",
              "      <td>95.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>11</td>\n",
              "      <td>76.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>11</td>\n",
              "      <td>81.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>11</td>\n",
              "      <td>95.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>13</td>\n",
              "      <td>75.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>13</td>\n",
              "      <td>81.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>13</td>\n",
              "      <td>95.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>15</td>\n",
              "      <td>75.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>15</td>\n",
              "      <td>80.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>15</td>\n",
              "      <td>96.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>17</td>\n",
              "      <td>75.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>17</td>\n",
              "      <td>80.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>17</td>\n",
              "      <td>95.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>19</td>\n",
              "      <td>75.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>19</td>\n",
              "      <td>80.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>19</td>\n",
              "      <td>95.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>21</td>\n",
              "      <td>76.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>21</td>\n",
              "      <td>79.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>21</td>\n",
              "      <td>96.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>23</td>\n",
              "      <td>76.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>23</td>\n",
              "      <td>80.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>23</td>\n",
              "      <td>95.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Similarity-Measure   K  Accuracy(%)\n",
              "0             Hamming   1    72.166667\n",
              "1           Euclidean   1    78.500000\n",
              "2   TF-IDF-cosine-sim   1    93.333333\n",
              "3             Hamming   3    78.333333\n",
              "4           Euclidean   3    80.333333\n",
              "5   TF-IDF-cosine-sim   3    95.333333\n",
              "6             Hamming   5    78.333333\n",
              "7           Euclidean   5    81.000000\n",
              "8   TF-IDF-cosine-sim   5    95.500000\n",
              "9             Hamming   7    76.333333\n",
              "10          Euclidean   7    80.000000\n",
              "11  TF-IDF-cosine-sim   7    95.833333\n",
              "12            Hamming   9    75.833333\n",
              "13          Euclidean   9    78.333333\n",
              "14  TF-IDF-cosine-sim   9    95.833333\n",
              "15            Hamming  11    76.000000\n",
              "16          Euclidean  11    81.166667\n",
              "17  TF-IDF-cosine-sim  11    95.833333\n",
              "18            Hamming  13    75.333333\n",
              "19          Euclidean  13    81.666667\n",
              "20  TF-IDF-cosine-sim  13    95.333333\n",
              "21            Hamming  15    75.000000\n",
              "22          Euclidean  15    80.166667\n",
              "23  TF-IDF-cosine-sim  15    96.000000\n",
              "24            Hamming  17    75.166667\n",
              "25          Euclidean  17    80.500000\n",
              "26  TF-IDF-cosine-sim  17    95.500000\n",
              "27            Hamming  19    75.666667\n",
              "28          Euclidean  19    80.833333\n",
              "29  TF-IDF-cosine-sim  19    95.666667\n",
              "30            Hamming  21    76.833333\n",
              "31          Euclidean  21    79.500000\n",
              "32  TF-IDF-cosine-sim  21    96.166667\n",
              "33            Hamming  23    76.333333\n",
              "34          Euclidean  23    80.166667\n",
              "35  TF-IDF-cosine-sim  23    95.333333"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K-q5b6ZzqVP"
      },
      "source": [
        "mat = df_results_knn.values\n",
        "print(mat.shape)\n",
        "print(mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0LcdCK_RW8E"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBwmpIgW4XOD"
      },
      "source": [
        "### Combine all documents of each class i into one document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EagCOsAhzqiu"
      },
      "source": [
        "# unique_labels = np.unique(df_train['Label'].values)\n",
        "# dictionary_list_words_for_NB = {}\n",
        "# # for label in unique_labels:\n",
        "# #     if label not in dictionary_list_words:\n",
        "# #         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "    \n",
        "# for (list_words, label) in zip(df_train['stemmed_content'].values, df_train['Label'].values):\n",
        "#     if label not in dictionary_list_words_for_NB:\n",
        "#         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "#     dictionary_list_words_for_NB[label].append(list_words)\n",
        "\n",
        "# ## reduce/flat-out to make 1D list.\n",
        "# ## https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
        "# for label in dictionary_list_words_for_NB:\n",
        "#     dictionary_list_words_for_NB[label] = reduce(operator.concat, dictionary_list_words_for_NB[label])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkQbU3mCONb"
      },
      "source": [
        "## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "def keywithmaxval(d):\n",
        "     \"\"\" a) create a list of the dict's keys and values; \n",
        "         b) return the key with the max value\"\"\"  \n",
        "     v=list(d.values())\n",
        "     k=list(d.keys())\n",
        "     return k[v.index(max(v))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAcwIAKOFMJF",
        "outputId": "90999c8b-317e-449c-afb1-f8bbfa30ccda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self, vocab_size, alpha=0.01, to_print=False):\n",
        "        if to_print == True:\n",
        "            print(f\"NaiveBayes __init(alpha={alpha})__\")\n",
        "        self.alpha = alpha\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dictionary_list_words_for_NB = {}\n",
        "        self.dictionary_prior_probabilities = {}\n",
        "        self.dictionary_count_words_per_class = {}\n",
        "        self.dictionary_total_words_per_class = {}\n",
        "    \n",
        "\n",
        "    def fit(self, list_list_words, labels):\n",
        "        self.list_list_words = list_list_words\n",
        "        self.labels = labels\n",
        "        self.form_dictionary_list_words()\n",
        "        self.compute_probabilities()\n",
        "\n",
        "\n",
        "    def form_dictionary_list_words(self):\n",
        "        for (list_words, label) in zip(self.list_list_words, self.labels):\n",
        "            if label not in self.dictionary_list_words_for_NB:\n",
        "                self.dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "            self.dictionary_list_words_for_NB[label].append(list_words)\n",
        "        \n",
        "        for label in self.dictionary_list_words_for_NB: ## reduce/flat-out to make 1D list.\n",
        "            self.dictionary_list_words_for_NB[label] = reduce(operator.concat, self.dictionary_list_words_for_NB[label])\n",
        "\n",
        "\n",
        "    def compute_probabilities(self):\n",
        "        ## Compute prior probabilities/initial guesses\n",
        "        (classes, cnts) = np.unique(self.labels, return_counts=True)\n",
        "        cnts = cnts/np.sum(cnts) ## C_i / (C_1 + C_2 + ... + C_n)\n",
        "        for (lab, itr) in zip(classes, range(len(classes))):\n",
        "            self.dictionary_prior_probabilities[lab] = cnts[itr]\n",
        "\n",
        "        ## Compute per-word probabilities\n",
        "\n",
        "        ## Counter increment\n",
        "        for lab in classes:\n",
        "            num_words_this_class = 0\n",
        "            self.dictionary_count_words_per_class[lab] = {}\n",
        "            for word in self.dictionary_list_words_for_NB[lab]:\n",
        "                if word not in self.dictionary_count_words_per_class[lab]:\n",
        "                    self.dictionary_count_words_per_class[lab][word] = 0 ## initialize counter to 0.\n",
        "                self.dictionary_count_words_per_class[lab][word] = self.dictionary_count_words_per_class[lab][word] + 1 ## increment counter\n",
        "                num_words_this_class += 1\n",
        "            self.dictionary_total_words_per_class[lab] = num_words_this_class\n",
        "\n",
        "    def predict(self, list_words):\n",
        "        ## Compute probabilities for each label.\n",
        "        dict_probabilities_per_class = {}\n",
        "\n",
        "        for lab in np.unique(self.labels):\n",
        "            prob_log_curr_class = np.log(self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # prob_log_curr_class = (self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # print(f\"Before, prob_log_curr_class = {prob_log_curr_class}\")\n",
        "            for word in list_words: ## iterate per word\n",
        "                if word in self.dictionary_count_words_per_class[lab]: ## if word exists in THIS document.\n",
        "                    ## use smoothing factor alpha\n",
        "                    # prob_log_curr_class += np.log((self.dictionary_count_words_per_class[lab][word] + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size)) \n",
        "                    word_prob = self.dictionary_count_words_per_class[lab][word]\n",
        "                    # prob_log_curr_class = prob_log_curr_class*word_prob\n",
        "                else:\n",
        "                    word_prob = 0\n",
        "                prob_log_curr_class += np.log( (word_prob + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size) )\n",
        "\n",
        "\n",
        "            dict_probabilities_per_class[lab] = prob_log_curr_class\n",
        "            # print(dict_probabilities_per_class)\n",
        "\n",
        "        ## Get max probability class.\n",
        "        ## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "        return keywithmaxval(d=dict_probabilities_per_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################### Checking ############################################################\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=len(dictionary_vocab))\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "check = df_val['stemmed_content'].iloc[0]\n",
        "p = NB.predict(check)\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uILDNAL3FddD"
      },
      "source": [
        "list_new = [\n",
        "['coffee', 'tea', 'dew', 'dew', 'dew', 'dew'],\n",
        "['coffee', 'noir', 'homelander', 'dew'],\n",
        "['noir', 'noir', 'fool', 'fool', 'noir']\n",
        "]\n",
        "\n",
        "labs_new = [\n",
        "    'bev',\n",
        "    'supe',\n",
        "    'misc'\n",
        "]\n",
        "\n",
        "vocab_size = 6\n",
        "\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=6)\n",
        "NB.fit(list_new, labs_new)\n",
        "print(NB.predict(['coffee', 'tea', 'dew', 'dew', 'dew', 'dew']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f13yqf6EKAaJ"
      },
      "source": [
        "## Validation on NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgWpyxjCFMVM",
        "outputId": "4902df2d-4fb4-4ef1-c592-403221892c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"ALPHA\", \"Accuracy(%)\"]\n",
        "\n",
        "df_results_NB = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_results_NB.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ALPHA, Accuracy(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5FaA8KG8zu",
        "outputId": "0a5e720c-8719-4cf6-98c0-06524bfda118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "alpha_values = np.linspace(start=0.01, stop=1.0, num=10)\n",
        "print(alpha_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01 0.12 0.23 0.34 0.45 0.56 0.67 0.78 0.89 1.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rC01YFFMRn",
        "outputId": "a2ed1e7a-e575-4f32-b6b6-3453e6eb3e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "%%time\n",
        "### Validation NaiveBayes ###\n",
        "for alpha in (alpha_values):\n",
        "    NB = NaiveBayes(alpha=alpha, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "    NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "    nb_acc = 0\n",
        "    ## Predict each val set ##\n",
        "    for (x, y) in (zip(df_val['stemmed_content'].values, df_val['Label'].values)):\n",
        "        y_pred = NB.predict(x)\n",
        "        if y_pred == y:\n",
        "            nb_acc += 1\n",
        "\n",
        "\n",
        "    ## Append to dataframe and print.\n",
        "    # print(f\"NB alpha = {alpha}, accuracy = {nb_acc/len(df_val)*100} %\")\n",
        "    df_results_NB = add_to_dataframe(df_old=df_results_NB, to_add=[alpha, nb_acc/len(df_val)*100])\n",
        "    del NB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.01)__\n",
            "NaiveBayes __init(alpha=0.12)__\n",
            "NaiveBayes __init(alpha=0.23)__\n",
            "NaiveBayes __init(alpha=0.34)__\n",
            "NaiveBayes __init(alpha=0.45)__\n",
            "NaiveBayes __init(alpha=0.56)__\n",
            "NaiveBayes __init(alpha=0.67)__\n",
            "NaiveBayes __init(alpha=0.78)__\n",
            "NaiveBayes __init(alpha=0.89)__\n",
            "NaiveBayes __init(alpha=1.0)__\n",
            "CPU times: user 9.49 s, sys: 99.4 ms, total: 9.59 s\n",
            "Wall time: 9.46 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbwK_kMxTY"
      },
      "source": [
        "df_results_NB.sort_values(by=['Accuracy(%)'], inplace=True, ascending=False)\n",
        "display(df_results_NB.head(10))\n",
        "# df_results_NB.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/NB-val-3-topics.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtew8yEpMxep",
        "outputId": "1bf9aa42-9f68-46d8-8f88-030eb54cd13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "display(df_results_NB.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.03</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.07</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.25</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.06</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.09</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.08</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.15</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ALPHA  Accuracy(%)\n",
              "1    0.02    88.500000\n",
              "3    0.04    88.500000\n",
              "2    0.03    88.454545\n",
              "4    0.05    88.454545\n",
              "6    0.07    88.454545\n",
              "24   0.25    88.409091\n",
              "5    0.06    88.409091\n",
              "8    0.09    88.409091\n",
              "7    0.08    88.409091\n",
              "14   0.15    88.409091"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4G-LMq-nUTK"
      },
      "source": [
        "# Hypothesis testing on Test Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG7zmcwEnbbT"
      },
      "source": [
        "### Create and fit best performing models on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjLPICwjpSdJ",
        "outputId": "78366155-710a-4dd0-f8d4-630dc98f915c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "print(words_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sagul', 'give', 'deflect', 'valu', 'horizont', 'shelf', 'span', 'use', 'variou', 'materi', 'thick', 'unless', 'work', 'extrem', 'heavi', 'load', 'weight', 'countertop', 'peopl', 'danc', 'materi', 'vertic', 'cabinet', 'wall', 'adequ', 'make', 'wall', 'thinner', 'still', 'remain', 'surprisingli', 'strong', 'long', 'cabinet', 'design', 'prevent', 'rack', 'even', 'particleboard', 'known', 'strength', 'hold', 'well', 'vertic', 'compressionbuckl', 'may', 'know', 'youv', 'ever', 'pack', 'book', 'store', 'cheap', 'particleboard', 'bookcas']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaPA10d8na_G",
        "outputId": "c5d376bd-42cf-4c16-f606-2283ac3bb456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##### KNN was K=5, TF-IDF #####\n",
        "knn = KNN(K=5, mode='cosine_similarity', to_print=True)\n",
        "knn.populate_vectors(TF_IDF_whole_corpus, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN __init__(K=5, mode=cosine_similarity)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCgX5cNKpard",
        "outputId": "9719a77d-73a1-4f48-83ee-35aa3275f1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_tf_idf = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "print(knn.predict(val_tf_idf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOeVWWRdna3D",
        "outputId": "e7a77d69-d56a-4e9b-d1f2-7ef10077693f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##### Naive Bayes was alpha = 0.02/0.04, we will take 0.04 #####\n",
        "NB = NaiveBayes(alpha=0.04, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.04)__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHqRiSQYpfEB",
        "outputId": "cf46bb9a-6f2a-4e18-bbdd-844177b4197f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(NB.predict(words_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuz4R0PDoZsy"
      },
      "source": [
        "del df_train\n",
        "del df_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7ms6GZhm4CM"
      },
      "source": [
        "### Split test dataset 50 iterations per 10 of each topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCLTyO1nn34U"
      },
      "source": [
        "df_test.sort_values(by=['Label'], inplace=True)\n",
        "df_test.drop(labels='content', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08aKywOkullM",
        "outputId": "2fdc2938-9e09-4baf-db2d-37f7747ad9ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"KNN-Acc(%)\", \"NB-Acc(%)\"]\n",
        "\n",
        "df_results_test_set = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_results_test_set.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [KNN-Acc(%), NB-Acc(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNcU7hdTn3tc"
      },
      "source": [
        "%%time\n",
        "initial_offset = np.array([i*num_docs_in_each_topic for i in range(0, 11)])\n",
        "for counter_test in range(0, 50): ## run iterations 50 times\n",
        "    offsets = initial_offset + counter_test*10\n",
        "    start_indices = offsets\n",
        "    end_indices = offsets + 10\n",
        "    \n",
        "    # print(\"\\nCounter = \", counter_test)\n",
        "    # print(offsets)\n",
        "    # print(start_indices)\n",
        "    # print(end_indices)\n",
        "\n",
        "    ### Testing here ###\n",
        "    nb_correct = knn_correct = 0\n",
        "    for i in range(len(start_indices)): ## add all to list.\n",
        "        # print(np.unique(df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values, return_counts=True), end=' ')\n",
        "        for (x, y) in zip(df_test.iloc[start_indices[i]:end_indices[i]]['stemmed_content'].values, df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values):\n",
        "            \n",
        "            #### For KNN ####\n",
        "            x_tf_idf = form_TF_IDF_for_val_test(list_words=x, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "            y_pred = knn.predict(x_tf_idf)\n",
        "            if y_pred == y:\n",
        "                knn_correct += 1\n",
        "            \n",
        "            #### For Naive Bayes #####\n",
        "            y_pred = NB.predict(x)\n",
        "            if y_pred == y:\n",
        "                nb_correct += 1\n",
        "            \n",
        "    \n",
        "    print(f\"Done for counter_test = {counter_test}\")\n",
        "\n",
        "    NUM_DOCUMENTS = 10* len(np.unique(df_test['Label'].values))\n",
        "    knn_acc = knn_correct/( NUM_DOCUMENTS )*100  ## 10*11 total 110 test documents per iteration. [50 iterations]\n",
        "    nb_acc = nb_correct/( NUM_DOCUMENTS )*100\n",
        "    print(f\"KNN-Acc = {knn_acc}%, NB-Acc = {nb_acc}%\")\n",
        "\n",
        "    df_results_test_set = add_to_dataframe(df_old=df_results_test_set, to_add=[knn_acc, nb_acc])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkTag67arcpn",
        "outputId": "70e9bddf-699f-4acd-f64c-388d0ca1e824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f\"len df_results_test_set = {len(df_results_test_set)}\")\n",
        "display(df_results_test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_test_set = 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76.363636</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>70.909091</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>75.454545</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>74.545455</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>69.090909</td>\n",
              "      <td>85.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>89.090909</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>79.090909</td>\n",
              "      <td>93.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>93.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>75.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>79.090909</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>95.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>85.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>73.636364</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>75.454545</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>73.636364</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>70.000000</td>\n",
              "      <td>81.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>76.363636</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    KNN-Acc(%)  NB-Acc(%)\n",
              "0    81.818182  87.272727\n",
              "1    76.363636  84.545455\n",
              "2    78.181818  90.909091\n",
              "3    85.454545  90.000000\n",
              "4    82.727273  91.818182\n",
              "5    77.272727  86.363636\n",
              "6    70.909091  89.090909\n",
              "7    82.727273  91.818182\n",
              "8    75.454545  90.909091\n",
              "9    80.909091  89.090909\n",
              "10   86.363636  90.000000\n",
              "11   80.000000  92.727273\n",
              "12   80.909091  88.181818\n",
              "13   77.272727  90.909091\n",
              "14   80.909091  90.000000\n",
              "15   78.181818  88.181818\n",
              "16   77.272727  91.818182\n",
              "17   78.181818  86.363636\n",
              "18   74.545455  90.909091\n",
              "19   84.545455  90.909091\n",
              "20   69.090909  85.454545\n",
              "21   80.000000  89.090909\n",
              "22   89.090909  92.727273\n",
              "23   78.181818  94.545455\n",
              "24   77.272727  84.545455\n",
              "25   79.090909  93.636364\n",
              "26   77.272727  94.545455\n",
              "27   83.636364  89.090909\n",
              "28   82.727273  93.636364\n",
              "29   81.818182  90.909091\n",
              "30   75.454545  90.000000\n",
              "31   82.727273  92.727273\n",
              "32   79.090909  86.363636\n",
              "33   84.545455  86.363636\n",
              "34   82.727273  91.818182\n",
              "35   81.818182  95.454545\n",
              "36   80.909091  85.454545\n",
              "37   80.909091  90.909091\n",
              "38   73.636364  90.909091\n",
              "39   75.454545  89.090909\n",
              "40   73.636364  89.090909\n",
              "41   70.000000  81.818182\n",
              "42   83.636364  92.727273\n",
              "43   78.181818  88.181818\n",
              "44   76.363636  88.181818\n",
              "45   77.272727  89.090909\n",
              "46   86.363636  90.909091\n",
              "47   80.000000  88.181818\n",
              "48   83.636364  89.090909\n",
              "49   78.181818  87.272727"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrDQX97qrchw"
      },
      "source": [
        "df_results_test_set.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Test-Set-KNN-NB.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBHCUUtF2Kz8"
      },
      "source": [
        "## Load and analyze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dee3qAsxrcg6",
        "outputId": "9f926a87-3eff-4c07-c7d4-d96b69464562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_results_test_set = pd.read_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Test-Set-KNN-NB.csv')\n",
        "display(df_results_test_set.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76.363636</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   KNN-Acc(%)  NB-Acc(%)\n",
              "0   81.818182  87.272727\n",
              "1   76.363636  84.545455\n",
              "2   78.181818  90.909091\n",
              "3   85.454545  90.000000\n",
              "4   82.727273  91.818182"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4tqi3TS3LH0",
        "outputId": "d1b42722-3a1e-429a-b5c6-7ebb68089b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "knn_acc = df_results_test_set['KNN-Acc(%)'].values\n",
        "nb_acc = df_results_test_set['NB-Acc(%)'].values\n",
        "print(knn_acc.shape, nb_acc.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50,) (50,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDJKAptY5mEK",
        "outputId": "f3fc3d1d-ce83-4b56-a117-22154c40267b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"Method\", \"Mean-Acc(%)\", \"Minimum-Acc(%)\", \"Maximum-Acc(%)\", \"Std-dev\", \"Std-Error\"]\n",
        "\n",
        "df_stats = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_stats.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Mean-Acc(%)</th>\n",
              "      <th>Minimum-Acc(%)</th>\n",
              "      <th>Maximum-Acc(%)</th>\n",
              "      <th>Std-dev</th>\n",
              "      <th>Std-Error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Method, Mean-Acc(%), Minimum-Acc(%), Maximum-Acc(%), Std-dev, Std-Error]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MIAT9Dy3LF4"
      },
      "source": [
        "## Summarized results.\n",
        "df_stats = add_to_dataframe(df_old=df_stats, to_add=[\"KNN K=5, TF-IDF\", np.mean(knn_acc), min(knn_acc), max(knn_acc), np.std(knn_acc), stats.sem(knn_acc, axis=None, ddof=0)])\n",
        "df_stats = add_to_dataframe(df_old=df_stats, to_add=[\"NB alpha = 0.04\", np.mean(nb_acc), min(nb_acc), max(nb_acc), np.std(nb_acc), stats.sem(nb_acc, axis=None, ddof=0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tg1WtdcrcX9",
        "outputId": "cf2123ad-a44a-48f7-e75e-8b2116f99a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "display(df_stats)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Mean-Acc(%)</th>\n",
              "      <th>Minimum-Acc(%)</th>\n",
              "      <th>Maximum-Acc(%)</th>\n",
              "      <th>Std-dev</th>\n",
              "      <th>Std-Error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNN K=5, TF-IDF</td>\n",
              "      <td>79.454545</td>\n",
              "      <td>69.090909</td>\n",
              "      <td>89.090909</td>\n",
              "      <td>4.189716</td>\n",
              "      <td>0.592515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NB alpha = 0.04</td>\n",
              "      <td>89.672727</td>\n",
              "      <td>81.818182</td>\n",
              "      <td>95.454545</td>\n",
              "      <td>2.832865</td>\n",
              "      <td>0.400628</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Method  Mean-Acc(%)  ...   Std-dev  Std-Error\n",
              "0  KNN K=5, TF-IDF    79.454545  ...  4.189716   0.592515\n",
              "1  NB alpha = 0.04    89.672727  ...  2.832865   0.400628\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYKuEjwr7vdM"
      },
      "source": [
        "## Computing T-statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6pTUTIhRmUO",
        "outputId": "6e0d5477-f146-40b2-f7ad-6b9ad7961702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_results_test_set = pd.read_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Done-Using-Numpy-Single-Loop/Test-Set-KNN-NB.csv')\n",
        "df = df_results_test_set.head(5)\n",
        "display(df)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76.363636</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   KNN-Acc(%)  NB-Acc(%)\n",
              "0   81.818182  87.272727\n",
              "1   76.363636  84.545455\n",
              "2   78.181818  90.909091\n",
              "3   85.454545  90.000000\n",
              "4   82.727273  91.818182"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8nCDVZ3fdtf",
        "outputId": "e9483a9a-6dff-4e44-a405-5626910c7e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "string = df.to_latex(index=False, column_format='|c|c|')\n",
        "string"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\\\begin{tabular}{|c|c|}\\n\\\\toprule\\n KNN-Acc(\\\\%) &  NB-Acc(\\\\%) \\\\\\\\\\n\\\\midrule\\n  81.818182 &  87.272727 \\\\\\\\\\n  76.363636 &  84.545455 \\\\\\\\\\n  78.181818 &  90.909091 \\\\\\\\\\n  85.454545 &  90.000000 \\\\\\\\\\n  82.727273 &  91.818182 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVDiyV1lfdq_"
      },
      "source": [
        ""
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkcRv-Sefdoz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_KQzYnBRrA5",
        "outputId": "6205f56a-029c-4e05-ae27-e14234d8d3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df_results_test_set.columns.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['KNN-Acc(%)' 'NB-Acc(%)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyDAasE1RozN"
      },
      "source": [
        "knn_acc = df_results_test_set['KNN-Acc(%)']\n",
        "nb_acc = df_results_test_set['NB-Acc(%)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNcoJHAZ64EK",
        "outputId": "248db023-0fda-4fca-af5b-64df82840498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
        "\n",
        "# ans = stats.ttest_rel(knn_acc, nb_acc)\n",
        "# ans = stats.ttest_ind(knn_acc, nb_acc, equal_var=True)\n",
        "ans = stats.ttest_rel(knn_acc, nb_acc)\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ttest_relResult(statistic=-17.513701628384553, pvalue=9.835982137118567e-23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSZmV8YV64DO"
      },
      "source": [
        "# Ttest_indResult(statistic=-14.142663823802279, pvalue=3.641774754361954e-24)\n",
        "# Ttest_indResult(statistic=-14.142663823802279, pvalue=2.0995095838767057e-25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBFks-IP64CR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI990uMi634Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}