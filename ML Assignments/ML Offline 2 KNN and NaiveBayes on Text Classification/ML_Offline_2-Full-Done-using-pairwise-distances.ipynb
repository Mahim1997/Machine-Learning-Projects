{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Offline-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrnubMIEG0BZ"
      },
      "source": [
        "### MOUNT DRIVE ....."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1oJyY78cXnq"
      },
      "source": [
        "## First Import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import pickle\n",
        "\n",
        "from scipy import stats\n",
        "from xml.dom import minidom\n",
        "\n",
        "from sklearn.metrics import pairwise_distances, accuracy_score\n",
        "\n",
        "\n",
        "# !pip3 install tqdm\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "from functools import reduce"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFLKkx6Ybnlr"
      },
      "source": [
        "FOLDER_TRAIN = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/Training/'\n",
        "# FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics.txt'\n",
        "FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics-all.txt'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9hrTtEZp2A8"
      },
      "source": [
        "# !ls \"$FOLDER_TRAIN\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0bZ4kTHcXql"
      },
      "source": [
        "## Set random seed\n",
        "RANDOM_STATE = 22\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThQtktwcXtX",
        "outputId": "d240fe0f-6850-43a6-d807-50818374bae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Read data\n",
        "topic_remove = '3d_Printer' ## Remove 3D-printer\n",
        "list_doc_types = []\n",
        "with open(FILE_TOPICS, 'r') as fp:\n",
        "    topic = fp.readline()\n",
        "    while topic:\n",
        "        topic = topic.replace(\"\\n\", \"\")\n",
        "        list_doc_types.append(topic)\n",
        "        topic = fp.readline()\n",
        "\n",
        "list_doc_types.remove(topic_remove)\n",
        "print(list_doc_types)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Astronomy', 'Coffee', 'Space', 'Anime', 'Biology', 'Cooking', 'Windows_Phone', 'Arduino', 'Chess', 'Law', 'Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfC2PYXxg35A",
        "outputId": "6fe18384-1315-49ac-972e-2015bea19680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(list_doc_types)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Astronomy', 'Coffee', 'Space', 'Anime', 'Biology', 'Cooking', 'Windows_Phone', 'Arduino', 'Chess', 'Law', 'Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVPfrYjagVv4"
      },
      "source": [
        "def get_train_val_test_data(file_name):\n",
        "    xmldoc = minidom.parse(file_name)\n",
        "    xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "    \n",
        "    # print(f\"Inside get_train_val_test_data(), len(item_list) = {len(item_list)}\")\n",
        "\n",
        "    item_list = [x.attributes['Body'].value for x in xml_list]\n",
        "\n",
        "    train_list = item_list[0:500] ## first 500 train\n",
        "    val_list =  item_list[500:700] ## next 200 val\n",
        "    test_list = item_list[700:1200] ## next 500 test\n",
        "\n",
        "    ## delete original list.\n",
        "    del item_list\n",
        "    del xmldoc\n",
        "\n",
        "    ## return the new lists\n",
        "    return train_list, val_list, test_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvop_yNmUCMo"
      },
      "source": [
        "def add_to_dataframe(df_old, to_add):\n",
        "    df_old = df_old.append(pd.Series(to_add, index=df_old.columns), ignore_index=True)\n",
        "    return df_old"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MJXly5bt7aC"
      },
      "source": [
        "## Create three dataframes.\n",
        "\n",
        "## https://www.kite.com/python/answers/how-to-create-an-empty-dataframe-with-column-names-in-python\n",
        "column_names =[\"content\", \"Label\"]\n",
        "\n",
        "df_train = pd.DataFrame(columns = column_names)\n",
        "df_val = pd.DataFrame(columns = column_names)\n",
        "df_test = pd.DataFrame(columns = column_names)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxZHmfGt1g9e",
        "outputId": "d066d6c5-78a7-4007-a461-6c4bee91b76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "### Populate all topics\n",
        "def populate_data_frames(df_train, df_val, df_test, list_doc_types):\n",
        "    for topic in list_doc_types: ## iterating per topic/label\n",
        "        label = topic ## assign label\n",
        "        \n",
        "        ## read using xml package\n",
        "        file_name = FOLDER_TRAIN + topic + \".xml\"\n",
        "        xmldoc = minidom.parse(file_name)\n",
        "        xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "\n",
        "        ## get train, val, test lists\n",
        "        train_list, val_list, test_list = get_train_val_test_data(file_name=file_name)\n",
        "\n",
        "        print(len(train_list), len(val_list), len(test_list))\n",
        "\n",
        "        ## add to dataframe\n",
        "        for v1 in train_list:\n",
        "            df_train = add_to_dataframe(df_old=df_train, to_add=[v1, label])\n",
        "        for v2 in val_list:\n",
        "            df_val = add_to_dataframe(df_old=df_val, to_add=[v2, label])\n",
        "        for v3 in test_list:\n",
        "            df_test = add_to_dataframe(df_old=df_test, to_add=[v3, label])\n",
        "\n",
        "\n",
        "        ## delete original list\n",
        "        del train_list\n",
        "        del val_list\n",
        "        del test_list\n",
        "\n",
        "    ## return dataframes\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "### Call the function\n",
        "df_train, df_val, df_test = populate_data_frames(df_train=df_train, df_val=df_val, df_test=df_test, list_doc_types=list_doc_types)\n",
        "\n",
        "print(f\"len df_train = {len(df_train)}\")\n",
        "print(f\"len df_val = {len(df_val)}\")\n",
        "print(f\"len df_test = {len(df_test)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "len df_train = 5500\n",
            "len df_val = 2200\n",
            "len df_test = 5500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUI07UZKmPc"
      },
      "source": [
        "## Shuffle dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rcv33P3XXuN"
      },
      "source": [
        "df_train = df_train.sample(frac=1, random_state=RANDOM_STATE)\n",
        "df_val = df_val.sample(frac=1, random_state=RANDOM_STATE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1m0_Du8XYBK"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKSziS4PvdnZ"
      },
      "source": [
        "## https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
        "## https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "## from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIukiiJxDWdu",
        "outputId": "5bd34198-800f-4ab1-a55a-d21e6f2cdb28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "## https://stackoverflow.com/questions/26693736/nltk-and-stopwords-fail-lookuperror\n",
        "## https://stackoverflow.com/questions/26570944/resource-utokenizers-punkt-english-pickle-not-found\n",
        "\n",
        "# nltk.download()\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUsCIVJru269"
      },
      "source": [
        "## Preprocess each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-MlUQ4w1uM2",
        "cellView": "both",
        "outputId": "20fbac0b-f514-4560-91a4-ed2c994d42d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "## https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "## First lemmatize, and then stem.\n",
        "\n",
        "def pre_process_stem_sentence(sentence, stop_words, stemmer, lemmatizer):\n",
        "    ## save in another variable\n",
        "    text = sentence\n",
        "    ## remove HTML tags [using soup]\n",
        "    soup = BeautifulSoup(text)\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    ## remove <a href> type things\n",
        "    soup = BeautifulSoup(text) ## create soup again.\n",
        "    for a in soup.findAll('a'):\n",
        "        a.replaceWithChildren()\n",
        "    \n",
        "    text = str(soup) ## reform text\n",
        "\n",
        "\n",
        "    ## remove unicode.\n",
        "    text = re.sub(r\"&nbsp;\", \" \", text)\n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
        "\n",
        "    ## remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"https\\S+\", \"\", text)\n",
        "    ## text = re.sub(r\"www\\S+\", \"\", text)\n",
        "\n",
        "    ## to be safe, remove HTML tags again using regex\n",
        "    #### https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44  \n",
        "    clean = re.compile('<.*?>')\n",
        "    text = re.sub(clean, '', text)\n",
        "\n",
        "    ## convert to small letters.\n",
        "    text = text.lower()\n",
        "\n",
        "    ## replace newlines, tabs -> SPACE\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "\n",
        "    ## {COLON} make <a>:<b> become <a>[space]<b>\n",
        "    # text = text.replace(\": \", \":\") # :[space] -> : \n",
        "    text = text.replace(\":\", \" \")  # : -> [space]\n",
        "    \n",
        "    ## {HYPHEN}\n",
        "    # text = text.replace(\"- \", \"-\")\n",
        "    text = text.replace(\"-\", \" \")\n",
        "\n",
        "    ## numbers removal    \n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "\n",
        "    ## punctuations removal\n",
        "    text = text.translate((str.maketrans('','',string.punctuation)))   \n",
        "\n",
        "\n",
        "    ## [remove anything EXCEPT english letters]\n",
        "    ## https://stackoverflow.com/questions/6323296/python-remove-anything-that-is-not-a-letter-or-number\n",
        "    text = re.sub(  \"[^a-z ]\",              # Anything except 0..9, a..z and A..Z\n",
        "                    \"\",                     # replaced with nothing\n",
        "                    text)                   # in this string   \n",
        "\n",
        "    ## remove space initially and finally.\n",
        "    text = text.lstrip()\n",
        "\n",
        "    ## make double spaces become one space.\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    ## remove stop words (English).\n",
        "    # print(f\"Before stopwords removal, text.len = {len(text)}\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words_removed_words_list = [t for t in tokens if not t in stop_words]\n",
        "\n",
        "    # print(f\"After tokenize and stopwords, len stop_words_removed_words_list = {len(stop_words_removed_words_list)}\")\n",
        "\n",
        "    ## apply lemmatization.\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stop_words_removed_words_list]\n",
        "    \n",
        "    ## apply stemming.\n",
        "    stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
        "\n",
        "    ## return the final pre-processed list-of-words.\n",
        "    return stemmed_words\n",
        "\n",
        "################ Test on one sentence #######################\n",
        "\n",
        "## Obtain once.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "## Preprocess per element of dataframe to test.\n",
        "text_to_process = df_train['content'].iloc[499]\n",
        "print(text_to_process)\n",
        "\n",
        "print(\"--\"*90)\n",
        "\n",
        "preprocessed_text = pre_process_stem_sentence(sentence=text_to_process, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(preprocessed_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<h2>You never mentioned your tablet model... or Arduino model</h2>\n",
            "\n",
            "<p>There are some tablets designed to work with USB flash drives and printers. This is called OTG (on the go.</p>\n",
            "\n",
            "<p><strong>You need:</strong></p>\n",
            "\n",
            "<ul>\n",
            "<li>A tablet supporting OTG</li>\n",
            "<li>An OTG adapter that fits into your tablets USB slot and is compatible</li>\n",
            "<li><a href=\"https://play.google.com/store/apps/details?id=com.primavera.arduino.listener\" rel=\"nofollow\">The Arduino Uno Communicator App</a></li>\n",
            "<li>Arduino Uno (or clone) [Note: It says that it works with Atmega16U2 or Atmega8U2 programmed as a USB-to-serial converter so I would assume that that would cover a few boards more than the Uno.)</li>\n",
            "</ul>\n",
            "\n",
            "<p>Another alternitive is to look into <a href=\"https://www.google.com/search?q=arduino+bluetooth&amp;qscrl=1&amp;tbm=isch&amp;imgil=WyPC4O05Xn4rhM%253A%253Bhttps%253A%252F%252Fencrypted-tbn2.gstatic.com%252Fimages%253Fq%253Dtbn%253AANd9GcSfTtWqIuN9YYdd90OO_tlxsvLUYhV_pl7NfaMhHDzSRq9N6W9rNA%253B1200%253B1200%253B1soQc4CzMzh0MM%253Bhttp%25253A%25252F%25252Fm.dhgate.com%25252Fproduct%25252Fjy-mcu-arduino-bluetooth-wireless-serial%25252F151048482.html&amp;source=iu&amp;usg=__KyMLS4aeubxJnDaMvprgRgADxAs%3D&amp;sa=X&amp;ei=1BUCU635IYmEygGO1oHwCQ&amp;ved=0CGkQ9QEwBg&amp;biw=1280&amp;bih=899\" rel=\"nofollow\">Bluetooth</a> (Note: I just picked a random link but there are hundreds of similar BT adapters)</p>\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "['never', 'mention', 'tablet', 'model', 'arduino', 'model', 'tablet', 'design', 'work', 'usb', 'flash', 'drive', 'printer', 'call', 'otg', 'go', 'need', 'tablet', 'support', 'otg', 'otg', 'adapt', 'fit', 'tablet', 'usb', 'slot', 'compat', 'arduino', 'uno', 'commun', 'app', 'arduino', 'uno', 'clone', 'note', 'say', 'work', 'atmegau', 'atmegau', 'program', 'usb', 'serial', 'convert', 'would', 'assum', 'would', 'cover', 'board', 'uno', 'anoth', 'alternit', 'look', 'bluetooth', 'note', 'pick', 'random', 'link', 'hundr', 'similar', 'bt', 'adapt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4gGdZmwZDZ",
        "outputId": "cec51745-fc50-41ef-87d3-659d64a630b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "doc = df_val['content'].iloc[499]\n",
        "print(doc)\n",
        "\n",
        "print(\"--\"*85)\n",
        "\n",
        "preprocessed = pre_process_stem_sentence(sentence=doc, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(len(preprocessed))\n",
        "print(preprocessed)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>This sounds like it's more a matter of determining the original name / wording used to file the copyright. </p>\n",
            "\n",
            "<p>Without knowing more of that language, you're left to the typical sleuthing options:</p>\n",
            "\n",
            "<ul>\n",
            "<li>names of the company owners / major shareholders as of 15-18 years ago. </li>\n",
            "<li>other DBAs and holding companies of the company originally presumed to hold the copyright. </li>\n",
            "<li>brute force search of all categorically-related copyrights in the time range. </li>\n",
            "</ul>\n",
            "\n",
            "<p>The advice I would really like to give you is knowledge of how IP attorneys filed video game copyrights in the time span of 1985-2000.  This could reveal any unexpected filing categories that could have been used as part of niche or experimental copyright strategy (at that time) for this kind of IP. </p>\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "70\n",
            "['sound', 'like', 'matter', 'determin', 'origin', 'name', 'word', 'use', 'file', 'copyright', 'without', 'know', 'languag', 'your', 'left', 'typic', 'sleuth', 'option', 'name', 'compani', 'owner', 'major', 'sharehold', 'year', 'ago', 'dba', 'hold', 'compani', 'compani', 'origin', 'presum', 'hold', 'copyright', 'brute', 'forc', 'search', 'categor', 'relat', 'copyright', 'time', 'rang', 'advic', 'would', 'realli', 'like', 'give', 'knowledg', 'ip', 'attorney', 'file', 'video', 'game', 'copyright', 'time', 'span', 'could', 'reveal', 'unexpect', 'file', 'categori', 'could', 'use', 'part', 'nich', 'experiment', 'copyright', 'strategi', 'time', 'kind', 'ip']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbSKXXrRGGgo"
      },
      "source": [
        "## Preprocess for each sentence of train-dataframe.\n",
        "df_train[\"stemmed_content\"] = df_train.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_val[\"stemmed_content\"] = df_val.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_test[\"stemmed_content\"] = df_test.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puTtacUoHBZ9"
      },
      "source": [
        "# Form Vocabulary from dataframe_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwyJWOOIETs"
      },
      "source": [
        "def print_first_n_keys_and_vals_dict(dictionary, n=5):\n",
        "    print(f\"Len dictionary = {len(dictionary)}, printing first {n} keys, vals\")\n",
        "    itr = 0\n",
        "    for key in dictionary:\n",
        "        print(key, dictionary[key])\n",
        "        itr += 1\n",
        "        if itr == n:\n",
        "            break"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO4hhJYkXqRp",
        "outputId": "2861c25d-843b-41ac-bf8b-0ed1e1853b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Also remove one-len words\n",
        "dictionary_vocab = {} ## empty dict.\n",
        "for doc in df_train['stemmed_content']:\n",
        "    # print(f\"len doc = {len(doc)}\")\n",
        "    for word in doc:\n",
        "        if len(word) <= 1:\n",
        "            continue\n",
        "\n",
        "        len_currently = len(dictionary_vocab) ## add to len. [idx new]\n",
        "\n",
        "        if word not in dictionary_vocab:\n",
        "            dictionary_vocab[word] = (0, len_currently)\n",
        "        else:\n",
        "            (val, idx) = dictionary_vocab[word]\n",
        "            dictionary_vocab[word] = (val + 1, idx) ## Maintain the same index.\n",
        "\n",
        "print(f\"len dictionary_vocab = {len(dictionary_vocab)}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len dictionary_vocab = 17506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSQYB4eBf0sv"
      },
      "source": [
        "# file_name = \"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Vocab.txt\"\n",
        "# with open(file_name, 'w') as fw:\n",
        "#     for voc in dictionary_vocab:\n",
        "#         fw.write(voc)\n",
        "#         fw.write(\"\\n\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAgtrtyp13Tt"
      },
      "source": [
        "# ## https://www.kite.com/python/answers/how-to-save-a-dictionary-to-a-file-in-python\n",
        "# vocab_file = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/vocab_dict_df_train.pkl'\n",
        "\n",
        "# a_file = open(vocab_file, \"wb\")\n",
        "# pickle.dump(dictionary_vocab, a_file)\n",
        "# a_file.close()\n",
        "\n",
        "# # a_file = open(vocab_file, \"rb\")\n",
        "# # dictionary_vocab = pickle.load(a_file)\n",
        "# # print(dictionary_vocab)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJORdHUjHd7h"
      },
      "source": [
        "## Keep only those docs in train whose lengths are above 3 words i.e.\n",
        "### length of stemmed content > 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrZO1SycNmSX",
        "outputId": "ab4bb94d-8a37-434c-c9cd-42b06fb4b081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lens = [len(x) for x in df_train['stemmed_content']]\n",
        "print(max(lens))\n",
        "print(min(lens))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1381\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPJ5IMKKNmQV",
        "outputId": "b3879535-b5d1-40ff-a820-9d048ac5289d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(f\"Before removal, len df_train = {len(df_train)}\")\n",
        "\n",
        "MIN_WORD_COUNT_TO_REMOVE = 2\n",
        "# print(df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE][\"stemmed_content\"])\n",
        "idxToRemove = df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE].index\n",
        "print(f\"To remove num items = {len(idxToRemove)}\")\n",
        "\n",
        "labels_to_remove = df_train['Label'].iloc[idxToRemove].values\n",
        "print(np.unique(labels_to_remove, return_counts=True)) ## Almost fairly distributed.\n",
        "\n",
        "\n",
        "df_train.drop(idxToRemove , inplace=True) ### Removing\n",
        "\n",
        "print(f\"After removal, len df_train = {len(df_train)}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before removal, len df_train = 5500\n",
            "To remove num items = 224\n",
            "(array(['Anime', 'Arduino', 'Astronomy', 'Biology', 'Chess', 'Coffee',\n",
            "       'Cooking', 'Law', 'Space', 'Windows_Phone', 'Wood_Working'],\n",
            "      dtype=object), array([18, 15, 29, 19, 18, 27, 24, 21, 19, 14, 20]))\n",
            "After removal, len df_train = 5276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQCXuG-JhUU"
      },
      "source": [
        "## Create Hamming Distance Vectors by representing with 0/1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJfu0JkdKFRh"
      },
      "source": [
        "def form_hamming_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    for word in list_words:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index. [DO NOT]\n",
        "            # vec[UNKNOWN_WORD_INDEX] = 1\n",
        "            continue\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word]\n",
        "            vec[idx] = 1\n",
        "    return vec"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNrTRsZ2psv-"
      },
      "source": [
        "## Create hamming vectors for each col of dataframe\n",
        "df_train[\"ham_vector\"] = df_train.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"ham_vector\"] = df_val.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"ham_vector\"] = df_test.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNM1YUEWrfgk"
      },
      "source": [
        "## Create eucledian vectors by representing how many times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJXmvI8areMJ"
      },
      "source": [
        "def form_eucledian_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    \n",
        "    ## Form a small dictionary to store each word count\n",
        "    dict_local_vocab = {}\n",
        "    for word in list_words:\n",
        "        if word not in dict_local_vocab:\n",
        "            dict_local_vocab[word] = 1\n",
        "        else:\n",
        "            dict_local_vocab[word] = dict_local_vocab[word] + 1\n",
        "\n",
        "    # print(dict_local_vocab)\n",
        "\n",
        "    # unknown_word_count = 1\n",
        "    for word in dict_local_vocab:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index.\n",
        "            continue\n",
        "            # vec[UNKNOWN_WORD_INDEX] = unknown_word_count\n",
        "            # unknown_word_count += 1\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word] \n",
        "            vec[idx] = dict_local_vocab[word] ## replace with value of this word instead of 1.\n",
        "\n",
        "    del dict_local_vocab\n",
        "    return vec"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvxfO7dkpszM"
      },
      "source": [
        "df_train[\"euc_vector\"] = df_train.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"euc_vector\"] = df_val.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"euc_vector\"] = df_test.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ImEkZTTwY_"
      },
      "source": [
        "# FOLDER_DATASET = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset'\n",
        "# df_train.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/train.csv', index=False)\n",
        "# df_val.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/val.csv', index=False)\n",
        "# df_test.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Dataframes-Dataset/test.csv', index=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEuOjilfKauq"
      },
      "source": [
        "# Stack each of these vertically to form hamming and eucledian vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI3Cwy-2xb4s"
      },
      "source": [
        "def get_2D_vector(list_np_arr):\n",
        "    vec_2D = np.zeros((len(list_np_arr), len(list_np_arr[0])))\n",
        "    idx = 0\n",
        "    for vec in list_np_arr:\n",
        "        vec_2D[idx] = vec\n",
        "        idx += 1\n",
        "    return vec_2D"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB_4H48rL6kv",
        "outputId": "dc610678-20b6-491a-b2ab-e9005e879c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "vocab_size = len(dictionary_vocab)\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "\n",
        "num_documents = len(df_train)\n",
        "print(f\"num_documents = {num_documents}\")\n",
        "\n",
        "print(df_train.columns.values)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size = 17506\n",
            "num_documents = 5276\n",
            "['content' 'Label' 'stemmed_content' 'ham_vector' 'euc_vector']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oodrsKK6GlG",
        "outputId": "4dd12c70-c909-41cc-fb9e-703a4df4bd34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hamming_vectors_2D = get_2D_vector(list_np_arr=df_train['ham_vector'].values)\n",
        "eucledian_vectors_2D = get_2D_vector(list_np_arr=df_train['euc_vector'].values)\n",
        "print(hamming_vectors_2D.shape, eucledian_vectors_2D.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5276, 17506) (5276, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrEqp1_0LH5"
      },
      "source": [
        "## For now save these dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViUb7lUGzp9N"
      },
      "source": [
        "# df_train.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/train.csv\", index=False)\n",
        "# df_val.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/val.csv\", index=False)\n",
        "# df_test.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/test.csv\", index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmALIJNfzuMJ"
      },
      "source": [
        "## Now, finally create TF-IDF and pickel dump everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__-GfML7BDN",
        "outputId": "abb4bcba-fbe6-482f-dc62-59745dd7a52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "def compute_TF_IDF_all_train_set(hamming_vectors_2D, eucledian_vectors_2D, alpha=0.0001, beta=0.0001):\n",
        "    num_documents, num_words = eucledian_vectors_2D.shape\n",
        "    print(f\"num_documents, num_words = {num_documents, num_words}\")\n",
        "\n",
        "    TF = np.zeros((num_documents, num_words))\n",
        "\n",
        "    IDF = np.zeros((1, num_words))\n",
        "\n",
        "    ## Calculate TF(d, w) = N(d, w)/W(d) ; where N(d, w): count(w) in document d , W(d): Total #words in document d\n",
        "    for itr in range(num_documents): ## iterate row-wise\n",
        "        doc_eucledian = eucledian_vectors_2D[itr]\n",
        "        total_num_words_doc_eucledian = num_words - np.sum(doc_eucledian == 0)        \n",
        "        TF[itr] = doc_eucledian/total_num_words_doc_eucledian\n",
        "\n",
        "        # if itr == 1:\n",
        "            # print(f\"itr = {itr}, doc_euc[itr] = {np.unique(doc_eucledian[itr], return_counts=True)}\")\n",
        "            # print(f\"itr = {itr}, TF[itr] = {np.unique(TF[itr], return_counts=True)}\")\n",
        "            # break\n",
        "\n",
        "    ## Calculate IDF(d, w) = log( (D + alpha)/(C(w) + beta) ) ; C(w) -> Total # docs with word 'w' ; D -> Total # documents\n",
        "    D = num_documents\n",
        "    for itr in range(num_words): ## itereate col-wise\n",
        "        C_w = np.sum(hamming_vectors_2D[:, itr] == 1) ## first calculate C(w)\n",
        "        \n",
        "        IDF[:, itr] = np.log( (D + alpha) / (C_w + beta) )\n",
        "\n",
        "        # break\n",
        "        \n",
        "    TF_IDF = TF*IDF\n",
        "    del TF\n",
        "    # del IDF\n",
        "    return TF_IDF, IDF\n",
        "\n",
        "TF_IDF_whole_corpus, IDF_whole_corpus = compute_TF_IDF_all_train_set(hamming_vectors_2D=hamming_vectors_2D, eucledian_vectors_2D=eucledian_vectors_2D)\n",
        "print(f\"TF_IDF_whole_corpus.shape = {TF_IDF_whole_corpus.shape}\")\n",
        "print(f\"IDF_whole_corpus.shape = {IDF_whole_corpus.shape}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_documents, num_words = (5276, 17506)\n",
            "TF_IDF_whole_corpus.shape = (5276, 17506)\n",
            "IDF_whole_corpus.shape = (1, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWeLmV_094XH",
        "outputId": "1ab24a16-cfd9-4e02-99c8-06e7cf3f0a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def form_TF_IDF_for_val_test(list_words, IDF, hamming_vectors_2D, eucledian_vectors_2D, vocab_dict, alpha=0.0001, beta=0.0001, epslion=0.000001):\n",
        "    num_docs_train, num_words = eucledian_vectors_2D.shape\n",
        "\n",
        "    TF = np.zeros((1, num_words))\n",
        "    \n",
        "    euc_vec = form_eucledian_vector(list_words=list_words, vocab_dict=vocab_dict)\n",
        "    \n",
        "    W_d_num_words_in_document = 0\n",
        "    for word in list_words:\n",
        "        if word in vocab_dict:\n",
        "            W_d_num_words_in_document += 1\n",
        "\n",
        "    TF = euc_vec/(W_d_num_words_in_document + epslion)\n",
        "\n",
        "    TF_IDF = TF*IDF\n",
        "\n",
        "    return TF_IDF\n",
        "\n",
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "\n",
        "TF_IDF = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                    eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "print(f\"TF_IDF.shape = {TF_IDF.shape}\")\n",
        "# print(np.unique(TF_IDF, return_counts=True))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF_IDF.shape = (1, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us7RvCom6UuF",
        "outputId": "0197b5ff-d023-43bc-c8b5-e215dbf58fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df_val.columns.values)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['content' 'Label' 'stemmed_content' 'ham_vector' 'euc_vector']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohbmnfhCXJfD"
      },
      "source": [
        "# Now we start with K-Nearest Neighbor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBzTc76r7VNj"
      },
      "source": [
        "#### Form validation 2D set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFmtTTRF7Cmw",
        "outputId": "603fa9ee-770e-440e-ca29-14e041505c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hamming_vectors_2D_validation = get_2D_vector(list_np_arr=df_val['ham_vector'].values)\n",
        "euclidean_vectors_2D_validation = get_2D_vector(list_np_arr=df_val['euc_vector'].values)\n",
        "print(hamming_vectors_2D_validation.shape, euclidean_vectors_2D_validation.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2200, 17506) (2200, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOdAYwV-5iVa",
        "outputId": "bcf8a0db-77bc-4f67-d2ac-1422fae71109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf_idf_validation_set = np.asarray([\n",
        "    form_TF_IDF_for_val_test(list_words=x, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "        eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001) for x in df_val['stemmed_content'].values\n",
        "])\n",
        "tf_idf_validation_set = tf_idf_validation_set.reshape(tf_idf_validation_set.shape[0], -1)\n",
        "print(f\"tf_idf_validation_set.shape = {tf_idf_validation_set.shape}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf_idf_validation_set.shape = (2200, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RN0SomM8JCp"
      },
      "source": [
        "## Similarity functions for 2D vectorized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDbTTsgSDXUT"
      },
      "source": [
        "###### For vectorized ############\n",
        "def ham(a, b):\n",
        "    # return np.count_nonzero((a!=b[:, None]), axis=-1) ## returns 3-D matrix. Don't use.\n",
        "    return pairwise_distances(a, b, metric='euclidean') ## since binary vectors will return hamming-distance\n",
        "\n",
        "def euclidean(a, b):\n",
        "    # return np.linalg.norm((a-b[:, None]), axis=-1)\n",
        "    return pairwise_distances(a, b, metric='euclidean')\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    # return (b.dot(a.T))/(np.linalg.norm(a, axis=1) * np.linalg.norm(b[:, None], axis=-1)) ## (a@b.T).T  ## to get r2*r1\n",
        "    return pairwise_distances(a, b, metric='cosine')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu-Y52OvggBm",
        "cellView": "form"
      },
      "source": [
        "#@title Similarity functions without pairwise-distances\n",
        "####### For single loop ############\n",
        "# ## Similarity functions.\n",
        "# def ham(a, b):\n",
        "#     return np.count_nonzero((a!=b), axis=1)\n",
        "\n",
        "# def euclidean(a, b):\n",
        "#     return np.linalg.norm((a-b), axis=1)\n",
        "\n",
        "# ### https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
        "# def cosine_similarity(a, b):\n",
        "#     cos_sim = np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "#     return cos_sim\n",
        "\n",
        "\n",
        "# a = np.array([x for x in range(40)])\n",
        "# a = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1])\n",
        "# a = a.reshape(5, -1)\n",
        "# print(\"a: \\n\", a)\n",
        "\n",
        "# b = np.array([1, 0, 1])\n",
        "# b = b.reshape(1, -1)\n",
        "# print(\"\\nb: \", b)\n",
        "\n",
        "\n",
        "# # print(ham(a, b))\n",
        "# print(f\"\\nEuclidean(a, b) = {euclidean(a, b)}\")\n",
        "\n",
        "# a = np.array([2, 1, 3, 4, 5, 1, 10, 3, 22])\n",
        "# n = 4\n",
        "# indices_top = (-a).argsort()[:n]\n",
        "# print(f\"\\n n = {n}, indices_top = {indices_top}\")\n",
        "# print(a[indices_top])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnAxbCgrtNyq",
        "outputId": "5d94eba6-8d4b-4f6f-b3d6-ed5bb2fdea10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "labels_train = df_train['Label'].values\n",
        "print(labels_train.shape)\n",
        "\n",
        "labels_val = df_val['Label'].values\n",
        "print(labels_val.shape)\n",
        "\n",
        "val_euc_0 = euclidean_vectors_2D_validation[0:2]\n",
        "print(val_euc_0.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5276,)\n",
            "(2200,)\n",
            "(2, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GQLpzcgYY_L",
        "outputId": "791d6c53-7df3-4056-8aca-b20fc47c6d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "### KNN Vectorized.\n",
        "class KNN:\n",
        "    def __init__(self, mode=\"hamming\", to_print=False):\n",
        "        self.mode = mode\n",
        "        if to_print == True:\n",
        "            print(f\"KNN __init__(mode={mode})\")\n",
        "        \n",
        "\n",
        "    def compute_distances(self, v):\n",
        "        # print(f\"v.shape = {v.shape}\")\n",
        "        if self.mode == \"hamming\":\n",
        "            self.distances = ham(v, self.vectors_corpus)\n",
        "        elif self.mode == \"euclidean\":\n",
        "            self.distances = euclidean(v, self.vectors_corpus)\n",
        "        elif self.mode == 'cosine_similarity':\n",
        "            self.distances = cosine_similarity(v, self.vectors_corpus)\n",
        "\n",
        "        # print(f\"After compute_distances mode={self.mode}, distances.shape = {self.distances.shape}\")\n",
        "        # print(f\"{self.distances}\")\n",
        "\n",
        "    def populate_vectors(self, vectors_corpus, labels):\n",
        "        self.vectors_corpus = vectors_corpus\n",
        "        self.labels = labels\n",
        "\n",
        "    def predict(self, v, num_neighbors, compute_distance_flag=True):\n",
        "        K = num_neighbors\n",
        "        ## compute distances by using suitable similarity function.\n",
        "        if compute_distance_flag == True:\n",
        "            self.compute_distances(v)\n",
        "\n",
        "        ## take argmax top results indices\n",
        "        ## (-v.T).argsort(axis=0)[:K].reshape(-1, )\n",
        "        indices_top = (self.distances.T).argsort(axis=0)[:K] ## highest value is negative distance ? Don't know why, +ve should be taken.\n",
        "\n",
        "        ## apply indices to labels\n",
        "        top_labels = self.labels[indices_top]\n",
        "\n",
        "        ## take majority vote and return the label\n",
        "        top_most_label = stats.mode(top_labels)[0][0]\n",
        "\n",
        "        ## return the max label\n",
        "        return top_most_label\n",
        "\n",
        "################################################# Test #################################################\n",
        "\n",
        "# knn = KNN(mode='hamming')\n",
        "# knn.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "# knn.predict(val_ham_0, num_neighbors=1)\n",
        "\n",
        "knn = KNN(mode='euclidean')\n",
        "knn.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "ans = knn.predict(val_euc_0, num_neighbors=1)\n",
        "print(ans.shape)\n",
        "print(ans)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2,)\n",
            "['Wood_Working' 'Space']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RQnLJLEXI2X",
        "cellView": "form"
      },
      "source": [
        "#@title KNN Non-Vectorized\n",
        "# class KNN:\n",
        "#     def __init__(self, K = 1, mode=\"hamming\", to_print=False):\n",
        "#         self.mode = mode\n",
        "#         self.K = K\n",
        "#         if to_print == True:\n",
        "#             print(f\"KNN __init__(K={K}, mode={mode})\")\n",
        "        \n",
        "\n",
        "#     def populate_vectors(self, vectors_corpus, labels):\n",
        "#         self.vectors_corpus = vectors_corpus\n",
        "#         self.labels = labels\n",
        "\n",
        "#     def compute_distances(self, v):\n",
        "#         if self.mode == \"hamming\":\n",
        "#             v = v.reshape(1, -1) ## Reshape vector.\n",
        "#             self.distances = ham(v, self.vectors_corpus)\n",
        "#         elif self.mode == \"euclidean\":\n",
        "#             v = v.reshape(1, -1) ## Reshape vector.\n",
        "#             self.distances = euclidean(v, self.vectors_corpus)\n",
        "#         elif self.mode == 'cosine_similarity':\n",
        "#             v = v.reshape(-1)\n",
        "#             self.distances = cosine_similarity(v, self.vectors_corpus)\n",
        "\n",
        "#         # print(f\"After compute_distances mode={self.mode}, distances = {self.distances}\")\n",
        "\n",
        "#     def predict(self, v):\n",
        "#         ## compute distances by using suitable similarity function.\n",
        "#         self.compute_distances(v)\n",
        "\n",
        "#         ## take argmax top results indices\n",
        "#         if self.mode == 'cosine_similarity':\n",
        "#             indices_top = (-self.distances).argsort()[:self.K] ## for some reason, this works for cosine-similarity\n",
        "#         else:\n",
        "#             indices_top = (self.distances).argsort()[:self.K] ## THIS works for hamming and euclidean\n",
        "\n",
        "#         ## apply indices to labels\n",
        "#         top_labels = self.labels[indices_top]\n",
        "\n",
        "#         ## take majority vote and return the label\n",
        "#         top_most_label = stats.mode(top_labels)[0][0]\n",
        "\n",
        "#         ## return the max label\n",
        "#         return top_most_label\n",
        "\n",
        "# ################################################# Test #################################################\n",
        "\n",
        "# knn = KNN(K=1, mode='hamming')\n",
        "# knn.populate_vectors(hamming_vectors_2D, labels)\n",
        "# knn.predict(val_ham_0)\n",
        "\n",
        "# # knn = KNN(K=7, mode='euclidean')\n",
        "# # knn.populate_vectors(eucledian_vectors_2D, labels)\n",
        "# # knn.predict(val_euc_0)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9LVZRhU8sle"
      },
      "source": [
        "## Test for one value of K"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-qoC6HLuIdN",
        "outputId": "a55f048b-5a39-472f-fa5d-f26921859d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "###### Test for one K #######\n",
        "K = 1\n",
        "knn = KNN(mode='hamming')\n",
        "knn.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "y_preds = knn.predict(hamming_vectors_2D_validation, num_neighbors=K)\n",
        "acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "print(f\"Acc Hamming K = {K} = {acc}%\")\n",
        "\n",
        "# print(f\"Acc Hamming K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")\n",
        "del knn\n",
        "\n",
        "\n",
        "# knn = KNN(mode='euclidean')\n",
        "# knn.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "# y_preds = knn.predict(euclidean_vectors_2D_validation, num_neighbors=K)\n",
        "\n",
        "# print(f\"Acc Euclidean K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")\n",
        "\n",
        "\n",
        "# knn = KNN(mode='cosine_similarity')\n",
        "# knn.populate_vectors(TF_IDF_whole_corpus, labels_train)\n",
        "# y_preds = knn.predict(tf_idf_validation_set, num_neighbors=K)\n",
        "\n",
        "# print(f\"Acc TF-IDF-Cosine K = {K} = {np.sum(labels_val==y_preds)/len(labels_val)*100}%\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc Hamming K = 1 = 39.68181818181818%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wzDleMoXe-b"
      },
      "source": [
        "## Applying tests on validation set for KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKPaf3QVXens",
        "outputId": "d184df22-400c-4c0b-8340-49002849d6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "%%time\n",
        "## Initialize results dataframe\n",
        "column_names = [\"Similarity-Measure\", \"K\", \"Accuracy(%)\"]\n",
        "df_results_knn = pd.DataFrame(columns=column_names)\n",
        "\n",
        "### Initialize objects\n",
        "knn_ham = KNN(mode='hamming')\n",
        "knn_euc = KNN(mode='euclidean')\n",
        "knn_cosine = KNN(mode='cosine_similarity')\n",
        "\n",
        "### Populate initial vectors\n",
        "knn_ham.populate_vectors(hamming_vectors_2D, labels_train)\n",
        "knn_euc.populate_vectors(eucledian_vectors_2D, labels_train)\n",
        "knn_cosine.populate_vectors(TF_IDF_whole_corpus, labels_train)\n",
        "\n",
        "### Compute distances for each val-set\n",
        "knn_ham.compute_distances(hamming_vectors_2D_validation)\n",
        "knn_euc.compute_distances(euclidean_vectors_2D_validation)\n",
        "knn_cosine.compute_distances(tf_idf_validation_set)\n",
        "\n",
        "### Predict for each value of K\n",
        "for k in [1, 3, 5, 7, 9, 11, 13, 15]:\n",
        "    print(f\"Predicting for k = {k}\")\n",
        "    ## Predict and Append to dataframe.  \n",
        "    ## Signature: def predict(self, v, num_neighbors, compute_distance_flag=True)\n",
        "    y_preds = knn_ham.predict(hamming_vectors_2D_validation, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = np.sum(labels_val==y_preds)/len(labels_val)*100 ## accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Hamming\", k, acc])\n",
        "    \n",
        "    y_preds = knn_euc.predict(hamming_vectors_2D_validation, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Euclidean\", k, acc])\n",
        "\n",
        "    y_preds = knn_cosine.predict(hamming_vectors_2D_validation, num_neighbors=k, compute_distance_flag=False)\n",
        "    acc = accuracy_score(y_true=labels_val, y_pred=y_preds, normalize=True)*100\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"TF-IDF-cosine-sim\", k, acc])\n",
        "    \n",
        "\n",
        "### Delete each objects\n",
        "del knn_ham\n",
        "del knn_euc\n",
        "del knn_cosine\n",
        "\n",
        "### Print\n",
        "print(f\"len df_results_knn = {len(df_results_knn)}\")\n",
        "print(df_results_knn.head(3))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting for k = 1\n",
            "Predicting for k = 3\n",
            "Predicting for k = 5\n",
            "Predicting for k = 7\n",
            "Predicting for k = 9\n",
            "Predicting for k = 11\n",
            "Predicting for k = 13\n",
            "Predicting for k = 15\n",
            "len df_results_knn = 24\n",
            "  Similarity-Measure  K  Accuracy(%)\n",
            "0            Hamming  1    39.681818\n",
            "1          Euclidean  1    55.954545\n",
            "2  TF-IDF-cosine-sim  1    78.863636\n",
            "CPU times: user 2min 7s, sys: 847 ms, total: 2min 8s\n",
            "Wall time: 1min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvq54JeJXelm",
        "outputId": "a0ed7eec-2d7a-4e70-879e-4f0b2779a988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "source": [
        "print(f\"len df_results_knn = {len(df_results_knn)}\")\n",
        "display(df_results_knn)\n",
        "df_results_knn.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Using-2D-Pairwise/KNN-val-topics.csv', index=False)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_knn = 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Similarity-Measure</th>\n",
              "      <th>K</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>1</td>\n",
              "      <td>39.681818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>1</td>\n",
              "      <td>55.954545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>1</td>\n",
              "      <td>78.863636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>3</td>\n",
              "      <td>40.045455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>3</td>\n",
              "      <td>54.590909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>3</td>\n",
              "      <td>79.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>5</td>\n",
              "      <td>42.681818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>5</td>\n",
              "      <td>53.590909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>5</td>\n",
              "      <td>80.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>7</td>\n",
              "      <td>39.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>7</td>\n",
              "      <td>51.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>7</td>\n",
              "      <td>81.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>9</td>\n",
              "      <td>37.590909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>9</td>\n",
              "      <td>50.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>9</td>\n",
              "      <td>82.863636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>11</td>\n",
              "      <td>35.045455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>11</td>\n",
              "      <td>49.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>11</td>\n",
              "      <td>82.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>13</td>\n",
              "      <td>31.772727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>13</td>\n",
              "      <td>47.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>13</td>\n",
              "      <td>83.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>15</td>\n",
              "      <td>30.045455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>15</td>\n",
              "      <td>45.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>15</td>\n",
              "      <td>83.318182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Similarity-Measure   K  Accuracy(%)\n",
              "0             Hamming   1    39.681818\n",
              "1           Euclidean   1    55.954545\n",
              "2   TF-IDF-cosine-sim   1    78.863636\n",
              "3             Hamming   3    40.045455\n",
              "4           Euclidean   3    54.590909\n",
              "5   TF-IDF-cosine-sim   3    79.181818\n",
              "6             Hamming   5    42.681818\n",
              "7           Euclidean   5    53.590909\n",
              "8   TF-IDF-cosine-sim   5    80.500000\n",
              "9             Hamming   7    39.636364\n",
              "10          Euclidean   7    51.636364\n",
              "11  TF-IDF-cosine-sim   7    81.545455\n",
              "12            Hamming   9    37.590909\n",
              "13          Euclidean   9    50.136364\n",
              "14  TF-IDF-cosine-sim   9    82.863636\n",
              "15            Hamming  11    35.045455\n",
              "16          Euclidean  11    49.727273\n",
              "17  TF-IDF-cosine-sim  11    82.500000\n",
              "18            Hamming  13    31.772727\n",
              "19          Euclidean  13    47.545455\n",
              "20  TF-IDF-cosine-sim  13    83.136364\n",
              "21            Hamming  15    30.045455\n",
              "22          Euclidean  15    45.181818\n",
              "23  TF-IDF-cosine-sim  15    83.318182"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K-q5b6ZzqVP",
        "outputId": "390fcd28-8c61-40b8-cf20-fed8df00c7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "mat = df_results_knn.values\n",
        "print(mat.shape)\n",
        "print(mat)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24, 3)\n",
            "[['Hamming' 1 39.68181818181818]\n",
            " ['Euclidean' 1 55.95454545454545]\n",
            " ['TF-IDF-cosine-sim' 1 78.86363636363637]\n",
            " ['Hamming' 3 40.04545454545455]\n",
            " ['Euclidean' 3 54.59090909090909]\n",
            " ['TF-IDF-cosine-sim' 3 79.18181818181819]\n",
            " ['Hamming' 5 42.68181818181818]\n",
            " ['Euclidean' 5 53.590909090909086]\n",
            " ['TF-IDF-cosine-sim' 5 80.5]\n",
            " ['Hamming' 7 39.63636363636363]\n",
            " ['Euclidean' 7 51.63636363636363]\n",
            " ['TF-IDF-cosine-sim' 7 81.54545454545455]\n",
            " ['Hamming' 9 37.59090909090909]\n",
            " ['Euclidean' 9 50.13636363636363]\n",
            " ['TF-IDF-cosine-sim' 9 82.86363636363636]\n",
            " ['Hamming' 11 35.04545454545455]\n",
            " ['Euclidean' 11 49.72727272727273]\n",
            " ['TF-IDF-cosine-sim' 11 82.5]\n",
            " ['Hamming' 13 31.772727272727273]\n",
            " ['Euclidean' 13 47.54545454545455]\n",
            " ['TF-IDF-cosine-sim' 13 83.13636363636364]\n",
            " ['Hamming' 15 30.045454545454547]\n",
            " ['Euclidean' 15 45.18181818181819]\n",
            " ['TF-IDF-cosine-sim' 15 83.31818181818181]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0LcdCK_RW8E"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBwmpIgW4XOD"
      },
      "source": [
        "### Combine all documents of each class i into one document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EagCOsAhzqiu"
      },
      "source": [
        "# unique_labels = np.unique(df_train['Label'].values)\n",
        "# dictionary_list_words_for_NB = {}\n",
        "# # for label in unique_labels:\n",
        "# #     if label not in dictionary_list_words:\n",
        "# #         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "    \n",
        "# for (list_words, label) in zip(df_train['stemmed_content'].values, df_train['Label'].values):\n",
        "#     if label not in dictionary_list_words_for_NB:\n",
        "#         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "#     dictionary_list_words_for_NB[label].append(list_words)\n",
        "\n",
        "# ## reduce/flat-out to make 1D list.\n",
        "# ## https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
        "# for label in dictionary_list_words_for_NB:\n",
        "#     dictionary_list_words_for_NB[label] = reduce(operator.concat, dictionary_list_words_for_NB[label])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkQbU3mCONb"
      },
      "source": [
        "## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "def keywithmaxval(d):\n",
        "     \"\"\" a) create a list of the dict's keys and values; \n",
        "         b) return the key with the max value\"\"\"  \n",
        "     v=list(d.values())\n",
        "     k=list(d.keys())\n",
        "     return k[v.index(max(v))]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAcwIAKOFMJF",
        "outputId": "2375ae9f-a608-4f5d-f9e6-42c9395dd609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self, vocab_size, alpha=0.01, to_print=False):\n",
        "        if to_print == True:\n",
        "            print(f\"NaiveBayes __init(alpha={alpha})__\")\n",
        "        self.alpha = alpha\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dictionary_list_words_for_NB = {}\n",
        "        self.dictionary_prior_probabilities = {}\n",
        "        self.dictionary_count_words_per_class = {}\n",
        "        self.dictionary_total_words_per_class = {}\n",
        "    \n",
        "\n",
        "    def fit(self, list_list_words, labels):\n",
        "        self.list_list_words = list_list_words\n",
        "        self.labels = labels\n",
        "        self.form_dictionary_list_words()\n",
        "        self.compute_probabilities()\n",
        "\n",
        "\n",
        "    def form_dictionary_list_words(self):\n",
        "        for (list_words, label) in zip(self.list_list_words, self.labels):\n",
        "            if label not in self.dictionary_list_words_for_NB:\n",
        "                self.dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "            self.dictionary_list_words_for_NB[label].append(list_words)\n",
        "        \n",
        "        for label in self.dictionary_list_words_for_NB: ## reduce/flat-out to make 1D list.\n",
        "            self.dictionary_list_words_for_NB[label] = reduce(operator.concat, self.dictionary_list_words_for_NB[label])\n",
        "\n",
        "\n",
        "    def compute_probabilities(self):\n",
        "        ## Compute prior probabilities/initial guesses\n",
        "        (classes, cnts) = np.unique(self.labels, return_counts=True)\n",
        "        cnts = cnts/np.sum(cnts) ## C_i / (C_1 + C_2 + ... + C_n)\n",
        "        for (lab, itr) in zip(classes, range(len(classes))):\n",
        "            self.dictionary_prior_probabilities[lab] = cnts[itr]\n",
        "\n",
        "        ## Compute per-word probabilities\n",
        "\n",
        "        ## Counter increment\n",
        "        for lab in classes:\n",
        "            num_words_this_class = 0\n",
        "            self.dictionary_count_words_per_class[lab] = {}\n",
        "            for word in self.dictionary_list_words_for_NB[lab]:\n",
        "                if word not in self.dictionary_count_words_per_class[lab]:\n",
        "                    self.dictionary_count_words_per_class[lab][word] = 0 ## initialize counter to 0.\n",
        "                self.dictionary_count_words_per_class[lab][word] = self.dictionary_count_words_per_class[lab][word] + 1 ## increment counter\n",
        "                num_words_this_class += 1\n",
        "            self.dictionary_total_words_per_class[lab] = num_words_this_class\n",
        "\n",
        "    def predict(self, list_words):\n",
        "        ## Compute probabilities for each label.\n",
        "        dict_probabilities_per_class = {}\n",
        "\n",
        "        for lab in np.unique(self.labels):\n",
        "            prob_log_curr_class = np.log(self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # prob_log_curr_class = (self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # print(f\"Before, prob_log_curr_class = {prob_log_curr_class}\")\n",
        "            for word in list_words: ## iterate per word\n",
        "                if word in self.dictionary_count_words_per_class[lab]: ## if word exists in THIS document.\n",
        "                    ## use smoothing factor alpha\n",
        "                    # prob_log_curr_class += np.log((self.dictionary_count_words_per_class[lab][word] + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size)) \n",
        "                    word_prob = self.dictionary_count_words_per_class[lab][word]\n",
        "                    # prob_log_curr_class = prob_log_curr_class*word_prob\n",
        "                else:\n",
        "                    word_prob = 0\n",
        "                prob_log_curr_class += np.log( (word_prob + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size) )\n",
        "\n",
        "\n",
        "            dict_probabilities_per_class[lab] = prob_log_curr_class\n",
        "            # print(dict_probabilities_per_class)\n",
        "\n",
        "        ## Get max probability class.\n",
        "        ## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "        return keywithmaxval(d=dict_probabilities_per_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################### Checking ############################################################\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=len(dictionary_vocab))\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "check = df_val['stemmed_content'].iloc[0]\n",
        "p = NB.predict(check)\n",
        "print(p)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uILDNAL3FddD",
        "outputId": "924b98e7-b96d-42d6-b62a-39fce47013bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list_new = [\n",
        "['coffee', 'tea', 'dew', 'dew', 'dew', 'dew'],\n",
        "['coffee', 'noir', 'homelander', 'dew'],\n",
        "['noir', 'noir', 'fool', 'fool', 'noir']\n",
        "]\n",
        "\n",
        "labs_new = [\n",
        "    'bev',\n",
        "    'supe',\n",
        "    'misc'\n",
        "]\n",
        "\n",
        "vocab_size = 6\n",
        "\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=6)\n",
        "NB.fit(list_new, labs_new)\n",
        "print(NB.predict(['coffee', 'tea', 'dew', 'dew', 'dew', 'dew']))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f13yqf6EKAaJ"
      },
      "source": [
        "## Validation on NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgWpyxjCFMVM",
        "outputId": "8f2d9556-dc84-4210-9313-084f099cadd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"ALPHA\", \"Accuracy(%)\"]\n",
        "\n",
        "df_results_NB = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_results_NB.head())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ALPHA, Accuracy(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5FaA8KG8zu",
        "outputId": "6211c81b-29ad-4f00-9069-5b8cf8927e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "alpha_values = np.linspace(start=0.01, stop=1.0, num=100)\n",
        "print(alpha_values)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14\n",
            " 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28\n",
            " 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42\n",
            " 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56\n",
            " 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7\n",
            " 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84\n",
            " 0.85 0.86 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98\n",
            " 0.99 1.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rC01YFFMRn",
        "outputId": "eed4b2fa-9427-4bdc-c634-0cc8545b1f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "### Validation NaiveBayes ###\n",
        "for alpha in (alpha_values):\n",
        "    NB = NaiveBayes(alpha=alpha, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "    NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "    nb_acc = 0\n",
        "    ## Predict each val set ##\n",
        "    for (x, y) in (zip(df_val['stemmed_content'].values, df_val['Label'].values)):\n",
        "        y_pred = NB.predict(x)\n",
        "        if y_pred == y:\n",
        "            nb_acc += 1\n",
        "\n",
        "\n",
        "    ## Append to dataframe and print.\n",
        "    # print(f\"NB alpha = {alpha}, accuracy = {nb_acc/len(df_val)*100} %\")\n",
        "    df_results_NB = add_to_dataframe(df_old=df_results_NB, to_add=[alpha, nb_acc/len(df_val)*100])\n",
        "    del NB"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.01)__\n",
            "NaiveBayes __init(alpha=0.02)__\n",
            "NaiveBayes __init(alpha=0.03)__\n",
            "NaiveBayes __init(alpha=0.05)__\n",
            "NaiveBayes __init(alpha=0.060000000000000005)__\n",
            "NaiveBayes __init(alpha=0.06999999999999999)__\n",
            "NaiveBayes __init(alpha=0.09)__\n",
            "NaiveBayes __init(alpha=0.09999999999999999)__\n",
            "NaiveBayes __init(alpha=0.11)__\n",
            "NaiveBayes __init(alpha=0.12)__\n",
            "NaiveBayes __init(alpha=0.13)__\n",
            "NaiveBayes __init(alpha=0.14)__\n",
            "NaiveBayes __init(alpha=0.15000000000000002)__\n",
            "NaiveBayes __init(alpha=0.16)__\n",
            "NaiveBayes __init(alpha=0.17)__\n",
            "NaiveBayes __init(alpha=0.18000000000000002)__\n",
            "NaiveBayes __init(alpha=0.19)__\n",
            "NaiveBayes __init(alpha=0.2)__\n",
            "NaiveBayes __init(alpha=0.21000000000000002)__\n",
            "NaiveBayes __init(alpha=0.22)__\n",
            "NaiveBayes __init(alpha=0.23)__\n",
            "NaiveBayes __init(alpha=0.24000000000000002)__\n",
            "NaiveBayes __init(alpha=0.25)__\n",
            "NaiveBayes __init(alpha=0.26)__\n",
            "NaiveBayes __init(alpha=0.27)__\n",
            "NaiveBayes __init(alpha=0.28)__\n",
            "NaiveBayes __init(alpha=0.29000000000000004)__\n",
            "NaiveBayes __init(alpha=0.3)__\n",
            "NaiveBayes __init(alpha=0.31)__\n",
            "NaiveBayes __init(alpha=0.32)__\n",
            "NaiveBayes __init(alpha=0.33)__\n",
            "NaiveBayes __init(alpha=0.34)__\n",
            "NaiveBayes __init(alpha=0.35000000000000003)__\n",
            "NaiveBayes __init(alpha=0.36000000000000004)__\n",
            "NaiveBayes __init(alpha=0.37)__\n",
            "NaiveBayes __init(alpha=0.38)__\n",
            "NaiveBayes __init(alpha=0.39)__\n",
            "NaiveBayes __init(alpha=0.4)__\n",
            "NaiveBayes __init(alpha=0.41000000000000003)__\n",
            "NaiveBayes __init(alpha=0.42000000000000004)__\n",
            "NaiveBayes __init(alpha=0.43)__\n",
            "NaiveBayes __init(alpha=0.44)__\n",
            "NaiveBayes __init(alpha=0.45)__\n",
            "NaiveBayes __init(alpha=0.46)__\n",
            "NaiveBayes __init(alpha=0.47000000000000003)__\n",
            "NaiveBayes __init(alpha=0.48000000000000004)__\n",
            "NaiveBayes __init(alpha=0.49)__\n",
            "NaiveBayes __init(alpha=0.5)__\n",
            "NaiveBayes __init(alpha=0.51)__\n",
            "NaiveBayes __init(alpha=0.52)__\n",
            "NaiveBayes __init(alpha=0.53)__\n",
            "NaiveBayes __init(alpha=0.54)__\n",
            "NaiveBayes __init(alpha=0.55)__\n",
            "NaiveBayes __init(alpha=0.56)__\n",
            "NaiveBayes __init(alpha=0.5700000000000001)__\n",
            "NaiveBayes __init(alpha=0.5800000000000001)__\n",
            "NaiveBayes __init(alpha=0.59)__\n",
            "NaiveBayes __init(alpha=0.6)__\n",
            "NaiveBayes __init(alpha=0.61)__\n",
            "NaiveBayes __init(alpha=0.62)__\n",
            "NaiveBayes __init(alpha=0.63)__\n",
            "NaiveBayes __init(alpha=0.64)__\n",
            "NaiveBayes __init(alpha=0.65)__\n",
            "NaiveBayes __init(alpha=0.66)__\n",
            "NaiveBayes __init(alpha=0.67)__\n",
            "NaiveBayes __init(alpha=0.68)__\n",
            "NaiveBayes __init(alpha=0.6900000000000001)__\n",
            "NaiveBayes __init(alpha=0.7000000000000001)__\n",
            "NaiveBayes __init(alpha=0.7100000000000001)__\n",
            "NaiveBayes __init(alpha=0.72)__\n",
            "NaiveBayes __init(alpha=0.73)__\n",
            "NaiveBayes __init(alpha=0.74)__\n",
            "NaiveBayes __init(alpha=0.75)__\n",
            "NaiveBayes __init(alpha=0.76)__\n",
            "NaiveBayes __init(alpha=0.77)__\n",
            "NaiveBayes __init(alpha=0.78)__\n",
            "NaiveBayes __init(alpha=0.79)__\n",
            "NaiveBayes __init(alpha=0.8)__\n",
            "NaiveBayes __init(alpha=0.81)__\n",
            "NaiveBayes __init(alpha=0.8200000000000001)__\n",
            "NaiveBayes __init(alpha=0.8300000000000001)__\n",
            "NaiveBayes __init(alpha=0.8400000000000001)__\n",
            "NaiveBayes __init(alpha=0.85)__\n",
            "NaiveBayes __init(alpha=0.86)__\n",
            "NaiveBayes __init(alpha=0.87)__\n",
            "NaiveBayes __init(alpha=0.88)__\n",
            "NaiveBayes __init(alpha=0.89)__\n",
            "NaiveBayes __init(alpha=0.9)__\n",
            "NaiveBayes __init(alpha=0.91)__\n",
            "NaiveBayes __init(alpha=0.92)__\n",
            "NaiveBayes __init(alpha=0.93)__\n",
            "NaiveBayes __init(alpha=0.9400000000000001)__\n",
            "NaiveBayes __init(alpha=0.9500000000000001)__\n",
            "NaiveBayes __init(alpha=0.9600000000000001)__\n",
            "NaiveBayes __init(alpha=0.97)__\n",
            "NaiveBayes __init(alpha=0.98)__\n",
            "NaiveBayes __init(alpha=0.99)__\n",
            "NaiveBayes __init(alpha=1.0)__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbwK_kMxTY",
        "outputId": "170b91d7-9740-477c-c7ef-a2e01ce23dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "df_results_NB.sort_values(by=['Accuracy(%)'], inplace=True, ascending=False)\n",
        "display(df_results_NB.head(10))\n",
        "df_results_NB.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Using-2D-Pairwise/NB-val-topics.csv', index=False)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.03</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.07</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.15</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.24</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.18</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.17</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ALPHA  Accuracy(%)\n",
              "3    0.04    88.500000\n",
              "1    0.02    88.500000\n",
              "2    0.03    88.454545\n",
              "4    0.05    88.454545\n",
              "6    0.07    88.454545\n",
              "0    0.01    88.409091\n",
              "14   0.15    88.409091\n",
              "23   0.24    88.409091\n",
              "17   0.18    88.409091\n",
              "16   0.17    88.409091"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtew8yEpMxep",
        "outputId": "020e07cb-1870-4139-e098-30a2850d9c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "display(df_results_NB.head(10))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.03</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.07</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.15</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.24</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.18</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.17</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ALPHA  Accuracy(%)\n",
              "3    0.04    88.500000\n",
              "1    0.02    88.500000\n",
              "2    0.03    88.454545\n",
              "4    0.05    88.454545\n",
              "6    0.07    88.454545\n",
              "0    0.01    88.409091\n",
              "14   0.15    88.409091\n",
              "23   0.24    88.409091\n",
              "17   0.18    88.409091\n",
              "16   0.17    88.409091"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4G-LMq-nUTK"
      },
      "source": [
        "# Hypothesis testing on Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msSFrab2iWbT"
      },
      "source": [
        "del hamming_vectors_2D_validation\n",
        "del euclidean_vectors_2D_validation\n",
        "del tf_idf_validation_set"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG7zmcwEnbbT"
      },
      "source": [
        "### Create and fit best performing models on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjLPICwjpSdJ",
        "outputId": "a9b1b1f8-026f-473d-857b-528d27638859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "print(words_val)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sagul', 'give', 'deflect', 'valu', 'horizont', 'shelf', 'span', 'use', 'variou', 'materi', 'thick', 'unless', 'work', 'extrem', 'heavi', 'load', 'weight', 'countertop', 'peopl', 'danc', 'materi', 'vertic', 'cabinet', 'wall', 'adequ', 'make', 'wall', 'thinner', 'still', 'remain', 'surprisingli', 'strong', 'long', 'cabinet', 'design', 'prevent', 'rack', 'even', 'particleboard', 'known', 'strength', 'hold', 'well', 'vertic', 'compressionbuckl', 'may', 'know', 'youv', 'ever', 'pack', 'book', 'store', 'cheap', 'particleboard', 'bookcas']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaPA10d8na_G"
      },
      "source": [
        "##### KNN was K=15, TF-IDF #####\n",
        "K_best = 15\n",
        "knn = KNN(mode='cosine_similarity', to_print=True)\n",
        "knn.populate_vectors(TF_IDF_whole_corpus, labels_train)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCgX5cNKpard",
        "outputId": "f288b5bf-2166-41fa-8fd4-c0d3bd4e628c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_tf_idf = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "print(knn.predict(val_tf_idf, num_neighbors=15))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOeVWWRdna3D",
        "outputId": "95f47df4-be22-4305-8b64-4ef2c39a9ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##### Naive Bayes was alpha = 0.02/0.04, we will take 0.04 #####\n",
        "NB = NaiveBayes(alpha=0.04, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.04)__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHqRiSQYpfEB",
        "outputId": "665f912c-3838-47d7-b7be-f3c8a1c17d0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(NB.predict(words_val))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuz4R0PDoZsy"
      },
      "source": [
        "del df_train\n",
        "del df_val"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7ms6GZhm4CM"
      },
      "source": [
        "### Split test dataset 50 iterations per 10 of each topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er7I3xb2jxbS",
        "outputId": "44d4877d-6e11-4c62-a1fc-f4daaea4854c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df_test.columns.values)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['content' 'Label' 'stemmed_content' 'ham_vector' 'euc_vector']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCLTyO1nn34U"
      },
      "source": [
        "df_test.sort_values(by=['Label'], inplace=True)\n",
        "df_test.drop(labels=['content', 'ham_vector', 'euc_vector'], axis=1, inplace=True)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08aKywOkullM",
        "outputId": "5ee6428a-a315-4c21-ded9-8327e68050c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "column_names = [\"KNN-Acc(%)\", \"NB-Acc(%)\"]\n",
        "\n",
        "df_results_test_set = pd.DataFrame(columns=column_names)\n",
        "\n",
        "print(df_results_test_set.head())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [KNN-Acc(%), NB-Acc(%)]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNcU7hdTn3tc",
        "outputId": "ef438509-490b-4065-ffba-29175b03f7a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_docs_in_each_topic = 500 ##\n",
        "initial_offset = np.array([i*num_docs_in_each_topic for i in range(0, 11)])\n",
        "for counter_test in range(0, 50): ## run iterations 50 times\n",
        "    offsets = initial_offset + counter_test*10\n",
        "    start_indices = offsets\n",
        "    end_indices = offsets + 10\n",
        "    \n",
        "    print(\"\\nCounter = \", counter_test)\n",
        "    # print(offsets)\n",
        "    # print(start_indices)\n",
        "    # print(end_indices)\n",
        "\n",
        "    ### Testing here ###\n",
        "    nb_correct = knn_correct = 0\n",
        "    for i in range(len(start_indices)): ## add all to list.\n",
        "        # print(np.unique(df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values, return_counts=True), end=' ')\n",
        "        for (x, y) in zip(df_test.iloc[start_indices[i]:end_indices[i]]['stemmed_content'].values, df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values):\n",
        "            \n",
        "            #### For KNN ####\n",
        "            x_tf_idf = form_TF_IDF_for_val_test(list_words=x, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "            y_pred = knn.predict(x_tf_idf, num_neighbors=K_best)\n",
        "            if y_pred == y:\n",
        "                knn_correct += 1\n",
        "            \n",
        "            #### For Naive Bayes #####\n",
        "            y_pred = NB.predict(x)\n",
        "            if y_pred == y:\n",
        "                nb_correct += 1\n",
        "            \n",
        "    \n",
        "    print(f\"Done for counter_test = {counter_test}\")\n",
        "\n",
        "    NUM_DOCUMENTS = 10* len(np.unique(df_test['Label'].values))\n",
        "    knn_acc = knn_correct/( NUM_DOCUMENTS )*100  ## 10*11 total 110 test documents per iteration. [50 iterations]\n",
        "    nb_acc = nb_correct/( NUM_DOCUMENTS )*100\n",
        "    print(f\"KNN-Acc = {knn_acc}%, NB-Acc = {nb_acc}%\")\n",
        "\n",
        "    df_results_test_set = add_to_dataframe(df_old=df_results_test_set, to_add=[knn_acc, nb_acc])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Counter =  0\n",
            "Done for counter_test = 0\n",
            "KNN-Acc = 85.45454545454545%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  1\n",
            "Done for counter_test = 1\n",
            "KNN-Acc = 80.9090909090909%, NB-Acc = 84.54545454545455%\n",
            "\n",
            "Counter =  2\n",
            "Done for counter_test = 2\n",
            "KNN-Acc = 79.0909090909091%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  3\n",
            "Done for counter_test = 3\n",
            "KNN-Acc = 87.27272727272727%, NB-Acc = 90.0%\n",
            "\n",
            "Counter =  4\n",
            "Done for counter_test = 4\n",
            "KNN-Acc = 89.0909090909091%, NB-Acc = 91.81818181818183%\n",
            "\n",
            "Counter =  5\n",
            "Done for counter_test = 5\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 86.36363636363636%\n",
            "\n",
            "Counter =  6\n",
            "Done for counter_test = 6\n",
            "KNN-Acc = 77.27272727272727%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  7\n",
            "Done for counter_test = 7\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 91.81818181818183%\n",
            "\n",
            "Counter =  8\n",
            "Done for counter_test = 8\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  9\n",
            "Done for counter_test = 9\n",
            "KNN-Acc = 80.9090909090909%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  10\n",
            "Done for counter_test = 10\n",
            "KNN-Acc = 85.45454545454545%, NB-Acc = 90.0%\n",
            "\n",
            "Counter =  11\n",
            "Done for counter_test = 11\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 92.72727272727272%\n",
            "\n",
            "Counter =  12\n",
            "Done for counter_test = 12\n",
            "KNN-Acc = 86.36363636363636%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  13\n",
            "Done for counter_test = 13\n",
            "KNN-Acc = 85.45454545454545%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  14\n",
            "Done for counter_test = 14\n",
            "KNN-Acc = 80.0%, NB-Acc = 90.0%\n",
            "\n",
            "Counter =  15\n",
            "Done for counter_test = 15\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  16\n",
            "Done for counter_test = 16\n",
            "KNN-Acc = 80.9090909090909%, NB-Acc = 91.81818181818183%\n",
            "\n",
            "Counter =  17\n",
            "Done for counter_test = 17\n",
            "KNN-Acc = 80.0%, NB-Acc = 86.36363636363636%\n",
            "\n",
            "Counter =  18\n",
            "Done for counter_test = 18\n",
            "KNN-Acc = 87.27272727272727%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  19\n",
            "Done for counter_test = 19\n",
            "KNN-Acc = 88.18181818181819%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  20\n",
            "Done for counter_test = 20\n",
            "KNN-Acc = 80.0%, NB-Acc = 85.45454545454545%\n",
            "\n",
            "Counter =  21\n",
            "Done for counter_test = 21\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 87.27272727272727%\n",
            "\n",
            "Counter =  22\n",
            "Done for counter_test = 22\n",
            "KNN-Acc = 88.18181818181819%, NB-Acc = 92.72727272727272%\n",
            "\n",
            "Counter =  23\n",
            "Done for counter_test = 23\n",
            "KNN-Acc = 86.36363636363636%, NB-Acc = 94.54545454545455%\n",
            "\n",
            "Counter =  24\n",
            "Done for counter_test = 24\n",
            "KNN-Acc = 79.0909090909091%, NB-Acc = 83.63636363636363%\n",
            "\n",
            "Counter =  25\n",
            "Done for counter_test = 25\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 93.63636363636364%\n",
            "\n",
            "Counter =  26\n",
            "Done for counter_test = 26\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 94.54545454545455%\n",
            "\n",
            "Counter =  27\n",
            "Done for counter_test = 27\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  28\n",
            "Done for counter_test = 28\n",
            "KNN-Acc = 85.45454545454545%, NB-Acc = 94.54545454545455%\n",
            "\n",
            "Counter =  29\n",
            "Done for counter_test = 29\n",
            "KNN-Acc = 80.9090909090909%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  30\n",
            "Done for counter_test = 30\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  31\n",
            "Done for counter_test = 31\n",
            "KNN-Acc = 84.54545454545455%, NB-Acc = 92.72727272727272%\n",
            "\n",
            "Counter =  32\n",
            "Done for counter_test = 32\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  33\n",
            "Done for counter_test = 33\n",
            "KNN-Acc = 87.27272727272727%, NB-Acc = 87.27272727272727%\n",
            "\n",
            "Counter =  34\n",
            "Done for counter_test = 34\n",
            "KNN-Acc = 89.0909090909091%, NB-Acc = 91.81818181818183%\n",
            "\n",
            "Counter =  35\n",
            "Done for counter_test = 35\n",
            "KNN-Acc = 88.18181818181819%, NB-Acc = 95.45454545454545%\n",
            "\n",
            "Counter =  36\n",
            "Done for counter_test = 36\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 86.36363636363636%\n",
            "\n",
            "Counter =  37\n",
            "Done for counter_test = 37\n",
            "KNN-Acc = 84.54545454545455%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  38\n",
            "Done for counter_test = 38\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 90.9090909090909%\n",
            "\n",
            "Counter =  39\n",
            "Done for counter_test = 39\n",
            "KNN-Acc = 82.72727272727273%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  40\n",
            "Done for counter_test = 40\n",
            "KNN-Acc = 78.18181818181819%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  41\n",
            "Done for counter_test = 41\n",
            "KNN-Acc = 70.9090909090909%, NB-Acc = 82.72727272727273%\n",
            "\n",
            "Counter =  42\n",
            "Done for counter_test = 42\n",
            "KNN-Acc = 87.27272727272727%, NB-Acc = 92.72727272727272%\n",
            "\n",
            "Counter =  43\n",
            "Done for counter_test = 43\n",
            "KNN-Acc = 84.54545454545455%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  44\n",
            "Done for counter_test = 44\n",
            "KNN-Acc = 80.0%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  45\n",
            "Done for counter_test = 45\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  46\n",
            "Done for counter_test = 47\n",
            "KNN-Acc = 83.63636363636363%, NB-Acc = 88.18181818181819%\n",
            "\n",
            "Counter =  48\n",
            "Done for counter_test = 48\n",
            "KNN-Acc = 87.27272727272727%, NB-Acc = 89.0909090909091%\n",
            "\n",
            "Counter =  49\n",
            "Done for counter_test = 49\n",
            "KNN-Acc = 81.81818181818183%, NB-Acc = 87.27272727272727%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrDQX97qrchw"
      },
      "source": [
        "df_results_test_set.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Using-2D-Pairwise/Test-Set-11-KNN-NB.csv', index=False)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkTag67arcpn",
        "outputId": "233f8244-a3c3-4f36-98e8-bcba259a762a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f\"len df_results_test_set = {len(df_results_test_set)}\")\n",
        "display(df_results_test_set)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_test_set = 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>79.090909</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>87.272727</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>89.090909</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>87.272727</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>88.181818</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>85.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>88.181818</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>79.090909</td>\n",
              "      <td>83.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>93.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>87.272727</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>89.090909</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>88.181818</td>\n",
              "      <td>95.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>70.909091</td>\n",
              "      <td>82.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>87.272727</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>87.272727</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>87.272727</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    KNN-Acc(%)  NB-Acc(%)\n",
              "0    85.454545  88.181818\n",
              "1    80.909091  84.545455\n",
              "2    79.090909  90.909091\n",
              "3    87.272727  90.000000\n",
              "4    89.090909  91.818182\n",
              "5    83.636364  86.363636\n",
              "6    77.272727  90.909091\n",
              "7    83.636364  91.818182\n",
              "8    83.636364  90.909091\n",
              "9    80.909091  89.090909\n",
              "10   85.454545  90.000000\n",
              "11   81.818182  92.727273\n",
              "12   86.363636  88.181818\n",
              "13   85.454545  90.909091\n",
              "14   80.000000  90.000000\n",
              "15   82.727273  88.181818\n",
              "16   80.909091  91.818182\n",
              "17   80.000000  86.363636\n",
              "18   87.272727  90.909091\n",
              "19   88.181818  90.909091\n",
              "20   80.000000  85.454545\n",
              "21   83.636364  87.272727\n",
              "22   88.181818  92.727273\n",
              "23   86.363636  94.545455\n",
              "24   79.090909  83.636364\n",
              "25   83.636364  93.636364\n",
              "26   82.727273  94.545455\n",
              "27   82.727273  89.090909\n",
              "28   85.454545  94.545455\n",
              "29   80.909091  90.909091\n",
              "30   82.727273  89.090909\n",
              "31   84.545455  92.727273\n",
              "32   81.818182  89.090909\n",
              "33   87.272727  87.272727\n",
              "34   89.090909  91.818182\n",
              "35   88.181818  95.454545\n",
              "36   81.818182  86.363636\n",
              "37   84.545455  90.909091\n",
              "38   81.818182  90.909091\n",
              "39   82.727273  89.090909\n",
              "40   78.181818  88.181818\n",
              "41   70.909091  82.727273\n",
              "42   87.272727  92.727273\n",
              "43   84.545455  89.090909\n",
              "44   80.000000  88.181818\n",
              "45   81.818182  89.090909\n",
              "46   87.272727  90.000000\n",
              "47   83.636364  88.181818\n",
              "48   87.272727  89.090909\n",
              "49   81.818182  87.272727"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBHCUUtF2Kz8"
      },
      "source": [
        "## Load and analyze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dee3qAsxrcg6",
        "outputId": "6231497c-4ba9-4083-cbf3-401168df1eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# df_results_test_set = pd.read_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Test-Set-KNN-NB.csv')\n",
        "print(df_results_test_set.head(5))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   KNN-Acc(%)  NB-Acc(%)\n",
            "0   85.454545  88.181818\n",
            "1   80.909091  84.545455\n",
            "2   79.090909  90.909091\n",
            "3   87.272727  90.000000\n",
            "4   89.090909  91.818182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4tqi3TS3LH0",
        "outputId": "1e02bcbe-c631-4a66-fb25-293d061c0b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "knn_acc = df_results_test_set['KNN-Acc(%)'].values\n",
        "nb_acc = df_results_test_set['NB-Acc(%)'].values\n",
        "print(knn_acc.shape, nb_acc.shape)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50,) (50,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDJKAptY5mEK",
        "outputId": "307bcebc-545e-4923-f938-e417c19d7ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "column_names = [\"Method\", \"Mean-Acc(%)\", \"Std-dev\", \"Std-Error\", \"Minimum-Acc(%)\", \"Maximum-Acc(%)\"]\n",
        "\n",
        "df_stats = pd.DataFrame(columns=column_names)\n",
        "\n",
        "print(df_stats.head())"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Method, Mean-Acc(%), Std-dev, Std-Error, Minimum-Acc(%), Maximum-Acc(%)]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MIAT9Dy3LF4"
      },
      "source": [
        "## Summarized results.\n",
        "df_stats = add_to_dataframe(df_old=df_stats, to_add=[\"KNN K=15, TF-IDF\", np.mean(knn_acc), np.std(knn_acc), stats.sem(knn_acc, axis=None, ddof=0), min(knn_acc), max(knn_acc)])\n",
        "df_stats = add_to_dataframe(df_old=df_stats, to_add=[\"NB alpha = 0.04\", np.mean(nb_acc), np.std(nb_acc), stats.sem(nb_acc, axis=None, ddof=0), min(nb_acc), max(nb_acc)])"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tg1WtdcrcX9",
        "outputId": "4280e533-b5ac-4659-caa3-8d51497b5e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(df_stats)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             Method  Mean-Acc(%)  ...  Minimum-Acc(%)  Maximum-Acc(%)\n",
            "0  KNN K=15, TF-IDF    83.381818  ...       70.909091       89.090909\n",
            "1   NB alpha = 0.04    89.763636  ...       82.727273       95.454545\n",
            "\n",
            "[2 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYKuEjwr7vdM"
      },
      "source": [
        "## Computing T-statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyDAasE1RozN"
      },
      "source": [
        "knn_acc = df_results_test_set['KNN-Acc(%)'].values\n",
        "nb_acc = df_results_test_set['NB-Acc(%)'].values"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNcoJHAZ64EK",
        "outputId": "eb05382b-7de1-4ea0-c5a9-5b6ee001a68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
        "\n",
        "# ans = stats.ttest_rel(nb_acc, knn_acc)\n",
        "# ans = stats.ttest_ind(nb_acc, knn_acc, equal_var=True)\n",
        "ans = stats.ttest_rel(nb_acc, knn_acc)\n",
        "print(ans)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ttest_relResult(statistic=14.245213763349957, pvalue=4.766189926008192e-19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBFks-IP64CR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI990uMi634Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}