{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Offline-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrnubMIEG0BZ"
      },
      "source": [
        "### MOUNT DRIVE ....."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1oJyY78cXnq"
      },
      "source": [
        "## First Import statements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import pickle\n",
        "\n",
        "from scipy import stats\n",
        "from xml.dom import minidom"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFLKkx6Ybnlr"
      },
      "source": [
        "FOLDER_TRAIN = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/Training/'\n",
        "# FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics.txt'\n",
        "FILE_TOPICS = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Data/topics-all.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9hrTtEZp2A8"
      },
      "source": [
        "# !ls \"$FOLDER_TRAIN\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0bZ4kTHcXql"
      },
      "source": [
        "## Set random seed\n",
        "RANDOM_STATE = 22\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThQtktwcXtX",
        "outputId": "3a0fba6e-1b4d-499f-b010-9914b98d9cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%%time\n",
        "## Read data\n",
        "topic_remove = '3d_Printer' ## Remove 3D-printer\n",
        "list_doc_types = []\n",
        "with open(FILE_TOPICS, 'r') as fp:\n",
        "    topic = fp.readline()\n",
        "    while topic:\n",
        "        topic = topic.replace(\"\\n\", \"\")\n",
        "        list_doc_types.append(topic)\n",
        "        topic = fp.readline()\n",
        "\n",
        "list_doc_types.remove(topic_remove)\n",
        "print(list_doc_types)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Astronomy', 'Coffee', 'Space', 'Anime', 'Biology', 'Cooking', 'Windows_Phone', 'Arduino', 'Chess', 'Law', 'Wood_Working']\n",
            "CPU times: user 1.6 ms, sys: 0 ns, total: 1.6 ms\n",
            "Wall time: 3.61 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfC2PYXxg35A",
        "outputId": "021f3437-1bb4-4590-cfd5-cbf57e383872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(list_doc_types)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Astronomy', 'Coffee', 'Space', 'Anime', 'Biology', 'Cooking', 'Windows_Phone', 'Arduino', 'Chess', 'Law', 'Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVPfrYjagVv4"
      },
      "source": [
        "def get_train_val_test_data(file_name):\n",
        "    xmldoc = minidom.parse(file_name)\n",
        "    xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "    \n",
        "    # print(f\"Inside get_train_val_test_data(), len(item_list) = {len(item_list)}\")\n",
        "\n",
        "    item_list = [x.attributes['Body'].value for x in xml_list]\n",
        "\n",
        "    train_list = item_list[0:500] ## first 500 train\n",
        "    val_list =  item_list[500:700] ## next 200 val\n",
        "    test_list = item_list[700:1200] ## next 500 test\n",
        "\n",
        "    ## delete original list.\n",
        "    del item_list\n",
        "    del xmldoc\n",
        "\n",
        "    ## return the new lists\n",
        "    return train_list, val_list, test_list"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvop_yNmUCMo"
      },
      "source": [
        "def add_to_dataframe(df_old, to_add):\n",
        "    df_old = df_old.append(pd.Series(to_add, index=df_old.columns), ignore_index=True)\n",
        "    return df_old"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MJXly5bt7aC"
      },
      "source": [
        "## Create three dataframes.\n",
        "\n",
        "## https://www.kite.com/python/answers/how-to-create-an-empty-dataframe-with-column-names-in-python\n",
        "column_names =[\"content\", \"Label\"]\n",
        "\n",
        "df_train = pd.DataFrame(columns = column_names)\n",
        "df_val = pd.DataFrame(columns = column_names)\n",
        "df_test = pd.DataFrame(columns = column_names)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxZHmfGt1g9e",
        "outputId": "0d545dda-cbe0-4f5d-e881-dfc43bb88f82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "%%time\n",
        "### Populate all topics\n",
        "def populate_data_frames(df_train, df_val, df_test, list_doc_types):\n",
        "    for topic in list_doc_types: ## iterating per topic/label\n",
        "        label = topic ## assign label\n",
        "        \n",
        "        ## read using xml package\n",
        "        file_name = FOLDER_TRAIN + topic + \".xml\"\n",
        "        xmldoc = minidom.parse(file_name)\n",
        "        xml_list = xmldoc.getElementsByTagName('row') ## tag using 'row'\n",
        "\n",
        "        ## get train, val, test lists\n",
        "        train_list, val_list, test_list = get_train_val_test_data(file_name=file_name)\n",
        "\n",
        "        print(len(train_list), len(val_list), len(test_list))\n",
        "\n",
        "        ## add to dataframe\n",
        "        for v1 in train_list:\n",
        "            df_train = add_to_dataframe(df_old=df_train, to_add=[v1, label])\n",
        "        for v2 in val_list:\n",
        "            df_val = add_to_dataframe(df_old=df_val, to_add=[v2, label])\n",
        "        for v3 in test_list:\n",
        "            df_test = add_to_dataframe(df_old=df_test, to_add=[v3, label])\n",
        "\n",
        "\n",
        "        ## delete original list\n",
        "        del train_list\n",
        "        del val_list\n",
        "        del test_list\n",
        "\n",
        "    ## return dataframes\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "### Call the function\n",
        "df_train, df_val, df_test = populate_data_frames(df_train=df_train, df_val=df_val, df_test=df_test, list_doc_types=list_doc_types)\n",
        "\n",
        "print(f\"len df_train = {len(df_train)}\")\n",
        "print(f\"len df_val = {len(df_val)}\")\n",
        "print(f\"len df_test = {len(df_test)}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "500 200 500\n",
            "len df_train = 5500\n",
            "len df_val = 2200\n",
            "len df_test = 5500\n",
            "CPU times: user 1min 11s, sys: 2.08 s, total: 1min 13s\n",
            "Wall time: 1min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUI07UZKmPc"
      },
      "source": [
        "## Shuffle dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rcv33P3XXuN"
      },
      "source": [
        "df_train = df_train.sample(frac=1, random_state=RANDOM_STATE)\n",
        "df_val = df_val.sample(frac=1, random_state=RANDOM_STATE)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1m0_Du8XYBK"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKSziS4PvdnZ"
      },
      "source": [
        "## https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
        "## https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "## from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIukiiJxDWdu",
        "outputId": "820f050d-8169-4e27-858a-dcfd83b11e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "## https://stackoverflow.com/questions/26693736/nltk-and-stopwords-fail-lookuperror\n",
        "## https://stackoverflow.com/questions/26570944/resource-utokenizers-punkt-english-pickle-not-found\n",
        "\n",
        "# nltk.download()\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUsCIVJru269"
      },
      "source": [
        "## Preprocess each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-MlUQ4w1uM2",
        "cellView": "both",
        "outputId": "90ad74ec-f203-4a14-ccf0-ed97c63a948c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "## https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "## First lemmatize, and then stem.\n",
        "\n",
        "def pre_process_stem_sentence(sentence, stop_words, stemmer, lemmatizer):\n",
        "    ## save in another variable\n",
        "    text = sentence\n",
        "    ## remove HTML tags [using soup]\n",
        "    soup = BeautifulSoup(text)\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    ## remove <a href> type things\n",
        "    soup = BeautifulSoup(text) ## create soup again.\n",
        "    for a in soup.findAll('a'):\n",
        "        a.replaceWithChildren()\n",
        "    \n",
        "    text = str(soup) ## reform text\n",
        "\n",
        "\n",
        "    ## remove unicode.\n",
        "    text = re.sub(r\"&nbsp;\", \" \", text)\n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
        "\n",
        "    ## remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"https\\S+\", \"\", text)\n",
        "    ## text = re.sub(r\"www\\S+\", \"\", text)\n",
        "\n",
        "    ## to be safe, remove HTML tags again using regex\n",
        "    #### https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44  \n",
        "    clean = re.compile('<.*?>')\n",
        "    text = re.sub(clean, '', text)\n",
        "\n",
        "    ## convert to small letters.\n",
        "    text = text.lower()\n",
        "\n",
        "    ## replace newlines, tabs -> SPACE\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "\n",
        "    ## {COLON} make <a>:<b> become <a>[space]<b>\n",
        "    # text = text.replace(\": \", \":\") # :[space] -> : \n",
        "    text = text.replace(\":\", \" \")  # : -> [space]\n",
        "    \n",
        "    ## {HYPHEN}\n",
        "    # text = text.replace(\"- \", \"-\")\n",
        "    text = text.replace(\"-\", \" \")\n",
        "\n",
        "    ## numbers removal    \n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "\n",
        "    ## punctuations removal\n",
        "    text = text.translate((str.maketrans('','',string.punctuation)))   \n",
        "\n",
        "\n",
        "    ## [remove anything EXCEPT english letters]\n",
        "    ## https://stackoverflow.com/questions/6323296/python-remove-anything-that-is-not-a-letter-or-number\n",
        "    text = re.sub(  \"[^a-z ]\",              # Anything except 0..9, a..z and A..Z\n",
        "                    \"\",                     # replaced with nothing\n",
        "                    text)                   # in this string   \n",
        "\n",
        "    ## remove space initially and finally.\n",
        "    text = text.lstrip()\n",
        "\n",
        "    ## make double spaces become one space.\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    ## remove stop words (English).\n",
        "    # print(f\"Before stopwords removal, text.len = {len(text)}\")\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words_removed_words_list = [t for t in tokens if not t in stop_words]\n",
        "\n",
        "    # print(f\"After tokenize and stopwords, len stop_words_removed_words_list = {len(stop_words_removed_words_list)}\")\n",
        "\n",
        "    ## apply lemmatization.\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stop_words_removed_words_list]\n",
        "    \n",
        "    ## apply stemming.\n",
        "    stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
        "\n",
        "    ## return the final pre-processed list-of-words.\n",
        "    return stemmed_words\n",
        "\n",
        "################ Test on one sentence #######################\n",
        "\n",
        "## Obtain once.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "## Preprocess per element of dataframe to test.\n",
        "text_to_process = df_train['content'].iloc[499]\n",
        "print(text_to_process)\n",
        "\n",
        "print(\"--\"*90)\n",
        "\n",
        "preprocessed_text = pre_process_stem_sentence(sentence=text_to_process, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(preprocessed_text)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<h2>You never mentioned your tablet model... or Arduino model</h2>\n",
            "\n",
            "<p>There are some tablets designed to work with USB flash drives and printers. This is called OTG (on the go.</p>\n",
            "\n",
            "<p><strong>You need:</strong></p>\n",
            "\n",
            "<ul>\n",
            "<li>A tablet supporting OTG</li>\n",
            "<li>An OTG adapter that fits into your tablets USB slot and is compatible</li>\n",
            "<li><a href=\"https://play.google.com/store/apps/details?id=com.primavera.arduino.listener\" rel=\"nofollow\">The Arduino Uno Communicator App</a></li>\n",
            "<li>Arduino Uno (or clone) [Note: It says that it works with Atmega16U2 or Atmega8U2 programmed as a USB-to-serial converter so I would assume that that would cover a few boards more than the Uno.)</li>\n",
            "</ul>\n",
            "\n",
            "<p>Another alternitive is to look into <a href=\"https://www.google.com/search?q=arduino+bluetooth&amp;qscrl=1&amp;tbm=isch&amp;imgil=WyPC4O05Xn4rhM%253A%253Bhttps%253A%252F%252Fencrypted-tbn2.gstatic.com%252Fimages%253Fq%253Dtbn%253AANd9GcSfTtWqIuN9YYdd90OO_tlxsvLUYhV_pl7NfaMhHDzSRq9N6W9rNA%253B1200%253B1200%253B1soQc4CzMzh0MM%253Bhttp%25253A%25252F%25252Fm.dhgate.com%25252Fproduct%25252Fjy-mcu-arduino-bluetooth-wireless-serial%25252F151048482.html&amp;source=iu&amp;usg=__KyMLS4aeubxJnDaMvprgRgADxAs%3D&amp;sa=X&amp;ei=1BUCU635IYmEygGO1oHwCQ&amp;ved=0CGkQ9QEwBg&amp;biw=1280&amp;bih=899\" rel=\"nofollow\">Bluetooth</a> (Note: I just picked a random link but there are hundreds of similar BT adapters)</p>\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "['never', 'mention', 'tablet', 'model', 'arduino', 'model', 'tablet', 'design', 'work', 'usb', 'flash', 'drive', 'printer', 'call', 'otg', 'go', 'need', 'tablet', 'support', 'otg', 'otg', 'adapt', 'fit', 'tablet', 'usb', 'slot', 'compat', 'arduino', 'uno', 'commun', 'app', 'arduino', 'uno', 'clone', 'note', 'say', 'work', 'atmegau', 'atmegau', 'program', 'usb', 'serial', 'convert', 'would', 'assum', 'would', 'cover', 'board', 'uno', 'anoth', 'alternit', 'look', 'bluetooth', 'note', 'pick', 'random', 'link', 'hundr', 'similar', 'bt', 'adapt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHlPyKTDg9a6",
        "cellView": "form"
      },
      "source": [
        "#@title Not using this\n",
        "## https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "## First lemmatize, and then stem.\n",
        "\n",
        "# def pre_process_stem_sentence(sentence, stop_words, stemmer, lemmatizer):\n",
        "#     ## save in another variable\n",
        "#     text = sentence\n",
        "\n",
        "#     ## remove HTML tags.\n",
        "#     clean = re.compile('<.*?>')\n",
        "#     text = re.sub(clean, '', text)\n",
        "\n",
        "#     ## Convert to small letters.\n",
        "#     text = text.lower()\n",
        "\n",
        "#     # Number Removal\n",
        "#     text = re.sub(r'[-+]?\\d+', '', text)\n",
        "\n",
        "\n",
        "#     ## remove links\n",
        "#     text = re.sub(r\"https\\S+\", \"\", text)\n",
        "#     text = re.sub(r\"www\\S+\", \"\", text)\n",
        "#     text = re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "\n",
        "#     text = text.replace(\"-\", \" \")\n",
        "    \n",
        "#     # Remove punctuations\n",
        "#     text = text.translate((str.maketrans('','',string.punctuation)))    \n",
        "\n",
        "    \n",
        "\n",
        "#     ## remove unicodes.\n",
        "#     # text = re.sub(r\"&nbsp;\", \" \", text)\n",
        "#     # text = re.sub(r'[-+]?\\d+', '', text)\n",
        "#     text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
        "#     # encoded_string = text.encode(\"ascii\", \"ignore\")\n",
        "#     # text = encoded_string.decode()\n",
        "\n",
        "\n",
        "\n",
        "#     # text = re.sub(  \"[^a-z ]\",              # Anything except 0..9, a..z and A..Z\n",
        "#     #                 \"\",                     # replaced with nothing\n",
        "#     #                 text)                   # in this string   \n",
        "\n",
        "    \n",
        "\n",
        "#     ## remove space initially and finally.\n",
        "#     # text = text.lstrip()\n",
        "\n",
        "#     ## make double spaces become one space.\n",
        "#     # text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "#     # Tokenize\n",
        "#     text = word_tokenize(text)\n",
        "\n",
        "#     # Remove stopwords\n",
        "#     # stop_words = set(stopwords.words('english'))\n",
        "#     text = [word for word in text if not word in stop_words]\n",
        "\n",
        "#     # Lemmatize tokens\n",
        "#     text = [lemmatizer.lemmatize(word) for word in text]\n",
        "\n",
        "#     # Stemming tokens\n",
        "#     text = [stemmer.stem(word) for word in text]\n",
        "\n",
        "#     return text\n",
        "\n",
        "################ Test on one sentence #######################\n",
        "\n",
        "# ## Obtain once.\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# stemmer = PorterStemmer()\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# ## Preprocess per element of dataframe to test.\n",
        "# text_to_process = df_train['content'].iloc[499]\n",
        "# print(text_to_process)\n",
        "\n",
        "# print(\"--\"*90)\n",
        "\n",
        "# preprocessed_text = pre_process_stem_sentence(sentence=text_to_process, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "# print(preprocessed_text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4gGdZmwZDZ",
        "outputId": "86c7104f-e03b-45f6-9082-41a9f953f8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "doc = df_val['content'].iloc[499]\n",
        "print(doc)\n",
        "\n",
        "print(\"--\"*85)\n",
        "\n",
        "preprocessed = pre_process_stem_sentence(sentence=doc, stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer)\n",
        "print(len(preprocessed))\n",
        "print(preprocessed)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>This sounds like it's more a matter of determining the original name / wording used to file the copyright. </p>\n",
            "\n",
            "<p>Without knowing more of that language, you're left to the typical sleuthing options:</p>\n",
            "\n",
            "<ul>\n",
            "<li>names of the company owners / major shareholders as of 15-18 years ago. </li>\n",
            "<li>other DBAs and holding companies of the company originally presumed to hold the copyright. </li>\n",
            "<li>brute force search of all categorically-related copyrights in the time range. </li>\n",
            "</ul>\n",
            "\n",
            "<p>The advice I would really like to give you is knowledge of how IP attorneys filed video game copyrights in the time span of 1985-2000.  This could reveal any unexpected filing categories that could have been used as part of niche or experimental copyright strategy (at that time) for this kind of IP. </p>\n",
            "\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "70\n",
            "['sound', 'like', 'matter', 'determin', 'origin', 'name', 'word', 'use', 'file', 'copyright', 'without', 'know', 'languag', 'your', 'left', 'typic', 'sleuth', 'option', 'name', 'compani', 'owner', 'major', 'sharehold', 'year', 'ago', 'dba', 'hold', 'compani', 'compani', 'origin', 'presum', 'hold', 'copyright', 'brute', 'forc', 'search', 'categor', 'relat', 'copyright', 'time', 'rang', 'advic', 'would', 'realli', 'like', 'give', 'knowledg', 'ip', 'attorney', 'file', 'video', 'game', 'copyright', 'time', 'span', 'could', 'reveal', 'unexpect', 'file', 'categori', 'could', 'use', 'part', 'nich', 'experiment', 'copyright', 'strategi', 'time', 'kind', 'ip']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbSKXXrRGGgo"
      },
      "source": [
        "## Preprocess for each sentence of train-dataframe.\n",
        "df_train[\"stemmed_content\"] = df_train.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_val[\"stemmed_content\"] = df_val.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)\n",
        "df_test[\"stemmed_content\"] = df_test.apply(lambda row : pre_process_stem_sentence(sentence=row['content'], stop_words=stop_words, stemmer=stemmer, lemmatizer=lemmatizer), axis=1)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puTtacUoHBZ9"
      },
      "source": [
        "# Form Vocabulary from dataframe_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwyJWOOIETs"
      },
      "source": [
        "def print_first_n_keys_and_vals_dict(dictionary, n=5):\n",
        "    print(f\"Len dictionary = {len(dictionary)}, printing first {n} keys, vals\")\n",
        "    itr = 0\n",
        "    for key in dictionary:\n",
        "        print(key, dictionary[key])\n",
        "        itr += 1\n",
        "        if itr == n:\n",
        "            break"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO4hhJYkXqRp",
        "outputId": "0fa56bb1-30f5-49cc-d55c-51ba3e97a25f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%%time\n",
        "### Also remove one-len words\n",
        "dictionary_vocab = {} ## empty dict.\n",
        "for doc in df_train['stemmed_content']:\n",
        "    # print(f\"len doc = {len(doc)}\")\n",
        "    for word in doc:\n",
        "        if len(word) == 1:\n",
        "            continue\n",
        "\n",
        "        len_currently = len(dictionary_vocab) ## add to len. [idx new]\n",
        "\n",
        "        if word not in dictionary_vocab:\n",
        "            dictionary_vocab[word] = (0, len_currently)\n",
        "        else:\n",
        "            (val, idx) = dictionary_vocab[word]\n",
        "            dictionary_vocab[word] = (val + 1, idx) ## Maintain the same index.\n",
        "\n",
        "print(f\"len dictionary_vocab = {len(dictionary_vocab)}\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len dictionary_vocab = 17506\n",
            "CPU times: user 267 ms, sys: 971 Âµs, total: 268 ms\n",
            "Wall time: 269 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSQYB4eBf0sv"
      },
      "source": [
        "# file_name = \"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Vocab.txt\"\n",
        "# with open(file_name, 'w') as fw:\n",
        "#     for voc in dictionary_vocab:\n",
        "#         fw.write(voc)\n",
        "#         fw.write(\"\\n\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAgtrtyp13Tt"
      },
      "source": [
        "# ## https://www.kite.com/python/answers/how-to-save-a-dictionary-to-a-file-in-python\n",
        "# vocab_file = '/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/vocab_dict_df_train.pkl'\n",
        "\n",
        "# a_file = open(vocab_file, \"wb\")\n",
        "# pickle.dump(dictionary_vocab, a_file)\n",
        "# a_file.close()\n",
        "\n",
        "# # a_file = open(vocab_file, \"rb\")\n",
        "# # dictionary_vocab = pickle.load(a_file)\n",
        "# # print(dictionary_vocab)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJORdHUjHd7h"
      },
      "source": [
        "## Keep only those docs in train whose lengths are above 3 words i.e.\n",
        "### length of stemmed content > 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrZO1SycNmSX",
        "outputId": "20d1744f-a46a-4636-987a-efff4b516ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lens = [len(x) for x in df_train['stemmed_content']]\n",
        "print(max(lens))\n",
        "print(min(lens))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1381\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPJ5IMKKNmQV",
        "outputId": "8348bd6e-e922-4bc5-e8fc-ce8b290f7de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MIN_WORD_COUNT_TO_REMOVE = 3\n",
        "# print(df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE][\"stemmed_content\"])\n",
        "idxToRemove = df_train[df_train['stemmed_content'].str.len() <= MIN_WORD_COUNT_TO_REMOVE].index\n",
        "df_train.drop(idxToRemove , inplace=True)\n",
        "\n",
        "print(f\"After removal, len df_train = {len(df_train)}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After removal, len df_train = 5255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQCXuG-JhUU"
      },
      "source": [
        "## Create Hamming Distance Vectors by representing with 0/1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJfu0JkdKFRh"
      },
      "source": [
        "def form_hamming_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    for word in list_words:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index. [DO NOT]\n",
        "            # vec[UNKNOWN_WORD_INDEX] = 1\n",
        "            continue\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word]\n",
        "            vec[idx] = 1\n",
        "    return vec"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNrTRsZ2psv-"
      },
      "source": [
        "## Create hamming vectors for each col of dataframe\n",
        "df_train[\"ham_vector\"] = df_train.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"ham_vector\"] = df_val.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"ham_vector\"] = df_test.apply(lambda row : form_hamming_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNM1YUEWrfgk"
      },
      "source": [
        "## Create eucledian vectors by representing how many times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJXmvI8areMJ"
      },
      "source": [
        "def form_eucledian_vector(list_words, vocab_dict):\n",
        "    vec = np.zeros(len(vocab_dict)) # +1 for unknown word.\n",
        "    \n",
        "    ## Form a small dictionary to store each word count\n",
        "    dict_local_vocab = {}\n",
        "    for word in list_words:\n",
        "        if word not in dict_local_vocab:\n",
        "            dict_local_vocab[word] = 1\n",
        "        else:\n",
        "            dict_local_vocab[word] = dict_local_vocab[word] + 1\n",
        "\n",
        "    # print(dict_local_vocab)\n",
        "\n",
        "    # unknown_word_count = 1\n",
        "    for word in dict_local_vocab:\n",
        "        if word not in vocab_dict: ## add 1 to unkown word index.\n",
        "            continue\n",
        "            # vec[UNKNOWN_WORD_INDEX] = unknown_word_count\n",
        "            # unknown_word_count += 1\n",
        "        else: ## word is present in vocab, get index.\n",
        "            (value, idx) = vocab_dict[word] \n",
        "            vec[idx] = dict_local_vocab[word] ## replace with value of this word instead of 1.\n",
        "\n",
        "    del dict_local_vocab\n",
        "    return vec"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvxfO7dkpszM"
      },
      "source": [
        "df_train[\"euc_vector\"] = df_train.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_val[\"euc_vector\"] = df_val.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)\n",
        "df_test[\"euc_vector\"] = df_test.apply(lambda row : form_eucledian_vector(list_words=row['stemmed_content'], vocab_dict=dictionary_vocab), axis=1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEuOjilfKauq"
      },
      "source": [
        "# Stack each of these vertically to form hamming and eucledian vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB_4H48rL6kv",
        "outputId": "c020db6f-78d9-4925-d112-119afd7779d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "vocab_size = len(dictionary_vocab) ## PLUS 1 for <UNKNOWN> word\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "\n",
        "num_documents = len(df_train)\n",
        "print(f\"num_documents = {num_documents}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size = 17506\n",
            "num_documents = 5255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFUcRAcIKaPR",
        "outputId": "2c1e4f9c-7b2a-4c83-a467-1a98e5448573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "hamming_vectors_2D = np.zeros((num_documents, vocab_size))\n",
        "print(f\"hamming_vectors_2D.shape = {hamming_vectors_2D.shape}\")\n",
        "\n",
        "idx = 0\n",
        "for ham_vec in df_train['ham_vector'].values:\n",
        "    hamming_vectors_2D[idx] = ham_vec\n",
        "    idx += 1\n",
        "\n",
        "print(hamming_vectors_2D.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hamming_vectors_2D.shape = (5255, 17506)\n",
            "(5255, 17506)\n",
            "CPU times: user 340 ms, sys: 570 ms, total: 910 ms\n",
            "Wall time: 914 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_5z3V5lLu08",
        "outputId": "dc03de88-b6b2-47b0-aa58-e05fb360821b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "eucledian_vectors_2D = np.zeros((num_documents, vocab_size))\n",
        "print(f\"eucledian_vectors_2D.shape = {eucledian_vectors_2D.shape}\")\n",
        "\n",
        "idx = 0\n",
        "for vec in df_train['euc_vector'].values:\n",
        "    eucledian_vectors_2D[idx] = vec\n",
        "    idx += 1\n",
        "\n",
        "print(eucledian_vectors_2D.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eucledian_vectors_2D.shape = (5255, 17506)\n",
            "(5255, 17506)\n",
            "CPU times: user 247 ms, sys: 454 ms, total: 700 ms\n",
            "Wall time: 702 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZJFsqnGS1dv"
      },
      "source": [
        "# df_train.columns.values\n",
        "labels = df_train['Label'].values"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrEqp1_0LH5"
      },
      "source": [
        "## For now save these dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViUb7lUGzp9N"
      },
      "source": [
        "# df_train.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/train.csv\", index=False)\n",
        "# df_val.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/val.csv\", index=False)\n",
        "# df_test.to_csv(\"/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/test.csv\", index=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmALIJNfzuMJ"
      },
      "source": [
        "## Now, finally create TF-IDF and pickel dump everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__-GfML7BDN",
        "outputId": "f98b0d16-4089-4d51-d2e3-db4928389697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# %%time\n",
        "def compute_TF_IDF_all_train_set(hamming_vectors_2D, eucledian_vectors_2D, alpha=0.0001, beta=0.0001):\n",
        "    num_documents, num_words = eucledian_vectors_2D.shape\n",
        "    print(f\"num_documents, num_words = {num_documents, num_words}\")\n",
        "\n",
        "    TF = np.zeros((num_documents, num_words))\n",
        "\n",
        "    IDF = np.zeros((1, num_words))\n",
        "\n",
        "    ## Calculate TF(d, w) = N(d, w)/W(d) ; where N(d, w): count(w) in document d , W(d): Total #words in document d\n",
        "    for itr in range(num_documents): ## iterate row-wise\n",
        "        doc_eucledian = eucledian_vectors_2D[itr]\n",
        "        total_num_words_doc_eucledian = num_words - np.sum(doc_eucledian == 0)        \n",
        "        TF[itr] = doc_eucledian/total_num_words_doc_eucledian\n",
        "\n",
        "        # if itr == 1:\n",
        "            # print(f\"itr = {itr}, doc_euc[itr] = {np.unique(doc_eucledian[itr], return_counts=True)}\")\n",
        "            # print(f\"itr = {itr}, TF[itr] = {np.unique(TF[itr], return_counts=True)}\")\n",
        "            # break\n",
        "\n",
        "    ## Calculate IDF(d, w) = log( (D + alpha)/(C(w) + beta) ) ; C(w) -> Total # docs with word 'w' ; D -> Total # documents\n",
        "    D = num_documents\n",
        "    for itr in range(num_words): ## itereate col-wise\n",
        "        C_w = np.sum(hamming_vectors_2D[:, itr] == 1) ## first calculate C(w)\n",
        "        \n",
        "        IDF[:, itr] = np.log( (D + alpha) / (C_w + beta) )\n",
        "\n",
        "        # break\n",
        "        \n",
        "    TF_IDF = TF*IDF\n",
        "    del TF\n",
        "    # del IDF\n",
        "    return TF_IDF, IDF\n",
        "\n",
        "TF_IDF_whole_corpus, IDF_whole_corpus = compute_TF_IDF_all_train_set(hamming_vectors_2D=hamming_vectors_2D, eucledian_vectors_2D=eucledian_vectors_2D)\n",
        "print(f\"TF_IDF_whole_corpus.shape = {TF_IDF_whole_corpus.shape}\")\n",
        "print(f\"IDF_whole_corpus.shape = {IDF_whole_corpus.shape}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_documents, num_words = (5255, 17506)\n",
            "TF_IDF_whole_corpus.shape = (5255, 17506)\n",
            "IDF_whole_corpus.shape = (1, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWeLmV_094XH",
        "outputId": "1f9f2c1b-e93d-4f45-9649-633b8ead12da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def form_TF_IDF_for_val_test(list_words, IDF, hamming_vectors_2D, eucledian_vectors_2D, vocab_dict, alpha=0.0001, beta=0.0001):\n",
        "    num_docs_train, num_words = eucledian_vectors_2D.shape\n",
        "\n",
        "    TF = np.zeros((1, num_words))\n",
        "    \n",
        "    euc_vec = form_eucledian_vector(list_words=list_words, vocab_dict=vocab_dict)\n",
        "    \n",
        "    W_d_num_words_in_document = 0\n",
        "    for word in list_words:\n",
        "        if word in vocab_dict:\n",
        "            W_d_num_words_in_document += 1\n",
        "\n",
        "    TF = euc_vec/W_d_num_words_in_document\n",
        "\n",
        "    TF_IDF = TF*IDF\n",
        "\n",
        "    return TF_IDF\n",
        "\n",
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "\n",
        "TF_IDF = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                    eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "print(f\"TF_IDF.shape = {TF_IDF.shape}\")\n",
        "# print(np.unique(TF_IDF, return_counts=True))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF_IDF.shape = (1, 17506)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohbmnfhCXJfD"
      },
      "source": [
        "# Now we start with K-Nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl5ewWSbnxve",
        "outputId": "6a55048b-ba8b-477f-b0ca-597edcd20bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "df_val.columns.values"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['content', 'Label', 'stemmed_content', 'ham_vector', 'euc_vector'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th8E8_thY2en",
        "outputId": "436e1a5d-79df-4e3f-b7a0-5e32d9779476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "val_ham_0 = df_val['ham_vector'].iloc[0]\n",
        "print(val_ham_0.shape)\n",
        "\n",
        "val_euc_0 = df_val['euc_vector'].iloc[0]\n",
        "print(val_euc_0.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17506,)\n",
            "(17506,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu-Y52OvggBm"
      },
      "source": [
        "## Similarity functions.\n",
        "def ham(a, b):\n",
        "    return np.count_nonzero((a!=b), axis=1)\n",
        "\n",
        "def euclidean(a, b):\n",
        "    return np.linalg.norm((a-b), axis=1)\n",
        "\n",
        "### https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
        "def cosine_similarity(a, b):\n",
        "    cos_sim = np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "    return cos_sim"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GQLpzcgYY_L"
      },
      "source": [
        "# # a = np.array([x for x in range(40)])\n",
        "# a = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1])\n",
        "# a = a.reshape(5, -1)\n",
        "# print(a)\n",
        "\n",
        "# b = np.array([1, 0, 1])\n",
        "# b = b.reshape(1, -1)\n",
        "# print(\"\\n\", b)\n",
        "\n",
        "\n",
        "# print(ham(a, b))\n",
        "# print(euclidean(a, b))\n",
        "\n",
        "# a = np.array([2, 1, 3, 4, 5, 1, 10, 3, 22])\n",
        "# n = 4\n",
        "# indices_top = (-a).argsort()[:n]\n",
        "# print(f\"indices_top = {indices_top}\")\n",
        "# print(a[indices_top])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RQnLJLEXI2X",
        "outputId": "699bc50a-e946-42cc-f143-3e3dfc34e782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class KNN:\n",
        "    def __init__(self, K = 1, mode=\"hamming\", to_print=False):\n",
        "        self.mode = mode\n",
        "        self.K = K\n",
        "        if to_print == True:\n",
        "            print(f\"KNN __init__(K={K}, mode={mode})\")\n",
        "        \n",
        "\n",
        "    def populate_vectors(self, vectors_corpus, labels):\n",
        "        self.vectors_corpus = vectors_corpus\n",
        "        self.labels = labels\n",
        "\n",
        "    def compute_distances(self, v):\n",
        "        if self.mode == \"hamming\":\n",
        "            v = v.reshape(1, -1) ## Reshape vector.\n",
        "            self.distances = ham(v, self.vectors_corpus)\n",
        "        elif self.mode == \"euclidean\":\n",
        "            v = v.reshape(1, -1) ## Reshape vector.\n",
        "            self.distances = euclidean(v, self.vectors_corpus)\n",
        "        elif self.mode == 'cosine_similarity':\n",
        "            v = v.reshape(-1)\n",
        "            self.distances = cosine_similarity(v, self.vectors_corpus)\n",
        "\n",
        "        # print(f\"After compute_distances mode={self.mode}, distances = {self.distances}\")\n",
        "\n",
        "    def predict(self, v):\n",
        "        ## compute distances by using suitable similarity function.\n",
        "        self.compute_distances(v)\n",
        "\n",
        "        ## take argmax top results indices\n",
        "        if self.mode == 'cosine_similarity':\n",
        "            indices_top = (-self.distances).argsort()[:self.K] ## for some reason, this works for cosine-similarity\n",
        "        else:\n",
        "            indices_top = (self.distances).argsort()[:self.K] ## THIS works for hamming and euclidean\n",
        "\n",
        "        ## apply indices to labels\n",
        "        top_labels = self.labels[indices_top]\n",
        "\n",
        "        ## take majority vote and return the label\n",
        "        top_most_label = stats.mode(top_labels)[0][0]\n",
        "\n",
        "        ## return the max label\n",
        "        return top_most_label\n",
        "\n",
        "################################################# Test #################################################\n",
        "\n",
        "knn = KNN(K=1, mode='hamming')\n",
        "knn.populate_vectors(hamming_vectors_2D, labels)\n",
        "knn.predict(val_ham_0)\n",
        "\n",
        "# knn = KNN(K=7, mode='euclidean')\n",
        "# knn.populate_vectors(eucledian_vectors_2D, labels)\n",
        "# knn.predict(val_euc_0)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Coffee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQjSecagA0nX",
        "outputId": "5c613251-5910-49de-8b95-e73aa8efe27f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "knn = KNN(K=1, mode='cosine_similarity')\n",
        "knn.populate_vectors(TF_IDF_whole_corpus, labels)\n",
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "tf_idf_val_0 = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                    eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "knn.predict(tf_idf_val_0)\n",
        "tf_idf_val_0 = tf_idf_val_0.reshape(-1)\n",
        "cs = cosine_similarity(tf_idf_val_0, knn.vectors_corpus)\n",
        "print(cs.shape)\n",
        "print(cs)\n",
        "\n",
        "indices_top = (-cs).argsort()[:knn.K]\n",
        "print(indices_top)\n",
        "print(knn.labels[indices_top])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5255,)\n",
            "[0.         0.00026631 0.00062275 ... 0.00010283 0.         0.        ]\n",
            "[3056]\n",
            "['Wood_Working']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xr7vkezDOiB",
        "outputId": "79a7077d-2669-4a75-93b6-d71ef5605b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "tf_idf_val_0 = tf_idf_val_0.reshape(-1)\n",
        "print(tf_idf_val_0.shape)\n",
        "print(TF_IDF_whole_corpus.shape)\n",
        "\n",
        "cs = cosine_similarity(tf_idf_val_0, TF_IDF_whole_corpus)\n",
        "\n",
        "print(cs.shape)\n",
        "print(cs)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17506,)\n",
            "(5255, 17506)\n",
            "(5255,)\n",
            "[0.         0.00026631 0.00062275 ... 0.00010283 0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wzDleMoXe-b"
      },
      "source": [
        "## Applying tests on validation set for KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELPWJfGnICjU",
        "outputId": "db62077f-e054-4428-e407-3afd811469db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "# def add_to_dataframe_results(df_old, to_add):\n",
        "#     df_old = df_old.append(pd.Series(to_add, index=df_train.columns), ignore_index=True)\n",
        "#     return df_old\n",
        "\n",
        "column_names = [\"Similarity-Measure\", \"K\", \"Accuracy(%)\"]\n",
        "\n",
        "df_results_knn = pd.DataFrame(columns=column_names)\n",
        "\n",
        "# df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Hamming\", k, 22])\n",
        "\n",
        "display(df_results_knn.head())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Similarity-Measure</th>\n",
              "      <th>K</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Similarity-Measure, K, Accuracy(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G--zPyF5baEC"
      },
      "source": [
        "# !pip3 install tqdm\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "from functools import reduce"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKPaf3QVXens",
        "outputId": "210bc013-85ba-43ac-be08-acd792c3bd98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%%time\n",
        "for k in [1, 3, 5]: # [1, 3, 5]:\n",
        "    knn_ham = KNN(K=k, mode='hamming')\n",
        "    knn_euc = KNN(K=k, mode='euclidean')\n",
        "    knn_cosine = KNN(K=k, mode='cosine_similarity')\n",
        "\n",
        "\n",
        "    knn_ham.populate_vectors(hamming_vectors_2D, labels)\n",
        "    knn_euc.populate_vectors(eucledian_vectors_2D, labels)\n",
        "    knn_cosine.populate_vectors(TF_IDF_whole_corpus, labels)\n",
        "\n",
        "    \n",
        "\n",
        "    ham_correct = 0\n",
        "    euc_correct = 0\n",
        "    cosine_correct = 0\n",
        "\n",
        "    itr = 0\n",
        "    for (ham_val, lab_true) in (zip(df_val['ham_vector'].values, df_val['Label'].values)):\n",
        "        itr += 1\n",
        "        # if itr%300 == 0:\n",
        "            # print(f\"Ham itr = {itr}\")\n",
        "        lab_pred = knn_ham.predict(ham_val)\n",
        "        if lab_pred == lab_true:\n",
        "            ham_correct += 1\n",
        "\n",
        "    itr = 0\n",
        "    for (euc_val, lab_true) in (zip(df_val['euc_vector'].values, df_val['Label'].values)):\n",
        "        itr += 1\n",
        "        # if itr%300 == 0:\n",
        "            # print(f\"Euc itr = {itr}\")\n",
        "        lab_pred = knn_euc.predict(euc_val)\n",
        "        if lab_pred == lab_true:\n",
        "            euc_correct += 1\n",
        "\n",
        "    itr = 0\n",
        "    for (words_val, lab_true) in (zip(df_val['stemmed_content'].values, df_val['Label'].values)):\n",
        "        itr += 1\n",
        "        # if itr%300 == 0:\n",
        "        #     print(f\"TF-IDF itr = {itr}\")\n",
        "        val_tf_idf = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "        # if (itr % 10) == 0:\n",
        "            # print(np.unique(val_tf_idf, return_counts=True))\n",
        "            # break\n",
        "        lab_pred = knn_cosine.predict(val_tf_idf)\n",
        "        # print(knn_cosine.distances.shape)\n",
        "\n",
        "        if lab_pred == lab_true:\n",
        "            cosine_correct += 1\n",
        "\n",
        "    del knn_ham\n",
        "    del knn_euc\n",
        "    del knn_cosine\n",
        "\n",
        "\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Hamming\", k, ham_correct/len(df_val)*100])\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"Euclidean\", k, euc_correct/len(df_val)*100])\n",
        "    df_results_knn = add_to_dataframe(df_old=df_results_knn, to_add=[\"TF-IDF-cosine-sim\", k, cosine_correct/len(df_val)*100])\n",
        "\n",
        "    print(f\"k = {k}, Hamming: {(ham_correct/len(df_val)*100)}%, Euclidean: {(euc_correct/len(df_val)*100)}%, TF-IDF: {(cosine_correct/len(df_val)*100)}%\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "k = 1, Hamming: 72.16666666666667%, Euclidean: 78.5%, TF-IDF: 91.66666666666666%\n",
            "k = 3, Hamming: 78.33333333333333%, Euclidean: 80.33333333333333%, TF-IDF: 94.0%\n",
            "k = 5, Hamming: 78.33333333333333%, Euclidean: 81.0%, TF-IDF: 95.33333333333334%\n",
            "CPU times: user 4min 9s, sys: 4.49 s, total: 4min 14s\n",
            "Wall time: 3min 32s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvq54JeJXelm",
        "outputId": "59bb7db6-c7bb-4475-e041-30dbc6102752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "print(f\"len df_results_knn = {len(df_results_knn)}\")\n",
        "display(df_results_knn)\n",
        "# df_results_knn.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/KNN-val-3-topics.csv', index=False)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_knn = 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Similarity-Measure</th>\n",
              "      <th>K</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>1</td>\n",
              "      <td>46.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>1</td>\n",
              "      <td>58.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>1</td>\n",
              "      <td>72.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>3</td>\n",
              "      <td>45.954545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>3</td>\n",
              "      <td>55.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>3</td>\n",
              "      <td>76.954545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Hamming</td>\n",
              "      <td>5</td>\n",
              "      <td>48.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Euclidean</td>\n",
              "      <td>5</td>\n",
              "      <td>57.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TF-IDF-cosine-sim</td>\n",
              "      <td>5</td>\n",
              "      <td>78.727273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Similarity-Measure  K  Accuracy(%)\n",
              "0            Hamming  1    46.454545\n",
              "1          Euclidean  1    58.090909\n",
              "2  TF-IDF-cosine-sim  1    72.090909\n",
              "3            Hamming  3    45.954545\n",
              "4          Euclidean  3    55.909091\n",
              "5  TF-IDF-cosine-sim  3    76.954545\n",
              "6            Hamming  5    48.090909\n",
              "7          Euclidean  5    57.272727\n",
              "8  TF-IDF-cosine-sim  5    78.727273"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K-q5b6ZzqVP"
      },
      "source": [
        "mat = df_results_knn.values\n",
        "print(mat.shape)\n",
        "print(mat)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0LcdCK_RW8E"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBwmpIgW4XOD"
      },
      "source": [
        "### Combine all documents of each class i into one document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EagCOsAhzqiu"
      },
      "source": [
        "# unique_labels = np.unique(df_train['Label'].values)\n",
        "# dictionary_list_words_for_NB = {}\n",
        "# # for label in unique_labels:\n",
        "# #     if label not in dictionary_list_words:\n",
        "# #         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "    \n",
        "# for (list_words, label) in zip(df_train['stemmed_content'].values, df_train['Label'].values):\n",
        "#     if label not in dictionary_list_words_for_NB:\n",
        "#         dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "#     dictionary_list_words_for_NB[label].append(list_words)\n",
        "\n",
        "# ## reduce/flat-out to make 1D list.\n",
        "# ## https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
        "# for label in dictionary_list_words_for_NB:\n",
        "#     dictionary_list_words_for_NB[label] = reduce(operator.concat, dictionary_list_words_for_NB[label])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkQbU3mCONb"
      },
      "source": [
        "## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "def keywithmaxval(d):\n",
        "     \"\"\" a) create a list of the dict's keys and values; \n",
        "         b) return the key with the max value\"\"\"  \n",
        "     v=list(d.values())\n",
        "     k=list(d.keys())\n",
        "     return k[v.index(max(v))]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAcwIAKOFMJF",
        "outputId": "90999c8b-317e-449c-afb1-f8bbfa30ccda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self, vocab_size, alpha=0.01, to_print=False):\n",
        "        if to_print == True:\n",
        "            print(f\"NaiveBayes __init(alpha={alpha})__\")\n",
        "        self.alpha = alpha\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dictionary_list_words_for_NB = {}\n",
        "        self.dictionary_prior_probabilities = {}\n",
        "        self.dictionary_count_words_per_class = {}\n",
        "        self.dictionary_total_words_per_class = {}\n",
        "    \n",
        "\n",
        "    def fit(self, list_list_words, labels):\n",
        "        self.list_list_words = list_list_words\n",
        "        self.labels = labels\n",
        "        self.form_dictionary_list_words()\n",
        "        self.compute_probabilities()\n",
        "\n",
        "\n",
        "    def form_dictionary_list_words(self):\n",
        "        for (list_words, label) in zip(self.list_list_words, self.labels):\n",
        "            if label not in self.dictionary_list_words_for_NB:\n",
        "                self.dictionary_list_words_for_NB[label] = [] ## create new list.\n",
        "            self.dictionary_list_words_for_NB[label].append(list_words)\n",
        "        \n",
        "        for label in self.dictionary_list_words_for_NB: ## reduce/flat-out to make 1D list.\n",
        "            self.dictionary_list_words_for_NB[label] = reduce(operator.concat, self.dictionary_list_words_for_NB[label])\n",
        "\n",
        "\n",
        "    def compute_probabilities(self):\n",
        "        ## Compute prior probabilities/initial guesses\n",
        "        (classes, cnts) = np.unique(self.labels, return_counts=True)\n",
        "        cnts = cnts/np.sum(cnts) ## C_i / (C_1 + C_2 + ... + C_n)\n",
        "        for (lab, itr) in zip(classes, range(len(classes))):\n",
        "            self.dictionary_prior_probabilities[lab] = cnts[itr]\n",
        "\n",
        "        ## Compute per-word probabilities\n",
        "\n",
        "        ## Counter increment\n",
        "        for lab in classes:\n",
        "            num_words_this_class = 0\n",
        "            self.dictionary_count_words_per_class[lab] = {}\n",
        "            for word in self.dictionary_list_words_for_NB[lab]:\n",
        "                if word not in self.dictionary_count_words_per_class[lab]:\n",
        "                    self.dictionary_count_words_per_class[lab][word] = 0 ## initialize counter to 0.\n",
        "                self.dictionary_count_words_per_class[lab][word] = self.dictionary_count_words_per_class[lab][word] + 1 ## increment counter\n",
        "                num_words_this_class += 1\n",
        "            self.dictionary_total_words_per_class[lab] = num_words_this_class\n",
        "\n",
        "    def predict(self, list_words):\n",
        "        ## Compute probabilities for each label.\n",
        "        dict_probabilities_per_class = {}\n",
        "\n",
        "        for lab in np.unique(self.labels):\n",
        "            prob_log_curr_class = np.log(self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # prob_log_curr_class = (self.dictionary_prior_probabilities[lab]) ## start with prior probabilities\n",
        "            # print(f\"Before, prob_log_curr_class = {prob_log_curr_class}\")\n",
        "            for word in list_words: ## iterate per word\n",
        "                if word in self.dictionary_count_words_per_class[lab]: ## if word exists in THIS document.\n",
        "                    ## use smoothing factor alpha\n",
        "                    # prob_log_curr_class += np.log((self.dictionary_count_words_per_class[lab][word] + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size)) \n",
        "                    word_prob = self.dictionary_count_words_per_class[lab][word]\n",
        "                    # prob_log_curr_class = prob_log_curr_class*word_prob\n",
        "                else:\n",
        "                    word_prob = 0\n",
        "                prob_log_curr_class += np.log( (word_prob + self.alpha)/(self.dictionary_total_words_per_class[lab] + self.alpha*self.vocab_size) )\n",
        "\n",
        "\n",
        "            dict_probabilities_per_class[lab] = prob_log_curr_class\n",
        "            # print(dict_probabilities_per_class)\n",
        "\n",
        "        ## Get max probability class.\n",
        "        ## https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
        "        return keywithmaxval(d=dict_probabilities_per_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################### Checking ############################################################\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=len(dictionary_vocab))\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "check = df_val['stemmed_content'].iloc[0]\n",
        "p = NB.predict(check)\n",
        "print(p)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uILDNAL3FddD"
      },
      "source": [
        "list_new = [\n",
        "['coffee', 'tea', 'dew', 'dew', 'dew', 'dew'],\n",
        "['coffee', 'noir', 'homelander', 'dew'],\n",
        "['noir', 'noir', 'fool', 'fool', 'noir']\n",
        "]\n",
        "\n",
        "labs_new = [\n",
        "    'bev',\n",
        "    'supe',\n",
        "    'misc'\n",
        "]\n",
        "\n",
        "vocab_size = 6\n",
        "\n",
        "NB = NaiveBayes(alpha=0.01, vocab_size=6)\n",
        "NB.fit(list_new, labs_new)\n",
        "print(NB.predict(['coffee', 'tea', 'dew', 'dew', 'dew', 'dew']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f13yqf6EKAaJ"
      },
      "source": [
        "## Validation on NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgWpyxjCFMVM",
        "outputId": "4902df2d-4fb4-4ef1-c592-403221892c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"ALPHA\", \"Accuracy(%)\"]\n",
        "\n",
        "df_results_NB = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_results_NB.head())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ALPHA, Accuracy(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5FaA8KG8zu",
        "outputId": "0a5e720c-8719-4cf6-98c0-06524bfda118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "alpha_values = np.linspace(start=0.01, stop=1.0, num=10)\n",
        "print(alpha_values)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01 0.12 0.23 0.34 0.45 0.56 0.67 0.78 0.89 1.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rC01YFFMRn",
        "outputId": "a2ed1e7a-e575-4f32-b6b6-3453e6eb3e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "%%time\n",
        "### Validation NaiveBayes ###\n",
        "for alpha in (alpha_values):\n",
        "    NB = NaiveBayes(alpha=alpha, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "    NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)\n",
        "\n",
        "    nb_acc = 0\n",
        "    ## Predict each val set ##\n",
        "    for (x, y) in (zip(df_val['stemmed_content'].values, df_val['Label'].values)):\n",
        "        y_pred = NB.predict(x)\n",
        "        if y_pred == y:\n",
        "            nb_acc += 1\n",
        "\n",
        "\n",
        "    ## Append to dataframe and print.\n",
        "    # print(f\"NB alpha = {alpha}, accuracy = {nb_acc/len(df_val)*100} %\")\n",
        "    df_results_NB = add_to_dataframe(df_old=df_results_NB, to_add=[alpha, nb_acc/len(df_val)*100])\n",
        "    del NB"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.01)__\n",
            "NaiveBayes __init(alpha=0.12)__\n",
            "NaiveBayes __init(alpha=0.23)__\n",
            "NaiveBayes __init(alpha=0.34)__\n",
            "NaiveBayes __init(alpha=0.45)__\n",
            "NaiveBayes __init(alpha=0.56)__\n",
            "NaiveBayes __init(alpha=0.67)__\n",
            "NaiveBayes __init(alpha=0.78)__\n",
            "NaiveBayes __init(alpha=0.89)__\n",
            "NaiveBayes __init(alpha=1.0)__\n",
            "CPU times: user 9.49 s, sys: 99.4 ms, total: 9.59 s\n",
            "Wall time: 9.46 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIbwK_kMxTY"
      },
      "source": [
        "df_results_NB.sort_values(by=['Accuracy(%)'], inplace=True, ascending=False)\n",
        "display(df_results_NB.head(10))\n",
        "# df_results_NB.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/NB-val-3-topics.csv', index=False)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtew8yEpMxep",
        "outputId": "1bf9aa42-9f68-46d8-8f88-030eb54cd13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "display(df_results_NB.head(10))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALPHA</th>\n",
              "      <th>Accuracy(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>88.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.03</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.07</td>\n",
              "      <td>88.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.25</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.06</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.09</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.08</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.15</td>\n",
              "      <td>88.409091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ALPHA  Accuracy(%)\n",
              "1    0.02    88.500000\n",
              "3    0.04    88.500000\n",
              "2    0.03    88.454545\n",
              "4    0.05    88.454545\n",
              "6    0.07    88.454545\n",
              "24   0.25    88.409091\n",
              "5    0.06    88.409091\n",
              "8    0.09    88.409091\n",
              "7    0.08    88.409091\n",
              "14   0.15    88.409091"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4G-LMq-nUTK"
      },
      "source": [
        "# Hypothesis testing on Test Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG7zmcwEnbbT"
      },
      "source": [
        "### Create and fit best performing models on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjLPICwjpSdJ",
        "outputId": "78366155-710a-4dd0-f8d4-630dc98f915c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "words_val = df_val['stemmed_content'].iloc[0]\n",
        "print(words_val)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sagul', 'give', 'deflect', 'valu', 'horizont', 'shelf', 'span', 'use', 'variou', 'materi', 'thick', 'unless', 'work', 'extrem', 'heavi', 'load', 'weight', 'countertop', 'peopl', 'danc', 'materi', 'vertic', 'cabinet', 'wall', 'adequ', 'make', 'wall', 'thinner', 'still', 'remain', 'surprisingli', 'strong', 'long', 'cabinet', 'design', 'prevent', 'rack', 'even', 'particleboard', 'known', 'strength', 'hold', 'well', 'vertic', 'compressionbuckl', 'may', 'know', 'youv', 'ever', 'pack', 'book', 'store', 'cheap', 'particleboard', 'bookcas']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaPA10d8na_G",
        "outputId": "c5d376bd-42cf-4c16-f606-2283ac3bb456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##### KNN was K=5, TF-IDF #####\n",
        "knn = KNN(K=5, mode='cosine_similarity', to_print=True)\n",
        "knn.populate_vectors(TF_IDF_whole_corpus, labels)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN __init__(K=5, mode=cosine_similarity)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCgX5cNKpard",
        "outputId": "9719a77d-73a1-4f48-83ee-35aa3275f1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_tf_idf = form_TF_IDF_for_val_test(list_words=words_val, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "print(knn.predict(val_tf_idf))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOeVWWRdna3D",
        "outputId": "e7a77d69-d56a-4e9b-d1f2-7ef10077693f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##### Naive Bayes was alpha = 0.02/0.04, we will take 0.04 #####\n",
        "NB = NaiveBayes(alpha=0.04, vocab_size=len(dictionary_vocab), to_print=True)\n",
        "NB.fit(list_list_words=df_train['stemmed_content'].values, labels=df_train['Label'].values)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaiveBayes __init(alpha=0.04)__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHqRiSQYpfEB",
        "outputId": "cf46bb9a-6f2a-4e18-bbdd-844177b4197f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(NB.predict(words_val))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wood_Working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuz4R0PDoZsy"
      },
      "source": [
        "del df_train\n",
        "del df_val"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7ms6GZhm4CM"
      },
      "source": [
        "### Split test dataset 50 iterations per 10 of each topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCLTyO1nn34U"
      },
      "source": [
        "df_test.sort_values(by=['Label'], inplace=True)\n",
        "df_test.drop(labels='content', axis=1, inplace=True)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08aKywOkullM",
        "outputId": "2fdc2938-9e09-4baf-db2d-37f7747ad9ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"KNN-Acc(%)\", \"NB-Acc(%)\"]\n",
        "\n",
        "df_results_test_set = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_results_test_set.head())"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [KNN-Acc(%), NB-Acc(%)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNcU7hdTn3tc"
      },
      "source": [
        "%%time\n",
        "initial_offset = np.array([i*num_docs_in_each_topic for i in range(0, 11)])\n",
        "for counter_test in range(0, 50): ## run iterations 50 times\n",
        "    offsets = initial_offset + counter_test*10\n",
        "    start_indices = offsets\n",
        "    end_indices = offsets + 10\n",
        "    \n",
        "    # print(\"\\nCounter = \", counter_test)\n",
        "    # print(offsets)\n",
        "    # print(start_indices)\n",
        "    # print(end_indices)\n",
        "\n",
        "    ### Testing here ###\n",
        "    nb_correct = knn_correct = 0\n",
        "    for i in range(len(start_indices)): ## add all to list.\n",
        "        # print(np.unique(df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values, return_counts=True), end=' ')\n",
        "        for (x, y) in zip(df_test.iloc[start_indices[i]:end_indices[i]]['stemmed_content'].values, df_test.iloc[start_indices[i]:end_indices[i]]['Label'].values):\n",
        "            y_pred = NB.predict(x)\n",
        "            if y_pred == y:\n",
        "                nb_correct += 1\n",
        "            x_tf_idf = form_TF_IDF_for_val_test(list_words=x, IDF=IDF_whole_corpus, hamming_vectors_2D=hamming_vectors_2D, \n",
        "                eucledian_vectors_2D=eucledian_vectors_2D, vocab_dict=dictionary_vocab, alpha=0.0001, beta=0.0001)\n",
        "            y_pred = knn.predict(x_tf_idf)\n",
        "            if y_pred == y:\n",
        "                knn_correct += 1\n",
        "    \n",
        "    print(f\"Done for counter_test = {counter_test}\")\n",
        "\n",
        "    NUM_DOCUMENTS = 10* len(np.unique(df_test['Label'].values))\n",
        "    knn_acc = knn_correct/( NUM_DOCUMENTS )*100  ## 10*11 total 110 test documents per iteration. [50 iterations]\n",
        "    nb_acc = nb_correct/( NUM_DOCUMENTS )*100\n",
        "    print(f\"KNN-Acc = {knn_acc}%, NB-Acc = {nb_acc}%\")\n",
        "\n",
        "    df_results_test_set = add_to_dataframe(df_old=df_results_test_set, to_add=[knn_acc, nb_acc])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkTag67arcpn",
        "outputId": "70e9bddf-699f-4acd-f64c-388d0ca1e824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f\"len df_results_test_set = {len(df_results_test_set)}\")\n",
        "display(df_results_test_set)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len df_results_test_set = 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76.363636</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>70.909091</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>75.454545</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>74.545455</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>69.090909</td>\n",
              "      <td>85.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>89.090909</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>79.090909</td>\n",
              "      <td>93.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>94.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>93.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>75.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>79.090909</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>84.545455</td>\n",
              "      <td>86.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>95.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>85.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>80.909091</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>73.636364</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>75.454545</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>73.636364</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>70.000000</td>\n",
              "      <td>81.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>92.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>76.363636</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>77.272727</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>86.363636</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>88.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>83.636364</td>\n",
              "      <td>89.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    KNN-Acc(%)  NB-Acc(%)\n",
              "0    81.818182  87.272727\n",
              "1    76.363636  84.545455\n",
              "2    78.181818  90.909091\n",
              "3    85.454545  90.000000\n",
              "4    82.727273  91.818182\n",
              "5    77.272727  86.363636\n",
              "6    70.909091  89.090909\n",
              "7    82.727273  91.818182\n",
              "8    75.454545  90.909091\n",
              "9    80.909091  89.090909\n",
              "10   86.363636  90.000000\n",
              "11   80.000000  92.727273\n",
              "12   80.909091  88.181818\n",
              "13   77.272727  90.909091\n",
              "14   80.909091  90.000000\n",
              "15   78.181818  88.181818\n",
              "16   77.272727  91.818182\n",
              "17   78.181818  86.363636\n",
              "18   74.545455  90.909091\n",
              "19   84.545455  90.909091\n",
              "20   69.090909  85.454545\n",
              "21   80.000000  89.090909\n",
              "22   89.090909  92.727273\n",
              "23   78.181818  94.545455\n",
              "24   77.272727  84.545455\n",
              "25   79.090909  93.636364\n",
              "26   77.272727  94.545455\n",
              "27   83.636364  89.090909\n",
              "28   82.727273  93.636364\n",
              "29   81.818182  90.909091\n",
              "30   75.454545  90.000000\n",
              "31   82.727273  92.727273\n",
              "32   79.090909  86.363636\n",
              "33   84.545455  86.363636\n",
              "34   82.727273  91.818182\n",
              "35   81.818182  95.454545\n",
              "36   80.909091  85.454545\n",
              "37   80.909091  90.909091\n",
              "38   73.636364  90.909091\n",
              "39   75.454545  89.090909\n",
              "40   73.636364  89.090909\n",
              "41   70.000000  81.818182\n",
              "42   83.636364  92.727273\n",
              "43   78.181818  88.181818\n",
              "44   76.363636  88.181818\n",
              "45   77.272727  89.090909\n",
              "46   86.363636  90.909091\n",
              "47   80.000000  88.181818\n",
              "48   83.636364  89.090909\n",
              "49   78.181818  87.272727"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrDQX97qrchw"
      },
      "source": [
        "df_results_test_set.to_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Test-Set-KNN-NB.csv', index=False)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBHCUUtF2Kz8"
      },
      "source": [
        "## Load and analyze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dee3qAsxrcg6",
        "outputId": "9f926a87-3eff-4c07-c7d4-d96b69464562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_results_test_set = pd.read_csv('/content/drive/My Drive/ML-Undergrad-Assignments-Projects/Assignment-2 Text NaiveBayes KNN/Test-Set-KNN-NB.csv')\n",
        "display(df_results_test_set.head(5))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN-Acc(%)</th>\n",
              "      <th>NB-Acc(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.818182</td>\n",
              "      <td>87.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76.363636</td>\n",
              "      <td>84.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78.181818</td>\n",
              "      <td>90.909091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>85.454545</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82.727273</td>\n",
              "      <td>91.818182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   KNN-Acc(%)  NB-Acc(%)\n",
              "0   81.818182  87.272727\n",
              "1   76.363636  84.545455\n",
              "2   78.181818  90.909091\n",
              "3   85.454545  90.000000\n",
              "4   82.727273  91.818182"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4tqi3TS3LH0",
        "outputId": "d1b42722-3a1e-429a-b5c6-7ebb68089b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "knn_acc = df_results_test_set['KNN-Acc(%)'].values\n",
        "nb_acc = df_results_test_set['NB-Acc(%)'].values\n",
        "print(knn_acc.shape, nb_acc.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50,) (50,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDJKAptY5mEK",
        "outputId": "f3fc3d1d-ce83-4b56-a117-22154c40267b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "column_names = [\"Method\", \"Mean-Acc(%)\", \"Minimum-Acc(%)\", \"Maximum-Acc(%)\", \"Std-dev\", \"Std-Error\"]\n",
        "\n",
        "df_stats = pd.DataFrame(columns=column_names)\n",
        "\n",
        "display(df_stats.head())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Mean-Acc(%)</th>\n",
              "      <th>Minimum-Acc(%)</th>\n",
              "      <th>Maximum-Acc(%)</th>\n",
              "      <th>Std-dev</th>\n",
              "      <th>Std-Error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Method, Mean-Acc(%), Minimum-Acc(%), Maximum-Acc(%), Std-dev, Std-Error]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MIAT9Dy3LF4"
      },
      "source": [
        "## Summarized results.\n",
        "df_stats = add_to_dataframe(df_old=df_stats, to_add=[\"KNN K=5, TF-IDF\", np.mean(knn_acc), min(knn_acc), max(knn_acc), np.std(knn_acc), stats.sem(knn_acc, axis=None, ddof=0)])\n",
        "df_stats = add_to_dataframe(df_old=df_stats, to_add=[\"NB alpha = 0.04\", np.mean(nb_acc), min(nb_acc), max(nb_acc), np.std(nb_acc), stats.sem(nb_acc, axis=None, ddof=0)])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tg1WtdcrcX9",
        "outputId": "cf2123ad-a44a-48f7-e75e-8b2116f99a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "display(df_stats)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Mean-Acc(%)</th>\n",
              "      <th>Minimum-Acc(%)</th>\n",
              "      <th>Maximum-Acc(%)</th>\n",
              "      <th>Std-dev</th>\n",
              "      <th>Std-Error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNN K=5, TF-IDF</td>\n",
              "      <td>79.454545</td>\n",
              "      <td>69.090909</td>\n",
              "      <td>89.090909</td>\n",
              "      <td>4.189716</td>\n",
              "      <td>0.592515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NB alpha = 0.04</td>\n",
              "      <td>89.672727</td>\n",
              "      <td>81.818182</td>\n",
              "      <td>95.454545</td>\n",
              "      <td>2.832865</td>\n",
              "      <td>0.400628</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Method  Mean-Acc(%)  ...   Std-dev  Std-Error\n",
              "0  KNN K=5, TF-IDF    79.454545  ...  4.189716   0.592515\n",
              "1  NB alpha = 0.04    89.672727  ...  2.832865   0.400628\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYKuEjwr7vdM"
      },
      "source": [
        "## Computing T-statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNcoJHAZ64EK",
        "outputId": "1aea8c38-08e2-49c2-f3dc-b3527a1165e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
        "\n",
        "# ans = stats.ttest_rel(knn_acc, nb_acc)\n",
        "ans = stats.ttest_ind(knn_acc, nb_acc, equal_var=True)\n",
        "print(ans)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ttest_indResult(statistic=-14.142663823802279, pvalue=2.0995095838767057e-25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSZmV8YV64DO"
      },
      "source": [
        "# Ttest_indResult(statistic=-14.142663823802279, pvalue=3.641774754361954e-24)\n",
        "# Ttest_indResult(statistic=-14.142663823802279, pvalue=2.0995095838767057e-25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBFks-IP64CR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI990uMi634Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}